---
title: "Interaction terms in regression models"
author: "MACS 30200 - Perspectives on Computational Research"
output:
  html_document:
    toc: true
    toc_float: true
    code_folding: hide
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(cache = TRUE,
                      message = FALSE,
                      warning = FALSE)
```

# Objectives

* Define multiplicative interaction models and conditional effects
* Review mathematics required to calculate standard errors and conduct inference
* Identify major types of conceptual interactions
* Demonstrate how to estimate multiplicative interaction models in R
* Implement hypothesis testing and marginal effects plots
* Extend discussion to generalized linear models

```{r packages, cache = FALSE, message = FALSE, warning = FALSE}
library(tidyverse)
library(forcats)
library(broom)
library(modelr)
library(stringr)
library(titanic)
library(rcfss)
library(car)
library(plotly)

options(digits = 3)
set.seed(1234)
theme_set(theme_minimal())
```

# Multiplicative interaction models

## Additive model

Consider a linear additive model of the form:

$$Y = \beta_0 + \beta_1 X + \beta_2 Z + e_i$$

By definition, we expect $\beta_1$ to estimate the direct effect (or relationship) of $X_1$ on $Y$ independent of $Z$.^[Likewise for $\beta_2$.] A data generating process with a continuous value for $X$ and $Y$ and a dichotomous value for $Z$ following this functional form would look like:

```{r sim-linear}
n_sim <- 100

additive <- data_frame(x = runif(n_sim),
                       z = runif(n_sim)) %>%
  mutate(z = ifelse(z > .5, 1, 0),
         y = x + z + rnorm(n_sim))

additive %>%
  add_predictions(lm(y ~ x + z, data = .)) %>%
  ggplot(aes(x, y, color = factor(z))) +
  geom_point() +
  geom_line(aes(y = pred)) +
  theme(legend.position = "none")
```

Consider the net impact of $X$ on $Y$:

$$E(Y) = \beta_0 + \beta_1 X + \beta_2 Z$$

If we take the first derivative of this with respect to $X$, we get:

$$\frac{\delta E(Y)}{\delta X} = \beta_1$$

In other words, the marginal impact of $X$ on $Y$ is constant and completely independent of $Z$. Likewise, the marginal impact of $Z$ on $Y$ is:

$$\frac{\delta E(Y)}{\delta Z} = \beta_2$$

Interpreting the estimated parameters and standard errors is relatively straightforward in this setup.

## Multiplicative interaction model

But the additive functional form only makes sense if you theoretically believe the effects of $X$ and $Z$ are independent. Consider instead a simple interactive model with three variables:

$$Y = \beta_0 + \beta_1 X + \beta_2 Z + \beta_3 XZ + e_i$$

Here $XZ$ is just that: $X$ multiplied by $Z$. $\beta_1$ and $\beta_2$ are commonly (and erroneously) referred to as the **direct effects** of $X$ and $Z$ on $Y$,^[Brambor et. al. define $X$ and $Z$ as the **constitutive terms**] and $\beta_3$ as the coefficient for the **interaction term**.

With the variables multiplied together, the net impact of $X$ on $Y$ is now defined by:

$$
\begin{split}
E(Y) & = \beta_0 + \beta_1 X + \beta_2 Z + \beta_3 XZ \\
     & = \beta_0 + \beta_2 Z + (\beta_1 + \beta_3 Z) X
\end{split}
$$

And the first derivative of this with respect to $X$ is:

$$\frac{\delta E(Y)}{\delta X} = \beta_1 + \beta_3 Z$$

In other words, the marginal impact of $X$ on $Y$ now explicitly depends on the value of $Z$. Another way of phrasing the expected value function above is:

$$E(Y) = \beta_0 + \beta_2 Z + \psi_1 X$$

where $\psi_1 = \beta_1 + \beta_3 Z$ is a "quasi-coefficient" for the marginal impact of $X$ on $Y$. Likewise with respect to $Z$:

$$
\begin{split}
E(Y) & = \beta_0 + \beta_1 X + (\beta_2 + \beta_3 X) Z \\
     & = \beta_0 + \beta_2 Z + \psi_2 Z
\end{split}
$$

In this type of interactive model, each interacted covariate's influence on $Y$ is now conditional on the values of the other explanatory variable(s) with which it is interacted.

* The direct effects $\beta_1$ and $\beta_2$ represent the *conditional* impact of $X$ and $Z$ on $Y$ *conditional on the value of the other interacted variable being zero*.
    * If $Z = 0$, then:
    
        $$
\begin{split}
E(Y) & = \beta_0 + \beta_1 X + \beta_2 (0) + \beta_3 X (0) \\
     & = \beta_0 + \beta_1 X
\end{split}
        $$
        
    * If $X = 0$, then:
    
        $$
\begin{split}
E(Y) & = \beta_0 + \beta_1 (0) + \beta_2 Z + \beta_3 (0) Z \\
     & = \beta_0 + \beta_2 Z
\end{split}
        $$
    * Under these circumstances, $\psi_1 = \beta_1$ and $\psi_2 = \beta_2$
    * $\hat{\beta}_1$ and $\hat{\beta}_2$ are relevant, but only in the specific circumstance where the interacted variable is zero.
* The interaction coefficient $\beta_3$ indicates whether or not the effect of $X$ on $Y$ is systematically (and monotonically/linearly) different over different values of $Z$, and vice-versa
    * $+\beta_3$ means the impact of $X$ on $Y$ grows more positive/less negative at larger values of $Z$, and less positive/more negative at lower values of $Z$
    * $-\beta_3$ means the impact of $X$ on $Y$ grows less positive/more negative at larger values of $Z$, and more positive/less negative at lower values of $Z$
* In truth, what we care most about are not $\beta_1$, $\beta_2$, and $\beta_3$, but instead $\psi_1$ and $\psi_2$

> In an interactive model, all influences of interacted variables are now necessarily conditional on the value(s) of the variable(s) with which they are interacted.

### Conducting inference

That said, a multiplicative interaction model is still a standard linear regression model, so the same principles of OLS apply. That means we can obtain estimates of the $\beta$s in the standard way, and we can estimate standard errors for those coefficient estimates in the usual way (as the square roots of the diagonal elements of the variance-covariance matrix $\hat{\sigma}^2 (\mathbf{X}' \mathbf{X})^{-1}$).

The problem is that we don't care about these quantities - we care about the marginal effects, that is $\psi_1$ and $\psi_2$. Calculating point estimates for these is trivial:

$$\hat{\psi}_1 = \hat{\beta}_1 + \hat{\beta}_3 Z$$
$$\hat{\psi}_2 = \hat{\beta}_2 + \hat{\beta}_3 X$$

But in order to conduct inference, we need to know if these values are statistically distinguishable from zero. For that, we need estimate the **variance** for these parameters. Three key rules help us define the equation for their variance. Where $X$ and $Y$ are random variables and $a$ is a constant:

1. $\text{Var}(aX) = a^2 \text{Var}(X)$
1. $\text{Var}(X+Y) = \text{Var}(X) + \text{Var}(Y) + 2 \text{Cov}(X,Y)$
1. $\text{Cov}(X, aY) = a \text{Cov}(X,Y)$

So if we are interested in $\text{Var} (\hat{\psi}_1)$:

$$\widehat{\text{Var}(\hat{\psi}_1}) = \widehat{\text{Var} (\hat{\beta}_1)} +Z^2  \widehat{\text{Var} (\hat{\beta}_3)} + 2 Z \widehat{\text{Cov} (\hat{\beta}_1, \hat{\beta}_3)}$$

Likewise for $\text{Var} (\hat{\psi}_2)$:

$$\widehat{\text{Var}(\hat{\psi}_2}) = \widehat{\text{Var} (\hat{\beta}_2)} + X^2  \widehat{\text{Var} (\hat{\beta}_3)} + 2 X \widehat{\text{Cov} (\hat{\beta}_2, \hat{\beta}_3)}$$

* Both depend on - but are not the same as - the estimated variances for the direct effects $\beta_1$ and $\beta_2$, and/or the interaction term $\beta_3$.
* Both also depend on the level/value of the interacted variable. This means that just as the slope of $X$ on $Y$ depends on $Z$, so too does the estimated variability around that slope estimate also depend on $Z$.

# Types of interactions

There are several major types of interaction terms that can occur within regression models. Here I present examples of these major types using simulated data.

## Two dichtomous covariates

$$Y = \beta_0 + \beta_1 D_1 + \beta_2 D_2 + \beta_3 D_1 D_2 + e_i$$

In this model, the expected value for $Y$ can take on four possible values:

$$
\begin{split}
E(Y | D_1 = 0, D_2 = 0) & = \beta_0 \\
E(Y | D_1 = 1, D_2 = 0) & = \beta_0 + \beta_1 \\
E(Y | D_1 = 0, D_2 = 1) & = \beta_0 + \beta_2 \\
E(Y | D_1 = 1, D_2 = 1) & = \beta_0 + \beta_1 + \beta_2 + \beta_3 \\
\end{split}
$$

Consider data generated from the following model:

$$Y = 10 + 20 D_1 - 20 D_2 + 40 D_1 D_2 + e, \text{ where } e \sim N(0, 5)$$

```{r sim-two-dich}
two_dich <- data_frame(x = runif(n_sim),
                       z = runif(n_sim)) %>%
  mutate_at(vars(x, z), funs(ifelse(. > .5, 1, 0))) %>%
  mutate(y = 10 + 20 * x - 20 * z + 40 * (x * z) + rnorm(n_sim, 0, 5))
```

We could visualize it a couple of ways. First as a histogram:

```{r sim-two-dich-hist}
ggplot(two_dich, aes(y, color = interaction(x, z))) +
  geom_density() +
  scale_color_discrete(labels = c("D1 = D2 = 0",
                                  "D1 = 1, D2 = 0",
                                  "D1 = 0, D2 = 1",
                                  "D1 = D2 = 1"),
                       guide = guide_legend(nrow = 2)) +
  labs(title = expression(paste("Values of ", Y, " for various combinations of values of ", D[1], ",", D[2])),
       x = "Value of Y",
       color = NULL) +
  theme(legend.position = "bottom")
```

Or as a boxplot:

```{r sim-two-dich-box}
ggplot(two_dich, aes(interaction(z, x), y)) +
  geom_boxplot() +
  scale_x_discrete(labels = c(expression("D1 = D2 = 0"),
                                  expression("D1 = 0, D2 = 1"),
                                  expression("D1 = 1, D2 = 0"),
                                  expression("D1 = D2 = 1"))) +
  labs(title = expression(paste("Values of ", Y, " for various combinations of values of ", D[1], ",", D[2])),
       x = NULL,
       y = "Values of Y")
```

This illustrates that:

* The effect of a change in $D_2$ varies depending on the value of $D_1$
    * The effect is negative when $D_1 = 0$
    * But positive when $D_1 = 1$
* Similar effects for the change in $D_1$ relative to $D_2$

## One dichotomous and one continuous covariate

More commonly, we have a situation where one variable in the interaction is continous and the other is dichotomous:

$$Y = \beta_0 + \beta_1 X + \beta_2 D + \beta_3 XD + e_i$$

$$
\begin{split}
E(Y | X, D = 0) & = \beta_0 + \beta_1 X \\
E(Y | X, D = 1) & = (\beta_0 + \beta_2) + (\beta_1 + \beta_3) X
\end{split}
$$

This suggests four possibilities:

* $X$ has both the same slope and the same intercept for $D=0$ and $D=1$; that is, $\beta_2 = \beta_3 = 0$.

    ```{r sim-cont-dich-00}
    data_frame(x = runif(n_sim, -2, 2),
               z = runif(n_sim, -2, 2)) %>%
      mutate_at(vars(z), funs(ifelse(. > .5, 1, 0))) %>%
      mutate(y = 0 + 20 * x + 0 * z + 0 * (x * z) + rnorm(n_sim, 0, 5)) %>%
      ggplot(aes(x, y, color = factor(z), shape = factor(z))) +
      geom_point() +
      geom_abline(intercept = 0, slope = 20) +
      labs(title = expression({beta[2] == beta[3]} == 0),
           color = "Z",
           shape = "Z") +
      theme(legend.position = "bottom")
    ```
    
* $X$ has both the same slope but different intercepts for $D=0$ and $D=1$; that is, $\beta_2 \neq 0$ and $\beta_3 = 0$.

    ```{r sim-cont-dich-01}
    data_frame(x = runif(n_sim, -2, 2),
               z = runif(n_sim, -2, 2)) %>%
      mutate_at(vars(z), funs(ifelse(. > .5, 1, 0))) %>%
      mutate(y = 0 + 20 * x + 10 * z + 0 * (x * z) + rnorm(n_sim, 0, 5)) %>%
      ggplot(aes(x, y, color = factor(z), shape = factor(z))) +
      geom_point() +
      geom_abline(intercept = 0, slope = 20) +
      geom_abline(intercept = 10, slope = 20, linetype = 2) +
      labs(title = expression(paste({beta[2] != 0}, ",", beta[3] == 0)),
           color = "Z",
           shape = "Z") +
      theme(legend.position = "bottom")
    ```
    
* $X$ has both the same intercept but different slopes for $D=0$ and $D=1$; that is, $\beta_2 = 0$ and $\beta_3 \neq 0$.

    ```{r sim-cont-dich-10}
    data_frame(x = runif(n_sim, -2, 2),
               z = runif(n_sim, -2, 2)) %>%
      mutate_at(vars(z), funs(ifelse(. > .5, 1, 0))) %>%
      mutate(y = 0 + 20 * x + 0 * z + -40 * (x * z) + rnorm(n_sim, 0, 5)) %>%
      ggplot(aes(x, y, color = factor(z), shape = factor(z))) +
      geom_point() +
      geom_abline(intercept = 0, slope = 20) +
      geom_abline(intercept = 0, slope = -20, linetype = 2) +
      labs(title = expression(paste({beta[2] == 0}, ",", beta[3] != 0)),
           color = "Z",
           shape = "Z") +
      theme(legend.position = "bottom")
    ```
    
* $X$ has both different intercepts and different slopes for $D=0$ and $D=1$; that is, $\beta_2 \neq 0$ and $\beta_3 \neq 0$.

    ```{r sim-cont-dich-11}
    data_frame(x = runif(n_sim, -2, 2),
               z = runif(n_sim, -2, 2)) %>%
      mutate_at(vars(z), funs(ifelse(. > .5, 1, 0))) %>%
      mutate(y = 0 + 20 * x + 20 * z + -40 * (x * z) + rnorm(n_sim, 0, 5)) %>%
      ggplot(aes(x, y, color = factor(z), shape = factor(z))) +
      geom_point() +
      geom_abline(intercept = 0, slope = 20) +
      geom_abline(intercept = 20, slope = -20, linetype = 2) +
      labs(title = expression(paste({beta[2] == 0}, ",", beta[3] != 0)),
           color = "Z",
           shape = "Z") +
      theme(legend.position = "bottom")
    ```

## Two continuous covariates

It is also common to have a model in which both interacted covariates are continuous:

$$Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 X_1 X_2 + e_i$$

The interpretation here is that each covariate's effect on $Y$ changes **smoothly** and **monotonically** as a function of the other covariate.

```{r two-cont}
# no interactive effects
data_frame(x = runif(n_sim, 0, 10),
           z = runif(n_sim, 0, 10),
           y = 10 + 10 * x - 10 * z + 0 * x * z) %>%
  plot_ly(x = ~x, y = ~ z, z = ~ y, type = "mesh3d") %>%
  layout(title = "No interactive effects")

# inteactive effects
data_frame(x = runif(n_sim, 0, 10),
           z = runif(n_sim, 0, 10),
           y = 10 + 10 * x - 10 * z + 10 * x * z) %>%
  plot_ly(x = ~x, y = ~ z, z = ~ y, type = "mesh3d") %>%
  layout(title = "Interactive effects")
```

Note that in the first figure, the marginal influence of a change in $X_1$ on $Y$ is not dependent on the value of $X_2$. By contrast, in the next figure when values of $X_1$ are small, the effect of $X_2$ on $Y$ is both negative and relatively small; however, when $X_1$ is large, the effect of $X_2$ on $Y$ is both large and positive.

## Quadratic, cubic, and other polynomial effects

Curvilinear or quadratic relationships for a single variable are also a type of interactive model. For example, consider a second-order polynomial regression:

$$Y = \beta_0 + \beta_1 X + \beta_2 X^2 + e$$

The marginal effect of $X$ on $Y$ is:

$$\frac{\delta E(Y)}{\delta X} = \beta_1 + 2 \beta_2 X$$

This tells us that the marginal effect of $X$ on $Y$ depends linearly on the value of $X$ itself.

```{r poly-interact}
data_frame(x = runif(n_sim, -2, 2)) %>%
  mutate(y = 10 + 10 * x - (50 * x^2) + rnorm(n_sim, 0, 5)) %>%
  add_predictions(lm(y ~ poly(x, 2, raw = TRUE), data = .)) %>%
  ggplot(aes(x, y)) +
  geom_point() +
  geom_line(aes(y = pred)) +
  labs(title = "Example of a quadratic relationship")

data_frame(x = runif(n_sim, -2, 2)) %>%
  mutate(y = -500 - 20 * x + 300 * x^2 + rnorm(n_sim, 0, 50)) %>%
  add_predictions(lm(y ~ poly(x, 2, raw = TRUE), data = .)) %>%
  ggplot(aes(x, y)) +
  geom_point() +
  geom_line(aes(y = pred)) +
  labs(title = "Example of a quadratic relationship")

data_frame(x = runif(n_sim, -2, 2)) %>%
  mutate(y = 10 + 10 * x - 50 * x^2 + 300 * x^3 + rnorm(n_sim, 0, 300)) %>%
  add_predictions(lm(y ~ poly(x, 3, raw = TRUE), data = .)) %>%
  ggplot(aes(x, y)) +
  geom_point() +
  geom_line(aes(y = pred)) +
  labs(title = "Example of a cubic relationship")
```


## Higher-order interaction terms

We could expand on this process and adopt higher-order interaction models:

$$Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 X_3 + \beta_4 X_1 X_2 + \beta_5 X_1 X_3 + \beta_6 X_2 X_3 + \beta_7 X_1 X_2 X_3 + e$$

The various $X$s could be continuous or dichotomous, with corresponding interpretations. In this model, each of the three $X$s conditions the influence of the other. This can be become very complex and confusing. In the simplest case with two dichotmous variables and one continuous $X$:

$$Y = \beta_0 + \beta_1 X + \beta_2 D_1 + \beta_3 D_2 + \beta_4 X D_1 + \beta_5 X D_2 + \beta_6 D_1 D_2 + \beta_7 X D_1 D_2 + e$$

```{r three-way}
data_frame(x = runif(n_sim, -2, 2),
           d1 = runif(n_sim),
           d2 = runif(n_sim)) %>%
  mutate_at(vars(d1, d2), funs(ifelse(. > .5, 1, 0))) %>%
  mutate(y = 10 + 10 * x + 10 * d1 - 20 * d2 - 20 * x * d1 + 20 * x *d2 - 50 * x * d1 * d2 + rnorm(n_sim, 0, 10)) %>%
  add_predictions(lm(y ~ x * d1 * d2, data = .)) %>%
  ggplot(aes(x, y)) +
  facet_grid(d1 ~ d2, labeller = label_both) +
  geom_point() +
  geom_line(aes(y = pred))
```

Here the slope of $Y$ on $X$ is positive when $D_1 = 0$ and negative when $D_1 = 1$. In addition, $D_2$ has an exacerbating effect: its presence makes the effect of $X$ on $Y$ stronger, irrespective of whether the effect is positive or negative.

# Key rules for estimating and interpreting interaction terms

* Don't omit the "direct effects" components from an interactive regression model. Doing so leads to bias in your estimates of the parameters - see Brambor et. al. (2006) for more details.
* Because the direct effects are the impact of one variable on the dependent variable when the other is zero, it is important that zero be a meaningful value.
    * E.g. if you interact with time, the year 0 A.D. is pretty out-of-sample
* Rescaling the variables in an interactive model can make the direct effects appear statistically distinguishable from zero (recall that the standard errors of the interactive effect are influenced directly by the value of the constituent term) - that doesn't mean the variables' effects are meaningful.
* Remember that there are [more flexible alternatives](persp010_nonparametric.html) to polynomial terms in a regression model.
* Three-way interactions are incredibly difficult to interpret and understand substantively. If possible, split the sample by a dummy variable and estimate multiple regression models.

# Estimating models with multiplicative interactions


## Model estimation


## Calculating standard errors


## Hypothesis testing


## Predicted values plots


## Marginal effects plots



# Session Info {.toc-ignore}

```{r child='_sessioninfo.Rmd'}
```




