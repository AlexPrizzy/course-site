<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />


<meta name="author" content="MACS 30200 - Perspectives on Computational Research" />


<title>Frequentist vs. Bayesian inference</title>

<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/readable.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="site_libs/jqueryui-1.11.4/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<script src="site_libs/navigation-1.1/codefolding.js"></script>
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-45631879-2', 'auto');
  ga('send', 'pageview');

</script>


<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
div.sourceCode {
  overflow-x: visible;
}
</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>


<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>


</head>

<body>

<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
.tabbed-pane {
  padding-top: 12px;
}
button.code-folding-btn:focus {
  outline: none;
}
</style>


<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 66px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 71px;
  margin-top: -71px;
}

.section h2 {
  padding-top: 71px;
  margin-top: -71px;
}
.section h3 {
  padding-top: 71px;
  margin-top: -71px;
}
.section h4 {
  padding-top: 71px;
  margin-top: -71px;
}
.section h5 {
  padding-top: 71px;
  margin-top: -71px;
}
.section h6 {
  padding-top: 71px;
  margin-top: -71px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>


<div class="container-fluid main-container">

<!-- tabsets -->
<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});
</script>

<!-- code folding -->
<style type="text/css">
.code-folding-btn { margin-bottom: 4px; }
</style>
<script>
$(document).ready(function () {
  window.initializeCodeFolding("hide" === "show");
});
</script>




<script>
$(document).ready(function ()  {

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_').toLowerCase();
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}


.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
  padding-left: 25px;
  text-indent: 0;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>

<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row-fluid">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">Computing for the Social Sciences</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">Home</a>
</li>
<li>
  <a href="faq.html">FAQ</a>
</li>
<li>
  <a href="syllabus.html">Syllabus</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div class="fluid-row" id="header">

<div class="btn-group pull-right">
<button type="button" class="btn btn-default btn-xs dropdown-toggle" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false"><span>Code</span> <span class="caret"></span></button>
<ul class="dropdown-menu" style="min-width: 50px;">
<li><a id="rmd-show-all-code" href="#">Show All Code</a></li>
<li><a id="rmd-hide-all-code" href="#">Hide All Code</a></li>
</ul>
</div>



<h1 class="title toc-ignore">Frequentist vs. Bayesian inference</h1>
<h4 class="author"><em>MACS 30200 - Perspectives on Computational Research</em></h4>

</div>


<div id="objectives" class="section level1">
<h1>Objectives</h1>
<ul>
<li>Define Bayes’ theorem</li>
<li>Define Bayesian inference</li>
<li>Define frequentist inference</li>
<li>Demonstrate basic Bayesian inference compared to frequentist inference</li>
<li>Identify computational methods for estimating Bayesian models</li>
</ul>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(tidyverse)
<span class="kw">library</span>(broom)
<span class="kw">library</span>(forcats)
<span class="kw">library</span>(modelr)
<span class="kw">library</span>(stringr)
<span class="kw">library</span>(car)
<span class="kw">library</span>(rcfss)
<span class="kw">library</span>(coefplot)
<span class="kw">library</span>(RColorBrewer)
<span class="kw">library</span>(lme4)

<span class="kw">options</span>(<span class="dt">digits =</span> <span class="dv">3</span>)
<span class="kw">set.seed</span>(<span class="dv">1234</span>)
<span class="kw">theme_set</span>(<span class="kw">theme_minimal</span>())</code></pre></div>
</div>
<div id="bayes-theorem" class="section level1">
<h1>Bayes’ theorem</h1>
<p><strong>Bayes’ theorem</strong> is a fundamental component of both probability and statistics and is central to understanding the differences between frequentist and Bayesian inference. For two events <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span>, Bayes’ theorem states that:</p>
<p><span class="math display">\[P(B|A) = \frac{P(A|B) \cdot P(B)}{P(A)}\]</span></p>
<p>Bayes’ rule tells us how to <strong>invert</strong> conditional probabilities. That is, to find <span class="math inline">\(P(B|A)\)</span> from <span class="math inline">\(P(A|B)\)</span>. Let’s walk through this with a few examples.</p>
<div id="coin-tossing" class="section level2">
<h2>Coin tossing</h2>
<p>Toss a coin 5 times. Let <span class="math inline">\(H_1 =\)</span> “first toss is heads” and let <span class="math inline">\(H_A =\)</span> “all 5 tosses are heads”. Therefore <span class="math inline">\(P(H_1 | H_A) = 1\)</span> (if all five tosses are heads, then the first one must by definition also be heads) and <span class="math inline">\(P(H_A | H_1) = \frac{1}{16}\)</span> (<span class="math inline">\(\frac{1}{2^4} = \frac{1}{16}\)</span>).</p>
<p>However we can also use Bayes’ theorem to calculate <span class="math inline">\(P(H_1 | H_A)\)</span> using <span class="math inline">\(P(H_A | H_1)\)</span>. The terms we need are:</p>
<ul>
<li><span class="math inline">\(P(H_A | H_1) = \frac{1}{16}\)</span></li>
<li><span class="math inline">\(P(H_1) = \frac{1}{2}\)</span></li>
<li><span class="math inline">\(P(H_A) = \frac{1}{32}\)</span></li>
</ul>
<p>So,</p>
<p><span class="math display">\[P(H_A | H_1) = \frac{P(H_A | H_1) \cdot P(H_1)}{P(H_A)} = \frac{\frac{1}{16} \cdot \frac{1}{2}}{\frac{1}{32}} = 1\]</span></p>
</div>
<div id="radar-detection-problem" class="section level2">
<h2>Radar detection problem</h2>
<p>If an aircraft is present in a certain area, a radar correctly registers its presence with probability <span class="math inline">\(0.99\)</span>. If it is not present, the radar falsely registers an aircraft presence with probability <span class="math inline">\(0.10\)</span>. We assume that an aircraft is present with probability <span class="math inline">\(0.05\)</span>. What is the probability that an airplane is present and the radar correctly detects it?</p>
<ul>
<li><span class="math inline">\(A = {\text{an aircraft is present}}\)</span></li>
<li><span class="math inline">\(B = {\text{the radar registers an aircraft presence}}\)</span></li>
<li><span class="math inline">\(P(A = 1) = 0.05\)</span></li>
<li><span class="math inline">\(P(B = 1 | A = 1) = 0.99\)</span></li>
<li><span class="math inline">\(P(B = 1 | A = 0) = 0.1\)</span></li>
</ul>
<p><span class="math display">\[
\begin{align}
P(\text{aircraft present} | \text{radar registers}) &amp;= P(A|B) \\
&amp; = \frac{P(A) \cdot P(B|A)}{P(B)} \\
&amp; = \frac{P(A) \cdot P(B|A)}{P(A) \cdot P(B|A) + P(A = 0) \cdot(B | A = 0)} \\
&amp; = \frac{0.05 \cdot 0.99}{0.05 \cdot 0.99 + 0.95 \cdot 0.1} \\
&amp; \approx 0.3426
\end{align}
\]</span></p>
<p>Notice that we never were given the <span class="math inline">\(P(A|B)\)</span> (probability an airplane is present and the radar detects it), but we could calculate it based on the <span class="math inline">\(P(B|A)\)</span> (probability the radar detects something and an airplane is present). Also note that <span class="math inline">\(P(A|B) \neq P(B|A)\)</span>.</p>
</div>
<div id="false-positive-fallacy" class="section level2">
<h2>False positive fallacy</h2>
<p>A test for a certain rare disease is assumed to be correct 95% of the time:</p>
<ul>
<li>If a person has the disease, then the test results are positive with probability 0.95</li>
<li>If the person does not have the disease, then the test results are negative with probability 0.95</li>
</ul>
<p>A random person drawn from a certain population has probability 0.001 of having the disease. Given that the person just tested positive, what is the probability of having the disease?</p>
<ul>
<li><span class="math inline">\(A = {\text{person has the disease}}\)</span></li>
<li><span class="math inline">\(B = {\text{test result is positive for the disease}}\)</span></li>
<li><span class="math inline">\(P(A) = 0.001\)</span></li>
<li><span class="math inline">\(P(B | A) = 0.95\)</span></li>
<li><span class="math inline">\(P(B | A = 0) = 0.05\)</span></li>
</ul>
<p><span class="math display">\[
\begin{align}
P(\text{person has the disease} | \text{test is positive}) &amp;= P(A|B) \\
&amp; = \frac{P(A) \cdot P(B|A)}{P(B)} \\
&amp; = \frac{P(A) \cdot P(B|A)}{P(A) \cdot P(B|A) + P(A = 0) \cdot(B | A = 0)} \\
&amp; = \frac{0.001 \cdot 0.95}{0.001 \cdot 0.95 + 0.999 \cdot 0.05} \\
&amp; = 0.0187
\end{align}
\]</span></p>
<p>Even though the test is fairly accurate, a person who has tested positive is still very unlikely (less than 2%) to have the disease. Because the base rate of the disease in the population is so low, the vast majority of people taking the test are healthy and even with an accurate test most of the positives will be healthy people.<a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a></p>
</div>
</div>
<div id="bayesian-inference" class="section level1">
<h1>Bayesian inference</h1>
<p>Much of the statistics we conduct is <strong>inferential statistics</strong> - we want to draw inferences about the world given empirical data. Generally we specify a statistical model for the process by which the data arises and estimate the parameters for the model. We rarely make definitive statements about such parameters because of error and random noise generated by the data generating process, so we want to couch our findings in probabilistic statements.</p>
<p>In scientific experiments, we start with a hypothesis and collect data to test the hypothesis. We will often let <span class="math inline">\(H\)</span> represent the event “our hypothesis is true” and let <span class="math inline">\(D\)</span> be the collected data. In other words, according to Bayes’ theorem:</p>
<p><span class="math display">\[P(\text{hypothesis is true}| \text{data}) = \frac{P(\text{data} | \text{hypothesis is true}) \cdot P(\text{hypothesis is true})}{P(\text{data})}\]</span></p>
<p>The left-hand term is the probability our hypothesis is true given the data we collected. This is what we want to know, if we can identify all the components on the right-hand side. But rarely do we have all those values. We can still work around this as we’ll see shortly.</p>
<div id="disease-screening-redux" class="section level2">
<h2>Disease screening redux</h2>
<p>Recall that under Bayes’ theorem:</p>
<p><span class="math display">\[
\begin{align}
P(\text{person has the disease} | \text{test is positive}) &amp;= P(A|B) \\
&amp; = \frac{P(A) \cdot P(B|A)}{P(B)} \\
&amp; = \frac{P(A) \cdot P(B|A)}{P(A) \cdot P(B|A) + P(A = 0) \cdot(B | A = 0)} \\
&amp; = \frac{0.001 \cdot 0.95}{0.001 \cdot 0.95 + 0.999 \cdot 0.05} \\
&amp; = 0.0187
\end{align}
\]</span></p>
<p>Before the test, we would have said the probability the person had the disease was 0.001. After the test, we see the probability is 0.0187, or 18 times more likely than before the test. That is, the positive test provides some evidence that the person has the disease (even if it isn’t good evidence).</p>
</div>
<div id="coin-tossing-example" class="section level2">
<h2>Coin tossing example</h2>
<p>Now let’s look at a coin tossing problem. There are three types of coins with different probabilities of landing heads when tossed.</p>
<ul>
<li>Type <span class="math inline">\(A\)</span> coins are fair, with <span class="math inline">\(p = 0.5\)</span> of heads</li>
<li>Type <span class="math inline">\(B\)</span> coins are bent, with <span class="math inline">\(p = 0.6\)</span> of heads</li>
<li>Type <span class="math inline">\(C\)</span> coins are bent, with <span class="math inline">\(p = 0.9\)</span> of heads</li>
</ul>
<p>Suppose I have a drawer containing 5 coins: 2 of type <span class="math inline">\(A\)</span>, 2 of type <span class="math inline">\(B\)</span>, and 1 of type <span class="math inline">\(C\)</span>. I reach into the drawer and pick a coin at random. Without showing you the coin I flip it once and get heads. What is the probability it is type <span class="math inline">\(A\)</span>? Type <span class="math inline">\(B\)</span>? Type <span class="math inline">\(C\)</span>?</p>
<div id="terminology" class="section level3">
<h3>Terminology</h3>
<p>Let <span class="math inline">\(A\)</span>, <span class="math inline">\(B\)</span>, and <span class="math inline">\(C\)</span> be the event the chosen coin was of the respective type. Let <span class="math inline">\(D\)</span> be the event that the toss is heads. The problem then asks us to find:</p>
<p><span class="math display">\[P(A|D), P(B|D), P(C|D)\]</span></p>
<p>Before applying Bayes’ theorem, we need to define a few things:</p>
<ul>
<li>Experiment - pick a coin from the drawer at random, flip it, and record the result</li>
<li>Data - the result of the experiment. Here, <span class="math inline">\(D = \text{heads}\)</span>. <span class="math inline">\(D\)</span> is data that provides evidence for or against each hypothesis</li>
<li>Hypotheses - we are testing three hypotheses: the coin is type <span class="math inline">\(A\)</span>, <span class="math inline">\(B\)</span>, or <span class="math inline">\(C\)</span></li>
<li><p>Prior probability - the probability of each hypothesis prior to tossing the coin (collecting data). Since the drawer has 2 coins of type <span class="math inline">\(A\)</span>, 2 of type <span class="math inline">\(B\)</span>, and 1 of type <span class="math inline">\(C\)</span>, we have:</p>
<p><span class="math display">\[P(A) = 0.4, P(B) = 0.4, P(C) = 0.2\]</span></p></li>
<li><p>Likelihood - the likelihood function (same as used for maximum-likelihood estimation). The likelihood function is <span class="math inline">\(P(D|H)\)</span>, the probability of the data assuming that the hypothesis is true. Most often we will consider the data as fixed and let the hypothesis vary. For example, <span class="math inline">\(P(D|A) =\)</span> probability of heads if the coin is type <span class="math inline">\(A\)</span>. In our case, the likelihoods are:</p>
<p><span class="math display">\[P(D|A) = 0.5, P(D|B) = 0.6, P(D|C) = 0.9\]</span></p>
<p>We can think of these as parameters for a series of Bernoulli distributions.</p></li>
<li><p>Posterior probability - the probability (posterior to) of each hypothesis given the data from tossing the coin:</p>
<p><span class="math display">\[P(A|D), P(B|D), P(C|D)\]</span></p>
<p>These posterior probabilities are what we want to find.</p></li>
</ul>
<p>We can now use Bayes’ theorem to compute each of the posterior probabilities. The theorem says:</p>
<p><span class="math display">\[P(A|D) = \frac{P(D|A) \cdot P(A)}{P(D)}\]</span> <span class="math display">\[P(B|D) = \frac{P(D|B) \cdot P(B)}{P(D)}\]</span> <span class="math display">\[P(C|D) = \frac{P(D|C) \cdot P(C)}{P(D)}\]</span></p>
<p><span class="math inline">\(P(D)\)</span> can be computed using the law of total probability:</p>
<p><span class="math display">\[
\begin{align}
P(D) &amp; = P(D|A) \cdot P(A) + P(D|B) \cdot P(B) + P(D|C) \cdot P(C) \\
&amp; = 0.5 \cdot 0.4 + 0.6 \cdot 0.4 + 0.9 \cdot 0.2 = 0.62
\end{align}
\]</span></p>
<p>So each of the posterior probabilities are:</p>
<p><span class="math display">\[P(A|D) = \frac{P(D|A) \cdot P(A)}{P(D)} = \frac{0.5 \cdot 0.4}{0.62} = \frac{0.2}{0.62}\]</span></p>
<p><span class="math display">\[P(B|D) = \frac{P(D|B) \cdot P(B)}{P(D)} = \frac{0.6 \cdot 0.4}{0.62} = \frac{0.24}{0.62}\]</span></p>
<p><span class="math display">\[P(C|D) = \frac{P(D|C) \cdot P(C)}{P(D)} = \frac{0.9 \cdot 0.2}{0.62} = \frac{0.18}{0.62}\]</span></p>
<p>Notice that the total probability <span class="math inline">\(P(D)\)</span> is the same in each of the denominators and is the sum of the three numerators.</p>
<table>
<thead>
<tr class="header">
<th>hypothesis</th>
<th>prior</th>
<th>likelihood</th>
<th>Bayes numerator</th>
<th>posterior</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(H\)</span></td>
<td><span class="math inline">\(P(H)\)</span></td>
<td><span class="math inline">\(P(D\mid H)\)</span></td>
<td><span class="math inline">\(P(D \mid H) \cdot P(H)\)</span></td>
<td><span class="math inline">\(P(H \mid D)\)</span></td>
</tr>
<tr class="even">
<td>A</td>
<td>0.4</td>
<td>0.5</td>
<td>0.2</td>
<td>0.3226</td>
</tr>
<tr class="odd">
<td>B</td>
<td>0.4</td>
<td>0.6</td>
<td>0.24</td>
<td>0.3871</td>
</tr>
<tr class="even">
<td>C</td>
<td>0.2</td>
<td>0.9</td>
<td>0.18</td>
<td>0.2903</td>
</tr>
<tr class="odd">
<td>total</td>
<td>1</td>
<td></td>
<td>0.62</td>
<td>1</td>
</tr>
</tbody>
</table>
<p>The <strong>Bayes numerator</strong> is the product of the prior and the likelihood. The posterior probability is obtained by dividing the Bayes numerator by <span class="math inline">\(P(D) = 0.625\)</span>.</p>
<p>The process of going from the prior probability <span class="math inline">\(P(H)\)</span> to the posterior <span class="math inline">\(P(H|D)\)</span> is called <strong>Bayesian updating</strong>. Bayesian updating uses the data to alter our understanding of the probability of each hypothesis.</p>
</div>
<div id="things-to-notice" class="section level3">
<h3>Things to notice</h3>
<ol style="list-style-type: decimal">
<li>The posterior probabilities for each hypothesis are in the last column. Coin <span class="math inline">\(B\)</span> is the most probable, even with the decrease from the prior to the posterior. <span class="math inline">\(C\)</span> has increased from 0.2 to 0.29.</li>
<li>The Bayes numerator determines the posterior probability. To compute the posterior probability, simply rescale the Bayes numerator so that it sums to 1.</li>
<li>If all we care about is finding the most likely hypothesis, the Bayes numerator works as well as the normalized posterior.</li>
<li>The posterior probability represents the outcome of a tug-of-war between the likelihood and the prior. When calculating the posterior, a large prior may be deflated by a small likelihood, and a small prior may be inflated by a large likelihood.</li>
</ol>
<p>Therefore we can express Bayes’ theorem as:</p>
<p><span class="math display">\[P(\text{hypothesis}| \text{data}) = \frac{P(\text{data} | \text{hypothesis}) \cdot P(\text{hypothesis})}{P(\text{data})}\]</span></p>
<p><span class="math display">\[P(H|D) = \frac{P(D | H) \cdot P(H)}{P(D)}\]</span></p>
<p>With the data fixed, the denominator <span class="math inline">\(P(D)\)</span> just serves to normalize the total posterior probability to 1. So we could express Bayes’ theorem as a statement about the proportionality of two functions of <span class="math inline">\(H\)</span>:</p>
<p><span class="math display">\[P(\text{hypothesis}| \text{data}) \propto P(\text{data} | \text{hypothesis}) \cdot P(\text{hypothesis})\]</span> <span class="math display">\[\text{posterior} \propto \text{likelhood} \cdot \text{prior}\]</span></p>
</div>
</div>
<div id="prior-and-posterior-probability-mass-functions" class="section level2">
<h2>Prior and posterior probability mass functions</h2>
<p>In more general terms, we use random variables and probability mass/density functions:</p>
<ul>
<li><span class="math inline">\(\theta\)</span> is the value of the hypothesis</li>
<li><span class="math inline">\(P(\theta)\)</span> is the prior probability mass function of the hypothesis</li>
<li><span class="math inline">\(p(\theta | D)\)</span> is the posterior probability mass function of the hypothesis given the data</li>
<li><span class="math inline">\(p(D | \theta)\)</span> is the likelihood function</li>
</ul>
</div>
<div id="updating-again-and-again" class="section level2">
<h2>Updating again and again</h2>
<p>In life we continually update our beliefs with each new experience of the world. In Bayesian inference, today’s posterior is tomorrow’s prior.</p>
<div id="terror-attack-example" class="section level3">
<h3>Terror attack example</h3>
<p>Consider the September 11th attacks in New York City. Say that before the first plane hit, our estimate of the probability of a terror attack on tall buildings in Manhattan was just 1 in 20,000, or 0.005 percent. But we also assign a low probability to a plane hitting the World Trade Center by accident: 1 in 12,500 on any given day.<a href="#fn2" class="footnoteRef" id="fnref2"><sup>2</sup></a> Consider the use of Bayes’ theorem in this instance. What is the probability of terrorists crashing planes into Manhattan skyscrapers given the first plane hitting the World Trade Center?</p>
<ul>
<li>Our initial estimate of how likely it is that terrorists would crash planes into Manhattan skyscrapers is <span class="math inline">\(p = 0.005\)</span></li>
<li>Probability of plane hitting if terrorists are attacking Manhattan is <span class="math inline">\(p = 1\)</span></li>
<li>Probability of plane hitting if terrorists are not attacking Manhattan skyscrapers (i.e. an accident) is <span class="math inline">\(p = 0.008\)</span></li>
</ul>
<p>Our posterior probability of a terror attack, given the first plane hitting the world trade center, is:</p>
<ul>
<li><span class="math inline">\(A =\)</span> terror attack</li>
<li><span class="math inline">\(B =\)</span> plane hitting the World Trade Center</li>
<li><span class="math inline">\(P(B|A) =\)</span> probability of a terrorist crashing a plane into Manhattan skyscrapers</li>
<li><span class="math inline">\(P(A) =\)</span> probability of a plane hitting if terrorists are attacking Manhattan skyscrapers</li>
<li><span class="math inline">\(P(B| A = 0) =\)</span> probability of a plane hitting if terrorists are not attacking Manhattan skyscrapers</li>
</ul>
<p><span class="math display">\[
\begin{align}
P(A|B) &amp;= \frac{P(B|A) \cdot P(A)}{P(B)} \\
 &amp;= \frac{P(B|A) \cdot P(A)}{P(B|A) \cdot P(A) + P(B| A = 0) \cdot P(A=0)} \\
&amp; = \frac{0.005 \cdot 1}{0.005 \cdot 1 + 0.008 \cdot 0.995} \\
&amp; \approx 0.38
\end{align}
\]</span></p>
<p>We would now estimate a posterior probability of a 38% chance of a terrorist attack on the World Trade Center. But we can continuously update this posterior probability as new data presents itself.</p>
<p><span class="math display">\[
\begin{align}
P(A|B) &amp;= \frac{P(B|A) \cdot P(A)}{P(B)} \\
 &amp;= \frac{P(B|A) \cdot P(A)}{P(B|A) \cdot P(A) + P(B| A = 0) \cdot P(A=0)} \\
&amp; = \frac{0.38 \cdot 1}{0.38 \cdot 1 + 0.38 \cdot 0.995} \\
&amp; \approx .9999
\end{align}
\]</span></p>
</div>
</div>
<div id="choosing-priors" class="section level2">
<h2>Choosing priors</h2>
<p>Bayesian inference requires a known prior. What happens if the prior is not known with certainty? Then we need to come up with one. A common approach uses a <strong>uniform prior</strong>, or a flat probability distribution.<a href="#fn3" class="footnoteRef" id="fnref3"><sup>3</sup></a> Other priors come from different distributions, such as Gamma or Beta distributions, or use previous posterior probability distributions as prior probability distributions in updating Bayesian models.</p>
<p><span class="math display">\[P(\text{hypothesis}| \text{data}) = \frac{P(\text{data} | \text{hypothesis}) \cdot P(\text{hypothesis})}{P(\text{data})}\]</span></p>
<p><span class="math display">\[\text{posterior} \propto \text{likelhood} \cdot \text{prior}\]</span></p>
<p>If we choose a different prior, we will not obtain the same posterior probabilities. This is one of the major critiques of Bayesian inference, since selection of the prior is based on domain knowledge (and potentially previously obtained posterior probabilities).</p>
</div>
<div id="probability-intervals" class="section level2">
<h2>Probability intervals</h2>
<p>Suppose we have a probability mass function <span class="math inline">\(P(\theta)\)</span> or probability density function <span class="math inline">\(f(\theta)\)</span> describing our belief about the value of an unknown parameter of interest <span class="math inline">\(\theta\)</span>. A <strong><span class="math inline">\(p\)</span>-probability interval</strong> for <span class="math inline">\(\theta\)</span> is an interval <span class="math inline">\([a,b]\)</span> with <span class="math inline">\(P(a \leq \theta \leq b) = p\)</span>.</p>
<p>Probability intervals are also known as <strong>credible intervals</strong>. <strong>These are not the same thing as confidence intervals</strong>.</p>
<p>There are many probability intervals for a given pdf. Different <span class="math inline">\(p\)</span>-probability intervals for <span class="math inline">\(\theta\)</span> may have different widths, depending on where we center the interval.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">sim_data &lt;-<span class="st"> </span><span class="kw">data_frame</span>(<span class="dt">x =</span> <span class="kw">rnorm</span>(<span class="dv">1000</span>, <span class="dv">0</span>, <span class="dv">1</span>))

xmin_start &lt;-<span class="st"> </span><span class="kw">c</span>(.<span class="dv">25</span>, .<span class="dv">01</span>, .<span class="dv">4</span>)
sim_50p &lt;-<span class="st"> </span><span class="kw">data_frame</span>(<span class="dt">xmin =</span> <span class="kw">qnorm</span>(xmin_start),
                      <span class="dt">xmax =</span> <span class="kw">qnorm</span>(xmin_start +<span class="st"> </span>.<span class="dv">5</span>),
                      <span class="dt">y =</span> <span class="kw">c</span>(-.<span class="dv">075</span>, -.<span class="dv">05</span>, -.<span class="dv">025</span>))

xmin_start &lt;-<span class="st"> </span><span class="kw">c</span>(.<span class="dv">1</span>, .<span class="dv">01</span>, .<span class="dv">05</span>)
sim_90p &lt;-<span class="st"> </span><span class="kw">data_frame</span>(<span class="dt">xmin =</span> <span class="kw">qnorm</span>(xmin_start),
                      <span class="dt">xmax =</span> <span class="kw">qnorm</span>(xmin_start +<span class="st"> </span>.<span class="dv">9</span>),
                      <span class="dt">y =</span> <span class="kw">c</span>(-.<span class="dv">15</span>, -.<span class="dv">125</span>, -.<span class="dv">1</span>))

<span class="kw">ggplot</span>(sim_data, <span class="kw">aes</span>(x)) +
<span class="st">  </span><span class="kw">stat_function</span>(<span class="dt">fun =</span> dnorm, <span class="dt">n =</span> <span class="dv">10000</span>, <span class="dt">args =</span> <span class="kw">list</span>(<span class="dt">mean =</span> <span class="dv">0</span>, <span class="dt">sd =</span> <span class="dv">1</span>)) +
<span class="st">  </span><span class="kw">geom_segment</span>(<span class="dt">data =</span> sim_50p, <span class="kw">aes</span>(<span class="dt">x =</span> xmin, <span class="dt">xend =</span> xmax, <span class="dt">y =</span> y, <span class="dt">yend =</span> y),
               <span class="dt">color =</span> <span class="st">&quot;blue&quot;</span>) +
<span class="st">  </span><span class="kw">geom_segment</span>(<span class="dt">data =</span> sim_90p, <span class="kw">aes</span>(<span class="dt">x =</span> xmin, <span class="dt">xend =</span> xmax, <span class="dt">y =</span> y, <span class="dt">yend =</span> y),
               <span class="dt">color =</span> <span class="st">&quot;orange&quot;</span>)</code></pre></div>
<p><img src="persp017_inference_files/figure-html/pi-width-1.png" width="672" /></p>
<p>The blue bars span .5-probability intervals. Each bar contains 50% of the probability mass in the pdf. The lengths differ because they are centered over different percentiles (e.g. <span class="math inline">\([.25, .75]\)</span>, <span class="math inline">\([.01, .51]\)</span>, <span class="math inline">\([.4, .9]\)</span>). Likewise for the .9-probability bars in orange. The shortest bars are symmetric and centered over the most likely value (e.g. the mean).</p>
<p>Probability intervals are an intuitive and effective way to summarize and communicate your beliefs. Likely you don’t want to describe an entire function <span class="math inline">\(f(\theta)\)</span> in words. Think about it:</p>
<blockquote>
<p>I think <span class="math inline">\(\theta\)</span> is between 0.45 and 0.65 with 50% probability.</p>
</blockquote>
<blockquote>
<p>I think <span class="math inline">\(\theta\)</span> follows a <span class="math inline">\(\text{Beta}(8, 6)\)</span> distribution.</p>
</blockquote>
</div>
</div>
<div id="frequentist-inference" class="section level1">
<h1>Frequentist inference</h1>
<p>Compare this Bayesian approach to the more familiar <strong>frequentist</strong> school of inference. Both of them are based on probability and Bayes’ theorem:</p>
<p><span class="math display">\[P(H|D) = \frac{P(D | H) \cdot P(H)}{P(D)}\]</span></p>
<p>When the prior is known, both Bayesians and frequentists use Bayes’ rule. However, when the prior is not known Bayesians develop one based on the best available information. Frequentists instead abandon Bayes’ theorem and conduct inference just based on the likelihood function:</p>
<p><span class="math display">\[L(H; D) = P(D|H)\]</span></p>
<p>The core reasons for this deviation is the meaning of probability. Frequentists believe that probabilities represent long-term frequencies of repeatable random experiments. So if “a coin has probability 0.5 of heads”, a frequentist believes this means the relative frequency of heads goes to 0.5 as the number of flips approaches infinity. This means the frequentist finds it nonsensical to specify a probability distribution for a parameter <span class="math inline">\(\theta\)</span> with a fixed value.</p>
<p>In short, Bayesians put probability distributions on both hypotheses and data, while frequentists put probability distributions on (random, repeatable, experimental) data given a hypothesis.<a href="#fn4" class="footnoteRef" id="fnref4"><sup>4</sup></a></p>
<div id="null-hypothesis-testing" class="section level2">
<h2>Null hypothesis testing</h2>
<p>This explains the <strong>null hypothesis test</strong>. The presumption is that some hypothesis (parameter specifying the conditional distribution of the data) is true and that the observed data is sampled from that distribution. Therefore, under the null hypothesis we assume the parameter(s) is zero. If that is true, what is the probability of observing the given data? It does not depend on a subjective prior that could vary from researcher to researcher. This is where we get all of the things like:</p>
<ul>
<li><span class="math inline">\(t\)</span>-tests</li>
<li><span class="math inline">\(p\)</span>-values</li>
<li>Confidence intervals
<ul>
<li>Note that confidence intervals are different from credible intervals</li>
<li>Confidence intervals describe variability in the data <span class="math inline">\(D\)</span> for a fixed parameter <span class="math inline">\(\theta\)</span>
<ul>
<li>“With a large number of repeated samples, 95% of such calculated confidence intervals would include the true value of the parameter”</li>
</ul></li>
<li>Credible intervals describe variability in the parameter <span class="math inline">\(\theta\)</span> for a fixed data <span class="math inline">\(D\)</span>
<ul>
<li>“There is a 90% chance that the parameter falls between these two values”</li>
</ul></li>
</ul></li>
</ul>
</div>
<div id="bayesian-inference-1" class="section level2">
<h2>Bayesian inference</h2>
<ul>
<li>Uses probabilities for both hypotheses and data</li>
<li>Depends on the prior and likelihood of observed data</li>
<li>Requires one to know or construct a “subjective prior”</li>
<li>May be computationally intensive due to integration over many parameters</li>
</ul>
</div>
<div id="frequentist-inference-1" class="section level2">
<h2>Frequentist inference</h2>
<ul>
<li>Never uses or gives the probability of a hypothesis</li>
<li>Depends on the likelihood <span class="math inline">\(P(D|H)\)</span> for both observed and unobserved data</li>
<li>Does not require a prior</li>
<li>Tends to be less computationally intensive</li>
</ul>
</div>
</div>
<div id="critiques-and-defenses" class="section level1">
<h1>Critiques and defenses</h1>
<div id="critique-of-bayesian-inference" class="section level2">
<h2>Critique of Bayesian inference</h2>
<ol style="list-style-type: decimal">
<li>The subjective prior is subjective. There is no single method for choosing a prior, so different (well-intentioned) people will produce different priors and therefore arrive at different posteriors and conclusions.</li>
<li>Philosophically, some object to assigning probabilities to hypotheses as hypotheses do not constitute outcomes of repeatable experiments in which one can measure long-term frequency. Rather, a hypothesis is either true or false, regardless of whether one knows which is the case.
<ul>
<li>A coin is either fair or unfair</li>
<li>Treatment 1 is either better or worse than treatment 2</li>
<li>The sun will or will not come up tomorrow</li>
<li>I will either win or not win the lottery</li>
</ul></li>
</ol>
</div>
<div id="defense-of-bayesian-inference" class="section level2">
<h2>Defense of Bayesian inference</h2>
<ol style="list-style-type: decimal">
<li>The probability of hypotheses is exactly what we need to make decisions. When the doctor tells me a screening test came back positive for a disease, what I really want to know is the probability of the hypothesis “I’m sick”.</li>
<li>Bayes’ theorem is logically rigorous (once we obtain a prior).</li>
<li>By testing different priors we can see how sensitive our results are to the choice of prior.</li>
<li>It is easy to communicate a result framed in terms of probabilities of hypotheses (try explaining the result of a null hypothesis test to a layperson).</li>
<li>Priors can be defended based on the assumptions made to arrive at it.</li>
<li>Evidence derived from the data is independent of notions about “data more extreme” that depend on the exact experimental setup.</li>
<li>Data can be used as it comes in. We don’t have to wait for every contingency to be planned for ahead of time.</li>
</ol>
</div>
<div id="critique-of-frequentist-inference" class="section level2">
<h2>Critique of frequentist inference</h2>
<ol style="list-style-type: decimal">
<li>It is ad-hoc and does not carry the force of deductive logic. p-value depends on the exact experimental setup.</li>
<li>Experiments need to be fully specified ahead of time.</li>
<li><a href="persp016_p_hacking.html">p-values and significance levels are notoriously prone to misinterpretation.</a></li>
</ol>
</div>
<div id="defense-of-frequentist-inference" class="section level2">
<h2>Defense of frequentist inference</h2>
<ol style="list-style-type: decimal">
<li>It is objective - all statisticians will agree on the p-value.<a href="#fn5" class="footnoteRef" id="fnref5"><sup>5</sup></a> Any individual can then decide if the p-value warrants rejecting the null hypothesis.</li>
<li>The hypothesis testing process is applied in a consistent and rigorous fashion. The interpretation of the results is left to the user by balancing the concerns of type I and type II errors.</li>
<li>Frequentist experimental design demands a careful description of the experiment and methods of analysis before starting. This helps control for experimenter bias.</li>
<li>The frequentist approach has been used for over 100 years. There’s lots of support for it in the sciences.</li>
</ol>
</div>
<div id="p-value-test" class="section level2">
<h2>p-value test</h2>
<p>You run a two-sample <span class="math inline">\(t\)</span>-test for equal means, with <span class="math inline">\(\alpha = 0.05\)</span> and obtain a p-value of 0.04. What are the odds that the two samples are drawn from distributions with the same mean?</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(\frac{19}{1}\)</span></li>
<li><span class="math inline">\(\frac{1}{19}\)</span></li>
<li><span class="math inline">\(\frac{1}{20}\)</span></li>
<li><span class="math inline">\(\frac{1}{24}\)</span></li>
</ol>
<p>Wrong! The answer is <strong>we don’t know</strong>. Frequentist methods only give probabilities of statistics conditioned on hypotheses:</p>
<blockquote>
<p>With a p-value of less than 0.05, we reject the null hypothesis that the difference in means between the two samples is zero.</p>
</blockquote>
<p>They do not give probabilities of hypotheses:</p>
<blockquote>
<p>There is a 95% probability that the difference in means between the two samples falls between <span class="math inline">\([-.02, .03]\)</span>.</p>
</blockquote>
</div>
</div>
<div id="computing-bayesian-models" class="section level1">
<h1>Computing Bayesian models</h1>
<p>In more simplistic models based on well-known distributions such as the normal, gamma, beta, Poisson, etc., prior and posterior distributions can be computed analytically in closed form. That is, all the algebra and calculus can be solved by hand (or with minimal computational assistance). However in models with multiple parameters and multivariate probability distributions, the algebra becomes overwhelming. Instead, we can use computational methods such as Markov chain Monte Carlo (MCMC) and Gibbs sampling to integrate over the likelihood function or simulate draws from the distribution in question.</p>
<p>This is a major reason Bayesian inference is not more common in academia. These computational methods are far beyond the scope of a single lecture on Bayesian inference. Furthermore, computers were not powerful enough until the 1990s/2000s to actually solve the computational tasks necessary to conduct sophisticated Bayesian inference.</p>
</div>
<div id="acknowledgments" class="section level1 toc-ignore">
<h1>Acknowledgments</h1>
<ul>
<li><a href="https://ocw.mit.edu/courses/mathematics/18-05-introduction-to-probability-and-statistics-spring-2014/">Introduction to Probability and Statistics</a></li>
<li><a href="http://www.stat.columbia.edu/~gelman/book/">Gelman, Andrew, et al. <em>Bayesian data analysis.</em> Vol. 2. Boca Raton, FL, USA: Chapman &amp; Hall/CRC, 2014.</a></li>
</ul>
</div>
<div id="session-info" class="section level1 toc-ignore">
<h1>Session Info</h1>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">devtools::<span class="kw">session_info</span>()</code></pre></div>
<pre><code>##  setting  value                       
##  version  R version 3.3.3 (2017-03-06)
##  system   x86_64, darwin13.4.0        
##  ui       X11                         
##  language (EN)                        
##  collate  en_US.UTF-8                 
##  tz       America/Chicago             
##  date     2017-05-22                  
## 
##  package      * version    date      
##  assertthat     0.2.0      2017-04-11
##  backports      1.0.5      2017-01-18
##  base         * 3.3.3      2017-03-07
##  broom        * 0.4.2      2017-02-13
##  car          * 2.1-4      2016-12-02
##  cellranger     1.1.0      2016-07-27
##  coefplot     * 1.2.4.9000 2017-04-25
##  colorspace     1.3-2      2016-12-14
##  datasets     * 3.3.3      2017-03-07
##  DBI            0.6-1      2017-04-01
##  devtools       1.13.0     2017-05-08
##  digest         0.6.12     2017-01-27
##  dplyr        * 0.5.0      2016-06-24
##  evaluate       0.10       2016-10-11
##  forcats      * 0.2.0      2017-01-23
##  foreign        0.8-68     2017-04-24
##  ggplot2      * 2.2.1.9000 2017-05-12
##  graphics     * 3.3.3      2017-03-07
##  grDevices    * 3.3.3      2017-03-07
##  grid           3.3.3      2017-03-07
##  gtable         0.2.0      2016-02-26
##  haven          1.0.0      2016-09-23
##  hms            0.3        2016-11-22
##  htmltools      0.3.6      2017-04-28
##  httr           1.2.1      2016-07-03
##  jsonlite       1.4        2017-04-08
##  knitr          1.15.1     2016-11-22
##  lattice        0.20-35    2017-03-25
##  lazyeval       0.2.0      2016-06-12
##  lme4         * 1.1-13     2017-04-19
##  lubridate      1.6.0      2016-09-13
##  magrittr       1.5        2014-11-22
##  MASS           7.3-47     2017-04-21
##  Matrix       * 1.2-10     2017-04-28
##  MatrixModels   0.4-1      2015-08-22
##  memoise        1.1.0      2017-04-21
##  methods      * 3.3.3      2017-03-07
##  mgcv           1.8-17     2017-02-08
##  minqa          1.2.4      2014-10-09
##  mnormt         1.5-5      2016-10-15
##  modelr       * 0.1.0      2016-08-31
##  munsell        0.4.3      2016-02-13
##  nlme           3.1-131    2017-02-06
##  nloptr         1.0.4      2014-08-04
##  nnet           7.3-12     2016-02-02
##  parallel       3.3.3      2017-03-07
##  pbkrtest       0.4-7      2017-03-15
##  plyr           1.8.4      2016-06-08
##  psych          1.7.5      2017-05-03
##  purrr        * 0.2.2.2    2017-05-11
##  quantreg       5.33       2017-04-18
##  R6             2.2.1      2017-05-10
##  rcfss        * 0.1.4      2017-02-28
##  RColorBrewer * 1.1-2      2014-12-07
##  Rcpp           0.12.10    2017-03-19
##  readr        * 1.1.0      2017-03-22
##  readxl         1.0.0      2017-04-18
##  reshape2       1.4.2      2016-10-22
##  rlang          0.1.9000   2017-05-12
##  rmarkdown      1.5        2017-04-26
##  rprojroot      1.2        2017-01-16
##  rvest          0.3.2      2016-06-17
##  scales         0.4.1      2016-11-09
##  SparseM        1.77       2017-04-23
##  splines        3.3.3      2017-03-07
##  stats        * 3.3.3      2017-03-07
##  stringi        1.1.5      2017-04-07
##  stringr      * 1.2.0      2017-02-18
##  tibble       * 1.3.0.9002 2017-05-12
##  tidyr        * 0.6.2      2017-05-04
##  tidyverse    * 1.1.1      2017-01-27
##  tools          3.3.3      2017-03-07
##  useful         1.2.1      2016-06-29
##  utils        * 3.3.3      2017-03-07
##  withr          1.0.2      2016-06-20
##  xml2           1.1.1      2017-01-24
##  yaml           2.1.14     2016-11-12
##  source                               
##  cran (@0.2.0)                        
##  CRAN (R 3.3.2)                       
##  local                                
##  CRAN (R 3.3.2)                       
##  CRAN (R 3.3.2)                       
##  CRAN (R 3.3.0)                       
##  Github (jaredlander/coefplot@0755a00)
##  CRAN (R 3.3.2)                       
##  local                                
##  CRAN (R 3.3.2)                       
##  CRAN (R 3.3.2)                       
##  CRAN (R 3.3.2)                       
##  CRAN (R 3.3.0)                       
##  CRAN (R 3.3.0)                       
##  CRAN (R 3.3.2)                       
##  CRAN (R 3.3.2)                       
##  Github (tidyverse/ggplot2@f4398b6)   
##  local                                
##  local                                
##  local                                
##  CRAN (R 3.3.0)                       
##  cran (@1.0.0)                        
##  CRAN (R 3.3.2)                       
##  cran (@0.3.6)                        
##  CRAN (R 3.3.0)                       
##  cran (@1.4)                          
##  cran (@1.15.1)                       
##  CRAN (R 3.3.2)                       
##  CRAN (R 3.3.0)                       
##  cran (@1.1-13)                       
##  CRAN (R 3.3.0)                       
##  CRAN (R 3.3.0)                       
##  CRAN (R 3.3.2)                       
##  CRAN (R 3.3.2)                       
##  CRAN (R 3.3.0)                       
##  CRAN (R 3.3.2)                       
##  local                                
##  CRAN (R 3.3.3)                       
##  cran (@1.2.4)                        
##  CRAN (R 3.3.0)                       
##  CRAN (R 3.3.0)                       
##  CRAN (R 3.3.0)                       
##  CRAN (R 3.3.3)                       
##  cran (@1.0.4)                        
##  CRAN (R 3.3.3)                       
##  local                                
##  CRAN (R 3.3.2)                       
##  CRAN (R 3.3.0)                       
##  CRAN (R 3.3.3)                       
##  CRAN (R 3.3.3)                       
##  CRAN (R 3.3.2)                       
##  CRAN (R 3.3.2)                       
##  local                                
##  CRAN (R 3.3.0)                       
##  cran (@0.12.10)                      
##  cran (@1.1.0)                        
##  CRAN (R 3.3.2)                       
##  CRAN (R 3.3.0)                       
##  Github (hadley/rlang@c17568e)        
##  CRAN (R 3.3.2)                       
##  CRAN (R 3.3.2)                       
##  CRAN (R 3.3.0)                       
##  CRAN (R 3.3.1)                       
##  CRAN (R 3.3.2)                       
##  local                                
##  local                                
##  CRAN (R 3.3.2)                       
##  CRAN (R 3.3.2)                       
##  Github (tidyverse/tibble@9103a30)    
##  CRAN (R 3.3.2)                       
##  CRAN (R 3.3.2)                       
##  local                                
##  CRAN (R 3.3.0)                       
##  local                                
##  CRAN (R 3.3.0)                       
##  CRAN (R 3.3.2)                       
##  cran (@2.1.14)</code></pre>
</div>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>Even worse, many physicians substantially miss the correct answer to this question.<a href="#fnref1">↩</a></p></li>
<li id="fn2"><p>Based on historical records of just two accidents involving planes hitting buildings in New York City from the 1940s-9/10/2011.<a href="#fnref2">↩</a></p></li>
<li id="fn3"><p>Based on the uniform probability distribution.<a href="#fnref3">↩</a></p></li>
<li id="fn4"><p>And generally extend this to observational data that is not in practical terms repeatable, though in theory can be.<a href="#fnref4">↩</a></p></li>
<li id="fn5"><p>We hope.<a href="#fnref5">↩</a></p></li>
</ol>
</div>

<p>This work is licensed under the  <a href="http://creativecommons.org/licenses/by-nc/4.0/">CC BY-NC 4.0 Creative Commons License</a>.</p>


</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
