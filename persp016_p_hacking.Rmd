---
title: "p-hacking"
author: "MACS 30200 - Perspectives on Computational Research"
output:
  html_document:
    toc: true
    toc_float: true
    code_folding: hide
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(cache = TRUE,
                      message = FALSE,
                      warning = FALSE)
```

# Objectives

* Define a p-value
* Identify methods for obtaining a statistically significant p-value
* Critique the "women wearing pink" article for potential p-hacking
* Identify approaches to reduce the chance of p-hacking

```{r packages, cache = FALSE, message = FALSE, warning = FALSE}
library(tidyverse)
library(broom)
library(forcats)
library(modelr)
library(stringr)
library(car)
library(rcfss)
library(coefplot)
library(RColorBrewer)
library(lme4)

options(digits = 3)
set.seed(1234)
theme_set(theme_minimal())
```

# What is a p-value?

## Hypothesis testing

When conducting inference, we test hypotheses about real-world relationships using observed data. Because we generally rely on a **sample** rather than the **population**, there is a chance that the results we observe could be purely driven by random chance. We just happened to draw a fluky sample from the population and the statistic of interest we observe is not in fact representative of the broader population.

In the context of regression, we generally test the **null hypothesis** against the **alternative hypothesis**. The null hypothesis $H_0$ states "there is no relationship between $X$ and $Y$", whereas the alternative hypothesis $H_a$ states "there is some relationship between $X$ and $Y$". Mathematically, this corresponds to

* $H_0$: $\beta_1 = 0$
* $H_a$: $\beta_1 \neq 0$

since if $\beta_1 = 0$, then the model reduces to $Y = \beta_0 + \epsilon$ and $X$ is not associated with $Y$. To test the null hypothesis, we need to determine whether the estimated coefficient $\hat{\beta}_1$ is sufficiently far from zero that we can be confident that $\beta_1$ is non-zero.

This determination is based on the **standard error** of the coefficient. If $\text{SE}(\hat{\beta}_1)$ is small, than even relatively small values of $\hat{\beta}_1$ may provide strong evidence that $\beta_1 \neq 0$ and there is a relationship between $X$ and $Y$. If $\text{SE}(\hat{\beta}_1)$ is large, than $\hat{\beta}_1$ must also be large to reject the null hypothesis. In practice, we compute a **t-statistic** given by

$$t = \frac{\hat{\beta}_1 - 0}{\text{SE}(\hat{\beta}_1)}$$

which measures the number of standard deviations that $\hat{\beta}_1$ is away from 0. If there really is no relationship between $X$ and $Y$, then we expect this function to follow a $t$-distribution with $n-2$ degrees of freedom. From this we can calculate the probability of observing any value equal to $|t|$ or larger, assuming $\beta_1=0$. This is the **p-value**. Informally, a small p-value indicates that it is unlikely to observare such substantial association between the predictor and the response due to chance, in the absence of any real association between the predictor and response.

## The importance of the .05 cutoff

![](http://marginalrevolution.com/wp-content/uploads/2014/05/Type-I-and-II-errors1-625x468.jpg)

p-values can fall anywhere between 0 and 1. Setting the cutoff point determines how likely one is to make a type I (false positive) or type II (false negative) error. Historically, the social sciences adopted a cutoff of $p=.05$. Anything below that value is considered "statistically significant". But there is no scientific basis for this threshold - it is an arbitrary standard, to which many academic journals and scholars continue to adhere. 

## What p-values can and cannot do

p-values are a key component of frequentist inference, dating back to the early 1900s. While their usage is widespread in academia, in practice their meaning is actually very limited.

> A p-value is the probability under a specified statistical model that a statistical summary of the data would be equal to or more extreme than its observed values.

**p-values can indicate how incompatible the data are with a specified statistical model**. This is why they are used for null hypothesis testing. The smaller the p-value, the greater the statistical incompatability of the data with the null hypothesis. This incompatability casts doubt on or provides evidence agains the null hypothesis.

**But it does not prove the alternative hypothesis to be true.**

* p-values do not measure the probability that the observed relationship is true, or the probability that the data were produced by random chance alone. p-values cannot be used to prove the null hypothesis:

    ![](http://www.azquotes.com/picture-quotes/quote-the-absence-of-evidence-is-not-the-evidence-of-absence-carl-sagan-43-51-12.jpg)
    
    p-values do not provide any conclusion about the explanation itself, only the probability of the observed data being generated if the null hypothesis is true.
* p-values below an arbitrary threshold should not be used for scientific, business, or policy conclusions. Again, the .05 threshold is a completely arbitrary standard. A statistic with a p-value of $.04$ is not "true" versus a statistic with a p-value of $.06$ is false. p-values are determined not only by the data itself but also by the researcher's decisions used to formulate and estimate the statistical model, and there are many ways to tweak or search for a significant p-value.
* p-values and statistical significance do not measure the size or importance of a result. Statistical significance is not the same thing as scientific, human, or economic significance. Smaller p-values do not imply important effects. p-values are determined by the precision of the estimated statistic, which is in turn influenced by sample size and measurement accuracy. A statistic can have a very small p-value but have an insignificant substantive effect because of a large sample size. Likewise, even large effects can have unimpressive p-values if the sample size is small.
* The p-value does not provide a good measure of evidence regarding a model or hypothesis. Just because a p-value is large does not mean the null hypothesis is true; many other alternative hypotheses could exist to explain the observed data, you just did not test for it.

# How to find a significant p-value

```{r pval-sim}
n_obs <- 1000

pval_dist <- function(n_obs){
  x <- replicate(10, rnorm(n_obs))
  y <- rnorm(n_obs)
  
  mod <- lm(y ~ x[, sample(1:10, 1)])
  pval <- summary(mod)$coefficients[2, 4]
  
  return(data_frame(pval = pval))
}

pvals <- 1000 %>%
  rerun(pval_dist(n_obs)) %>%
  bind_rows %>%
  mutate(sig = pval < .05)

ggplot(pvals, aes(pval, fill = sig)) +
  geom_histogram(binwidth = .025, boundary = 0) +
  labs(title = "Distribution of p-values when null is true",
       x = expression(P),
       y = NULL) +
  theme(legend.position = "none")
```

```{r pval-sim-mult-test}
pval_dist_mult <- function(n_obs){
  # generate simulated data
  x <- replicate(10, rnorm(n_obs))
  y <- rnorm(n_obs)
  
  # estimate a linear model for each column in x and record the pvalue
  pvals <- vector("double", ncol(x))
  
  for(i in seq_len(ncol(x))) {
    mod <- lm(y ~ x[, i])
    pvals[[i]] <- summary(mod)$coefficients[2, 4]
  }
  
  # keep mimimum pvalue
  return(data_frame(pval = min(pvals)))
}

pvals_mult <- 1000 %>%
  rerun(pval_dist_mult(n_obs)) %>%
  bind_rows %>%
  mutate(sig = pval < .05)

ggplot(pvals_mult, aes(pval, fill = sig)) +
  geom_histogram(binwidth = .025, boundary = 0) +
  labs(title = "Distribution of minimmum p-values for 10 test when null is true",
       x = expression(P),
       y = NULL) +
  theme(legend.position = "none")
```

# Acknowledgments {.toc-ignore}

# Session Info {.toc-ignore}

```{r child='_sessioninfo.Rmd'}
```




