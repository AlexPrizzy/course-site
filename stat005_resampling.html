<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />




<title>Statistical learning: resampling methods</title>

<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/readable.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="site_libs/jqueryui-1.11.4/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<script src="site_libs/htmlwidgets-0.8/htmlwidgets.js"></script>
<script src="site_libs/d3-3.5.6/d3.min.js"></script>
<link href="site_libs/profvis-0.3.2/profvis.css" rel="stylesheet" />
<script src="site_libs/profvis-0.3.2/profvis.js"></script>
<link href="site_libs/highlight-6.2.0/textmate.css" rel="stylesheet" />
<script src="site_libs/highlight-6.2.0/highlight.js"></script>
<script src="site_libs/profvis-binding-0.3.3/profvis.js"></script>
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-45631879-2', 'auto');
  ga('send', 'pageview');

</script>


<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>


<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>


</head>

<body>

<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
.tabbed-pane {
  padding-top: 12px;
}
button.code-folding-btn:focus {
  outline: none;
}
</style>


<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 66px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 71px;
  margin-top: -71px;
}

.section h2 {
  padding-top: 71px;
  margin-top: -71px;
}
.section h3 {
  padding-top: 71px;
  margin-top: -71px;
}
.section h4 {
  padding-top: 71px;
  margin-top: -71px;
}
.section h5 {
  padding-top: 71px;
  margin-top: -71px;
}
.section h6 {
  padding-top: 71px;
  margin-top: -71px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>


<div class="container-fluid main-container">

<!-- tabsets -->
<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});
</script>

<!-- code folding -->




<script>
$(document).ready(function ()  {

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_').toLowerCase();
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}


.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
  padding-left: 25px;
  text-indent: 0;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>

<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row-fluid">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">Computing for the Social Sciences</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">Home</a>
</li>
<li>
  <a href="faq.html">FAQ</a>
</li>
<li>
  <a href="syllabus.html">Syllabus</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div class="fluid-row" id="header">



<h1 class="title toc-ignore">Statistical learning: resampling methods</h1>

</div>


<div id="objectives" class="section level1">
<h1>Objectives</h1>
<ul>
<li>Define resampling methods</li>
<li>Compare and contrast the validation set approach with leave-one-out and <span class="math inline">\(k\)</span>-fold cross-validation</li>
<li>Define bootstrapping and explain when it can be used in research</li>
<li>Demonstrate how to conduct cross-validation and bootstrapping using <code>modelr</code></li>
</ul>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(tidyverse)
<span class="kw">library</span>(modelr)
<span class="kw">library</span>(broom)
<span class="kw">set.seed</span>(<span class="dv">1234</span>)

<span class="kw">theme_set</span>(<span class="kw">theme_minimal</span>())</code></pre></div>
</div>
<div id="resampling-methods" class="section level1">
<h1>Resampling methods</h1>
<p>Resampling methods are essential to test and evaluate statistical models. Because you likely do not have the resources or capabilities to repeatedly sample from your population of interest, instead you can repeatedly draw from your original sample to obtain additional information about your model. For instance, you could repeatedly draw samples from your data, estimate a linear regression model on each sample, and then examine how the estimated model differs across each sample. This allows you to assess the variability and stability of your model in a way not possible if you can only fit the model once.</p>
</div>
<div id="validation-set" class="section level1">
<h1>Validation set</h1>
<p>We have already seen the <strong>validation set</strong> approach in the <a href="stat003_logistic_regression.html">previous class</a>. By splitting our data into a <strong>training set</strong> and <strong>test set</strong>, we can evaluate the model’s effectiveness at predicting the response variable (in the context of either regression or classification) independently of the data used to estimate the model in the first place.</p>
<div id="classification" class="section level2">
<h2>Classification</h2>
<p>Recall how we used this approach to evaluate the accuracy of our <a href="stat003_logistic_regression.html#interactive_terms">interactive model predicting survival during the sinking of the Titanic</a>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(titanic)
titanic &lt;-<span class="st"> </span>titanic_train <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">as_tibble</span>()

titanic <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">head</span>() <span class="op">%&gt;%</span>
<span class="st">  </span>knitr<span class="op">::</span><span class="kw">kable</span>()</code></pre></div>
<table>
<thead>
<tr class="header">
<th align="right">PassengerId</th>
<th align="right">Survived</th>
<th align="right">Pclass</th>
<th align="left">Name</th>
<th align="left">Sex</th>
<th align="right">Age</th>
<th align="right">SibSp</th>
<th align="right">Parch</th>
<th align="left">Ticket</th>
<th align="right">Fare</th>
<th align="left">Cabin</th>
<th align="left">Embarked</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">1</td>
<td align="right">0</td>
<td align="right">3</td>
<td align="left">Braund, Mr. Owen Harris</td>
<td align="left">male</td>
<td align="right">22</td>
<td align="right">1</td>
<td align="right">0</td>
<td align="left">A/5 21171</td>
<td align="right">7.2500</td>
<td align="left"></td>
<td align="left">S</td>
</tr>
<tr class="even">
<td align="right">2</td>
<td align="right">1</td>
<td align="right">1</td>
<td align="left">Cumings, Mrs. John Bradley (Florence Briggs Thayer)</td>
<td align="left">female</td>
<td align="right">38</td>
<td align="right">1</td>
<td align="right">0</td>
<td align="left">PC 17599</td>
<td align="right">71.2833</td>
<td align="left">C85</td>
<td align="left">C</td>
</tr>
<tr class="odd">
<td align="right">3</td>
<td align="right">1</td>
<td align="right">3</td>
<td align="left">Heikkinen, Miss. Laina</td>
<td align="left">female</td>
<td align="right">26</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="left">STON/O2. 3101282</td>
<td align="right">7.9250</td>
<td align="left"></td>
<td align="left">S</td>
</tr>
<tr class="even">
<td align="right">4</td>
<td align="right">1</td>
<td align="right">1</td>
<td align="left">Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>
<td align="left">female</td>
<td align="right">35</td>
<td align="right">1</td>
<td align="right">0</td>
<td align="left">113803</td>
<td align="right">53.1000</td>
<td align="left">C123</td>
<td align="left">S</td>
</tr>
<tr class="odd">
<td align="right">5</td>
<td align="right">0</td>
<td align="right">3</td>
<td align="left">Allen, Mr. William Henry</td>
<td align="left">male</td>
<td align="right">35</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="left">373450</td>
<td align="right">8.0500</td>
<td align="left"></td>
<td align="left">S</td>
</tr>
<tr class="even">
<td align="right">6</td>
<td align="right">0</td>
<td align="right">3</td>
<td align="left">Moran, Mr. James</td>
<td align="left">male</td>
<td align="right">NA</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="left">330877</td>
<td align="right">8.4583</td>
<td align="left"></td>
<td align="left">Q</td>
</tr>
</tbody>
</table>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">survive_age_woman_x &lt;-<span class="st"> </span><span class="kw">glm</span>(Survived <span class="op">~</span><span class="st"> </span>Age <span class="op">*</span><span class="st"> </span>Sex, <span class="dt">data =</span> titanic,
                           <span class="dt">family =</span> binomial)
<span class="kw">summary</span>(survive_age_woman_x)</code></pre></div>
<pre><code>## 
## Call:
## glm(formula = Survived ~ Age * Sex, family = binomial, data = titanic)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -1.9401  -0.7136  -0.5883   0.7626   2.2455  
## 
## Coefficients:
##             Estimate Std. Error z value Pr(&gt;|z|)   
## (Intercept)  0.59380    0.31032   1.913  0.05569 . 
## Age          0.01970    0.01057   1.863  0.06240 . 
## Sexmale     -1.31775    0.40842  -3.226  0.00125 **
## Age:Sexmale -0.04112    0.01355  -3.034  0.00241 **
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 964.52  on 713  degrees of freedom
## Residual deviance: 740.40  on 710  degrees of freedom
##   (177 observations deleted due to missingness)
## AIC: 748.4
## 
## Number of Fisher Scoring iterations: 4</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">logit2prob &lt;-<span class="st"> </span><span class="cf">function</span>(x){
  <span class="kw">exp</span>(x) <span class="op">/</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">+</span><span class="st"> </span><span class="kw">exp</span>(x))
}</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(modelr)

titanic_split &lt;-<span class="st"> </span><span class="kw">resample_partition</span>(titanic, <span class="kw">c</span>(<span class="dt">test =</span> <span class="fl">0.3</span>, <span class="dt">train =</span> <span class="fl">0.7</span>))
<span class="kw">map</span>(titanic_split, dim)</code></pre></div>
<pre><code>## $test
## [1] 267  12
## 
## $train
## [1] 624  12</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">train_model &lt;-<span class="st"> </span><span class="kw">glm</span>(Survived <span class="op">~</span><span class="st"> </span>Age <span class="op">+</span><span class="st"> </span>Sex, <span class="dt">data =</span> titanic_split<span class="op">$</span>train,
                   <span class="dt">family =</span> binomial)
<span class="kw">summary</span>(train_model)</code></pre></div>
<pre><code>## 
## Call:
## glm(formula = Survived ~ Age + Sex, family = binomial, data = titanic_split$train)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -1.7885  -0.6857  -0.6772   0.6837   1.8007  
## 
## Coefficients:
##              Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept)  1.376270   0.284458   4.838 1.31e-06 ***
## Age         -0.001307   0.007671  -0.170    0.865    
## Sexmale     -2.672766   0.228740 -11.685  &lt; 2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 687.03  on 504  degrees of freedom
## Residual deviance: 516.27  on 502  degrees of freedom
##   (119 observations deleted due to missingness)
## AIC: 522.27
## 
## Number of Fisher Scoring iterations: 4</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">x_test_accuracy &lt;-<span class="st"> </span>titanic_split<span class="op">$</span>test <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">tbl_df</span>() <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">add_predictions</span>(train_model) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">pred =</span> <span class="kw">logit2prob</span>(pred),
         <span class="dt">pred =</span> <span class="kw">as.numeric</span>(pred <span class="op">&gt;</span><span class="st"> </span>.<span class="dv">5</span>))

<span class="kw">mean</span>(x_test_accuracy<span class="op">$</span>Survived <span class="op">==</span><span class="st"> </span>x_test_accuracy<span class="op">$</span>pred, <span class="dt">na.rm =</span> <span class="ot">TRUE</span>)</code></pre></div>
<pre><code>## [1] 0.7511962</code></pre>
</div>
<div id="regression" class="section level2">
<h2>Regression</h2>
<p>This method also works for regression analysis. Here we will examine the relationship between horsepower and car mileage in the <code>Auto</code> dataset (found in <code>library(ISLR)</code>):</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(ISLR)

Auto &lt;-<span class="st"> </span>Auto <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">tbl_df</span>()
Auto</code></pre></div>
<pre><code>## # A tibble: 392 × 9
##      mpg cylinders displacement horsepower weight acceleration  year
## *  &lt;dbl&gt;     &lt;dbl&gt;        &lt;dbl&gt;      &lt;dbl&gt;  &lt;dbl&gt;        &lt;dbl&gt; &lt;dbl&gt;
## 1     18         8          307        130   3504         12.0    70
## 2     15         8          350        165   3693         11.5    70
## 3     18         8          318        150   3436         11.0    70
## 4     16         8          304        150   3433         12.0    70
## 5     17         8          302        140   3449         10.5    70
## 6     15         8          429        198   4341         10.0    70
## 7     14         8          454        220   4354          9.0    70
## 8     14         8          440        215   4312          8.5    70
## 9     14         8          455        225   4425         10.0    70
## 10    15         8          390        190   3850          8.5    70
## # ... with 382 more rows, and 2 more variables: origin &lt;dbl&gt;, name &lt;fctr&gt;</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>(Auto, <span class="kw">aes</span>(horsepower, mpg)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_point</span>()</code></pre></div>
<p><img src="stat005_resampling_files/figure-html/auto_plot-1.png" width="672" /></p>
<p>The relationship does not appear to be strictly linear:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>(Auto, <span class="kw">aes</span>(horsepower, mpg)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_point</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_smooth</span>(<span class="dt">method =</span> <span class="st">&quot;lm&quot;</span>, <span class="dt">se =</span> <span class="ot">FALSE</span>)</code></pre></div>
<p><img src="stat005_resampling_files/figure-html/auto_plot_lm-1.png" width="672" /></p>
<p>Perhaps by adding <a href="stat003_logistic_regression.html#quadratic_terms">quadratic terms</a> to the linear regression we could improve overall model fit. To evaluate the model, we will split the data into a training set and test set, estimate a series of higher-order models, and calculate a test statistic summarizing the accuracy of the estimated <code>mpg</code>. Rather than relying on the raw error rate (which makes sense in a classification model), we will instead use <strong>Mean Squared Error</strong> (MSE), defined as</p>
<p><span class="math display">\[MSE = \frac{1}{n} \sum_{i = 1}^{n}{(y_i - \hat{f}(x_i))^2}\]</span></p>
<p>where:</p>
<ul>
<li><span class="math inline">\(y_i =\)</span> the observed response value for the <span class="math inline">\(i\)</span>th observation</li>
<li><span class="math inline">\(\hat{f}(x_i) =\)</span> the predicted response value for the <span class="math inline">\(i\)</span>th observation given by <span class="math inline">\(\hat{f}\)</span></li>
<li><span class="math inline">\(n =\)</span> the total number of observations</li>
</ul>
<p>Boo math! Actually this is pretty intuitive. All we’re doing is for each observation, calculating the difference between the actual and predicted values for <span class="math inline">\(y\)</span>, squaring that difference, then calculating the average across all observations. An MSE of 0 indicates the model perfectly predicted each observation. The larger the MSE, the more error in the model.</p>
<p>For this task, first we can use <code>modelr::resample_partition()</code> to create training and test sets (using a 50/50 split), then estimate a linear regression model without any quadratic terms.</p>
<ul>
<li>I use <code>set.seed()</code> in the beginning - whenever you are writing a script that involves randomization (here, random subsetting of the data), always set the seed at the beginning of the script. This ensures the results can be reproduced precisely.<a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a></li>
<li>I also use the <code>glm()</code> function rather than <code>lm()</code> - if you don’t change the <code>family</code> parameter, the results of <code>lm()</code> and <code>glm()</code> are exactly the same.<a href="#fn2" class="footnoteRef" id="fnref2"><sup>2</sup></a></li>
</ul>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">1234</span>)

auto_split &lt;-<span class="st"> </span><span class="kw">resample_partition</span>(Auto, <span class="kw">c</span>(<span class="dt">test =</span> <span class="fl">0.5</span>, <span class="dt">train =</span> <span class="fl">0.5</span>))
auto_train &lt;-<span class="st"> </span>auto_split<span class="op">$</span>train <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">tbl_df</span>()
auto_test &lt;-<span class="st"> </span>auto_split<span class="op">$</span>test <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">tbl_df</span>()</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">auto_lm &lt;-<span class="st"> </span><span class="kw">glm</span>(mpg <span class="op">~</span><span class="st"> </span>horsepower, <span class="dt">data =</span> auto_train)
<span class="kw">summary</span>(auto_lm)</code></pre></div>
<pre><code>## 
## Call:
## glm(formula = mpg ~ horsepower, data = auto_train)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -12.892   -2.864   -0.545    2.793   13.298  
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 38.005404   0.921129   41.26   &lt;2e-16 ***
## horsepower  -0.140459   0.007968  -17.63   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for gaussian family taken to be 20.48452)
## 
##     Null deviance: 10359.4  on 196  degrees of freedom
## Residual deviance:  3994.5  on 195  degrees of freedom
## AIC: 1157.9
## 
## Number of Fisher Scoring iterations: 2</code></pre>
<p>To estimate the MSE for a single partition (i.e. for a training or test set), I wrote a special function <code>mse()</code>:<a href="#fn3" class="footnoteRef" id="fnref3"><sup>3</sup></a></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">mse &lt;-<span class="st"> </span><span class="cf">function</span>(model, data) {
  x &lt;-<span class="st"> </span>modelr<span class="op">:::</span><span class="kw">residuals</span>(model, data)
  <span class="kw">mean</span>(x <span class="op">^</span><span class="st"> </span><span class="dv">2</span>, <span class="dt">na.rm =</span> <span class="ot">TRUE</span>)
}</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">mse</span>(auto_lm, auto_test)</code></pre></div>
<pre><code>## [1] 28.57255</code></pre>
<p>For a strictly linear model, the MSE for the test set is 28.57. How does this compare to a quadratic model? We can use the <code>poly()</code> function in conjunction with a <code>map()</code> iteration to estimate the MSE for a series of models with higher-order polynomial terms:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">auto_poly_results &lt;-<span class="st"> </span><span class="kw">data_frame</span>(<span class="dt">terms =</span> <span class="dv">1</span><span class="op">:</span><span class="dv">5</span>,
           <span class="dt">model =</span> <span class="kw">map</span>(terms, <span class="op">~</span><span class="st"> </span><span class="kw">glm</span>(mpg <span class="op">~</span><span class="st"> </span><span class="kw">poly</span>(horsepower, .), <span class="dt">data =</span> auto_train)),
           <span class="dt">MSE =</span> <span class="kw">map_dbl</span>(model, mse, <span class="dt">data =</span> auto_test))

<span class="kw">ggplot</span>(auto_poly_results, <span class="kw">aes</span>(terms, MSE)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_line</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="st">&quot;Comparing quadratic linear models&quot;</span>,
       <span class="dt">subtitle =</span> <span class="st">&quot;Using validation set&quot;</span>,
       <span class="dt">x =</span> <span class="st">&quot;Highest-order polynomial&quot;</span>,
       <span class="dt">y =</span> <span class="st">&quot;Mean Squared Error&quot;</span>)</code></pre></div>
<p><img src="stat005_resampling_files/figure-html/mse_poly-1.png" width="672" /></p>
<p>Based on the MSE for the validation (test) set, a polynomial model with a quadratic term (<span class="math inline">\(\text{horsepower}^2\)</span>) produces the lowest average error. Adding cubic or higher-order terms is just not necessary.</p>
</div>
<div id="drawbacks-to-validation-sets" class="section level2">
<h2>Drawbacks to validation sets</h2>
<p>There are two main problems with validation sets:</p>
<ol style="list-style-type: decimal">
<li><p>Validation estimates of the test error rates can be highly variable depending on which observations are sampled into the training and test sets. See what happens if we repeat the sampling, estimation, and validation procedure for the <code>Auto</code> data set:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">mse_variable &lt;-<span class="st"> </span><span class="cf">function</span>(Auto){
  auto_split &lt;-<span class="st"> </span><span class="kw">resample_partition</span>(Auto, <span class="kw">c</span>(<span class="dt">test =</span> <span class="fl">0.5</span>, <span class="dt">train =</span> <span class="fl">0.5</span>))
  auto_train &lt;-<span class="st"> </span>auto_split<span class="op">$</span>train <span class="op">%&gt;%</span>
<span class="st">    </span><span class="kw">tbl_df</span>()
  auto_test &lt;-<span class="st"> </span>auto_split<span class="op">$</span>test <span class="op">%&gt;%</span>
<span class="st">    </span><span class="kw">tbl_df</span>()

  results &lt;-<span class="st"> </span><span class="kw">data_frame</span>(<span class="dt">terms =</span> <span class="dv">1</span><span class="op">:</span><span class="dv">5</span>,
                        <span class="dt">model =</span> <span class="kw">map</span>(terms,
                                    <span class="op">~</span><span class="st"> </span><span class="kw">glm</span>(mpg <span class="op">~</span><span class="st"> </span><span class="kw">poly</span>(horsepower, .),
                                          <span class="dt">data =</span> auto_train)),
                        <span class="dt">MSE =</span> <span class="kw">map_dbl</span>(model, mse, <span class="dt">data =</span> auto_test))

  <span class="kw">return</span>(results)
}

<span class="kw">rerun</span>(<span class="dv">10</span>, <span class="kw">mse_variable</span>(Auto)) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">bind_rows</span>(<span class="dt">.id =</span> <span class="st">&quot;id&quot;</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(terms, MSE, <span class="dt">color =</span> id)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_line</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="st">&quot;Variability of MSE estimates&quot;</span>,
       <span class="dt">subtitle =</span> <span class="st">&quot;Using the validation set approach&quot;</span>,
       <span class="dt">x =</span> <span class="st">&quot;Degree of Polynomial&quot;</span>,
       <span class="dt">y =</span> <span class="st">&quot;Mean Squared Error&quot;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">theme</span>(<span class="dt">legend.position =</span> <span class="st">&quot;none&quot;</span>)</code></pre></div>
<p><img src="stat005_resampling_files/figure-html/auto_variable_mse-1.png" width="672" /></p>
<p>Depending on the specific training/test split, our MSE varies by up to 5.</p></li>
<li><p>If you don’t have a large data set, you’ll have to dramatically shrink the size of your training set. Most statistical learning methods perform better with more observations - if you don’t have enough data in the training set, you might overestimate the error rate in the test set.</p></li>
</ol>
</div>
</div>
<div id="leave-one-out-cross-validation" class="section level1">
<h1>Leave-one-out cross-validation</h1>
<p>An alternative method is <strong>leave-one-out cross validation</strong> (LOOCV). Like with the validation set approach, you split the data into two parts. However the difference is that you only remove one observation for the test set, and keep all remaining observations in the training set. The statistical learning method is fit on the <span class="math inline">\(n-1\)</span> training set. You then use the held-out observation to calculate the <span class="math inline">\(MSE = (y_1 - \hat{y}_1)^2\)</span> which should be an unbiased estimator of the test error. Because this MSE is highly dependent on which observation is held out, <strong>we repeat this process for every single observation in the data set</strong>. Mathematically, this looks like:</p>
<p><span class="math display">\[CV_{(n)} = \frac{1}{n} \sum_{i = 1}^{n}{MSE_i}\]</span></p>
<p>This method produces estimates of the error rate that have minimal bias and are relatively steady (i.e. non-varying), unlike the validation set approach where the MSE estimate is highly dependent on the sampling process for training/test sets. LOOCV is also highly flexible and works with any kind of predictive modeling.</p>
<p>Of course the downside is that this method is computationally difficult. You have to estimate <span class="math inline">\(n\)</span> different models - if you have a large <span class="math inline">\(n\)</span> or each individual model takes a long time to compute, you may be stuck waiting a long time for the computer to finish its calculations.</p>
<div id="loocv-in-linear-regression" class="section level2">
<h2>LOOCV in linear regression</h2>
<p>We can use the <code>crossv_kfold()</code> function in the <code>modelr</code> library to compute the LOOCV of any linear or logistic regression model. It takes two arguments: the data frame and the number of <span class="math inline">\(k\)</span>-folds (which we will define shortly). For our purposes now, all you need to know is that <code>k</code> should equal the number of observations in the data frame which we can retrieve using the <code>nrow()</code> function. For the <code>Auto</code> dataset, this looks like:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">loocv_data &lt;-<span class="st"> </span><span class="kw">crossv_kfold</span>(Auto, <span class="dt">k =</span> <span class="kw">nrow</span>(Auto))</code></pre></div>
<p>Now we estimate the linear model <span class="math inline">\(k\)</span> times, excluding the holdout test observation, then calculate the MSE:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">loocv_models &lt;-<span class="st"> </span><span class="kw">map</span>(loocv_data<span class="op">$</span>train, <span class="op">~</span><span class="st"> </span><span class="kw">lm</span>(mpg <span class="op">~</span><span class="st"> </span>horsepower, <span class="dt">data =</span> .))
loocv_mse &lt;-<span class="st"> </span><span class="kw">map2_dbl</span>(loocv_models, loocv_data<span class="op">$</span>test, mse)
<span class="kw">mean</span>(loocv_mse)</code></pre></div>
<pre><code>## [1] 24.23151</code></pre>
<p>The results of the mapped <code>mse()</code> function is the MSE for each iteration through the data, so there is one MSE for each observation. Calculating the <code>mean()</code> of that vector gives us the LOOCV MSE.</p>
<p>We can also use this method to compare the optimal number of polynomial terms as before.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">cv_error &lt;-<span class="st"> </span><span class="kw">vector</span>(<span class="st">&quot;numeric&quot;</span>, <span class="dv">5</span>)
terms &lt;-<span class="st"> </span><span class="dv">1</span><span class="op">:</span><span class="dv">5</span>

<span class="cf">for</span>(i <span class="cf">in</span> terms){
  loocv_models &lt;-<span class="st"> </span><span class="kw">map</span>(loocv_data<span class="op">$</span>train, <span class="op">~</span><span class="st"> </span><span class="kw">lm</span>(mpg <span class="op">~</span><span class="st"> </span><span class="kw">poly</span>(horsepower, i), <span class="dt">data =</span> .))
  loocv_mse &lt;-<span class="st"> </span><span class="kw">map2_dbl</span>(loocv_models, loocv_data<span class="op">$</span>test, mse)
  cv_error[[i]] &lt;-<span class="st"> </span><span class="kw">mean</span>(loocv_mse)
}

cv_mse &lt;-<span class="st"> </span><span class="kw">data_frame</span>(<span class="dt">terms =</span> terms,
           <span class="dt">cv_MSE =</span> cv_error)
cv_mse</code></pre></div>
<pre><code>## # A tibble: 5 × 2
##   terms   cv_MSE
##   &lt;int&gt;    &lt;dbl&gt;
## 1     1 24.23151
## 2     2 19.24821
## 3     3 19.33498
## 4     4 19.42443
## 5     5 19.03321</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>(cv_mse, <span class="kw">aes</span>(terms, cv_MSE)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_line</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="st">&quot;Comparing quadratic linear models&quot;</span>,
       <span class="dt">subtitle =</span> <span class="st">&quot;Using LOOCV&quot;</span>,
       <span class="dt">x =</span> <span class="st">&quot;Highest-order polynomial&quot;</span>,
       <span class="dt">y =</span> <span class="st">&quot;Mean Squared Error&quot;</span>)</code></pre></div>
<p><img src="stat005_resampling_files/figure-html/loocv_poly-1.png" width="672" /></p>
<p>And arrive at a similar conclusion. There may be a very marginal advantage to adding a fifth-order polynomial, but not substantial enough for the additional complexity over a mere second-order polynomial.</p>
</div>
<div id="loocv-in-classification" class="section level2">
<h2>LOOCV in classification</h2>
<p>Let’s use classification to validate the interactive terms model from before. For technical reasons, we need to use a custom <code>mse.glm()</code> function to properly calculate the MSE for binary response variables:<a href="#fn4" class="footnoteRef" id="fnref4"><sup>4</sup></a></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">mse.glm &lt;-<span class="st"> </span><span class="cf">function</span> (model, data){
  residuals.glm &lt;-<span class="st"> </span><span class="cf">function</span>(model, data) {
    modelr<span class="op">:::</span><span class="kw">response</span>(model, data) <span class="op">-</span><span class="st"> </span>stats<span class="op">::</span><span class="kw">predict</span>(model, data, <span class="dt">type =</span> <span class="st">&quot;response&quot;</span>)
  }
  
  x &lt;-<span class="st"> </span><span class="kw">residuals</span>(model, data)
  <span class="kw">mean</span>(x<span class="op">^</span><span class="dv">2</span>, <span class="dt">na.rm =</span> <span class="ot">TRUE</span>)
}</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">titanic_loocv &lt;-<span class="st"> </span><span class="kw">crossv_kfold</span>(titanic, <span class="dt">k =</span> <span class="kw">nrow</span>(titanic))
titanic_models &lt;-<span class="st"> </span><span class="kw">map</span>(titanic_loocv<span class="op">$</span>train, <span class="op">~</span><span class="st"> </span><span class="kw">glm</span>(Survived <span class="op">~</span><span class="st"> </span>Age <span class="op">*</span><span class="st"> </span>Sex, <span class="dt">data =</span> .,
                                               <span class="dt">family =</span> binomial))
titanic_mse &lt;-<span class="st"> </span><span class="kw">map2_dbl</span>(titanic_models, titanic_loocv<span class="op">$</span>test, mse.glm)
<span class="kw">mean</span>(titanic_mse, <span class="dt">na.rm =</span> <span class="ot">TRUE</span>)</code></pre></div>
<pre><code>## [1] 0.1703518</code></pre>
<p>In a classification problem, the LOOCV tells us the average error rate based on our predictions. So here, it tells us that the interactive <code>Age * Sex</code> model has a 17% error rate, or 83%. This is similar to the validation set result (<span class="math inline">\(75.1\%\)</span>)</p>
</div>
<div id="exercise-loocv-in-linear-regression" class="section level2">
<h2>Exercise: LOOCV in linear regression</h2>
<ol style="list-style-type: decimal">
<li><p>Estimate the LOOCV MSE of a linear regression of the relationship between admission rate and cost in the <a href="stat002_linear_models.html#exercise:_linear_regression_with_scorecard"><code>scorecard</code> dataset</a>.</p>
<details> <summary>Click for the solution</summary>
<p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(rcfss)

scorecard_loocv &lt;-<span class="st"> </span><span class="kw">crossv_kfold</span>(scorecard, <span class="dt">k =</span> <span class="kw">nrow</span>(scorecard)) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">model =</span> <span class="kw">map</span>(train, <span class="op">~</span><span class="st"> </span><span class="kw">lm</span>(cost <span class="op">~</span><span class="st"> </span>admrate, <span class="dt">data =</span> .)),
         <span class="dt">mse =</span> <span class="kw">map2_dbl</span>(model, test, mse))
<span class="kw">mean</span>(scorecard_loocv<span class="op">$</span>mse, <span class="dt">na.rm =</span> <span class="ot">TRUE</span>)</code></pre></div>
<pre><code>## [1] 147752431</code></pre>
</p>
<p></details></p></li>
<li><p>Estimate the LOOCV MSE of a <a href="stat003_logistic_regression.html#exercise:_logistic_regression_with_mental_health">logistic regression model of voter turnout</a> using only <code>mhealth</code> as the predictor. Compare this to the LOOCV MSE of a logistic regression model using all available predictors. Which is the better model?</p>
<details> <summary>Click for the solution</summary>
<p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># basic model</span>
mh_loocv_lite &lt;-<span class="st"> </span><span class="kw">crossv_kfold</span>(mental_health, <span class="dt">k =</span> <span class="kw">nrow</span>(mental_health)) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">model =</span> <span class="kw">map</span>(train, <span class="op">~</span><span class="st"> </span><span class="kw">glm</span>(vote96 <span class="op">~</span><span class="st"> </span>mhealth, <span class="dt">data =</span> .,
                                  <span class="dt">family =</span> binomial)),
         <span class="dt">mse =</span> <span class="kw">map2_dbl</span>(model, test, mse.glm))
<span class="kw">mean</span>(mh_loocv_lite<span class="op">$</span>mse, <span class="dt">na.rm =</span> <span class="ot">TRUE</span>)</code></pre></div>
<pre><code>## [1] 0.2092831</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># full model</span>
mh_loocv_full &lt;-<span class="st"> </span><span class="kw">crossv_kfold</span>(mental_health, <span class="dt">k =</span> <span class="kw">nrow</span>(mental_health)) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">model =</span> <span class="kw">map</span>(train, <span class="op">~</span><span class="st"> </span><span class="kw">glm</span>(vote96 <span class="op">~</span><span class="st"> </span>., <span class="dt">data =</span> .,
                                  <span class="dt">family =</span> binomial)),
         <span class="dt">mse =</span> <span class="kw">map2_dbl</span>(model, test, mse.glm))
<span class="kw">mean</span>(mh_loocv_full<span class="op">$</span>mse, <span class="dt">na.rm =</span> <span class="ot">TRUE</span>)</code></pre></div>
<pre><code>## [1] 0.1824597</code></pre>
<p>The full model is better and has a lower error rate.</p>
</p>
<p></details></p></li>
</ol>
</div>
</div>
<div id="k-fold-cross-validation" class="section level1">
<h1>k-fold cross-validation</h1>
<p>A less computationally-intensive approach to cross validation is <strong><span class="math inline">\(k\)</span>-fold cross-validation</strong>. Rather than dividing the data into <span class="math inline">\(n\)</span> groups, one divides the observations into <span class="math inline">\(k\)</span> groups, or <strong>folds</strong>, of approximately equal size. The first fold is treated as the validation set, and the model is estimated on the remaining <span class="math inline">\(k-1\)</span> folds. This process is repeated <span class="math inline">\(k\)</span> times, with each fold serving as the validation set precisely once. The <span class="math inline">\(k\)</span>-fold CV estimate is calculated by averaging the MSE values for each fold:</p>
<p><span class="math display">\[CV_{(k)} = \frac{1}{k} \sum_{i = 1}^{k}{MSE_i}\]</span></p>
<p>As you probably figured out by now, LOOCV is the special case of <span class="math inline">\(k\)</span>-fold cross-validation where <span class="math inline">\(k = n\)</span>. More typically researchers will use <span class="math inline">\(k=5\)</span> or <span class="math inline">\(k=10\)</span> depending on the size of the data set and the complexity of the statistical model.</p>
<div id="k-fold-cv-in-linear-regression" class="section level2">
<h2>k-fold CV in linear regression</h2>
<p>Let’s go back to the <code>Auto</code> data set. Instead of LOOCV, let’s use 10-fold CV to compare the different polynomial models.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">cv10_data &lt;-<span class="st"> </span><span class="kw">crossv_kfold</span>(Auto, <span class="dt">k =</span> <span class="dv">10</span>)

cv_error_fold10 &lt;-<span class="st"> </span><span class="kw">vector</span>(<span class="st">&quot;numeric&quot;</span>, <span class="dv">5</span>)
terms &lt;-<span class="st"> </span><span class="dv">1</span><span class="op">:</span><span class="dv">5</span>

<span class="cf">for</span>(i <span class="cf">in</span> terms){
  cv10_models &lt;-<span class="st"> </span><span class="kw">map</span>(cv10_data<span class="op">$</span>train, <span class="op">~</span><span class="st"> </span><span class="kw">lm</span>(mpg <span class="op">~</span><span class="st"> </span><span class="kw">poly</span>(horsepower, i), <span class="dt">data =</span> .))
  cv10_mse &lt;-<span class="st"> </span><span class="kw">map2_dbl</span>(cv10_models, cv10_data<span class="op">$</span>test, mse)
  cv_error_fold10[[i]] &lt;-<span class="st"> </span><span class="kw">mean</span>(cv10_mse)
}

cv_error_fold10</code></pre></div>
<pre><code>## [1] 24.21061 19.24482 19.34964 19.38367 18.94861</code></pre>
<p>How do these results compare to the LOOCV values?</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">data_frame</span>(<span class="dt">terms =</span> terms,
           <span class="dt">loocv =</span> cv_error,
           <span class="dt">fold10 =</span> cv_error_fold10) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">gather</span>(method, MSE, loocv<span class="op">:</span>fold10) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(terms, MSE, <span class="dt">color =</span> method)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_line</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="st">&quot;MSE estimates&quot;</span>,
       <span class="dt">x =</span> <span class="st">&quot;Degree of Polynomial&quot;</span>,
       <span class="dt">y =</span> <span class="st">&quot;Mean Squared Error&quot;</span>,
       <span class="dt">color =</span> <span class="st">&quot;CV Method&quot;</span>)</code></pre></div>
<p><img src="stat005_resampling_files/figure-html/10_fold_auto_loocv-1.png" width="672" /></p>
<p>Pretty much the same results.</p>
</div>
<div id="computational-speed-of-loocv-vs.-k-fold-cv" class="section level2">
<h2>Computational speed of LOOCV vs. <span class="math inline">\(k\)</span>-fold CV</h2>
<div id="loocv" class="section level3">
<h3>LOOCV</h3>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(profvis)

<span class="kw">profvis</span>({
  cv_error &lt;-<span class="st"> </span><span class="kw">vector</span>(<span class="st">&quot;numeric&quot;</span>, <span class="dv">5</span>)
  terms &lt;-<span class="st"> </span><span class="dv">1</span><span class="op">:</span><span class="dv">5</span>
  
  <span class="cf">for</span>(i <span class="cf">in</span> terms){
    loocv_models &lt;-<span class="st"> </span><span class="kw">map</span>(loocv_data<span class="op">$</span>train, <span class="op">~</span><span class="st"> </span><span class="kw">lm</span>(mpg <span class="op">~</span><span class="st"> </span><span class="kw">poly</span>(horsepower, i), <span class="dt">data =</span> .))
    loocv_mse &lt;-<span class="st"> </span><span class="kw">map2_dbl</span>(loocv_models, loocv_data<span class="op">$</span>test, mse)
    cv_error[[i]] &lt;-<span class="st"> </span><span class="kw">mean</span>(loocv_mse)
  }
})</code></pre></div>
<div id="htmlwidget-bed9c8f4ba1076df5c97" style="width:100%;height:600px;" class="profvis html-widget"></div>
<script type="application/json" data-for="htmlwidget-bed9c8f4ba1076df5c97">{"x":{"message":{"prof":{"time":[1,1,1,2,2,2,3,3,3,4,4,4,5,5,5,6,6,6,7,7,7,8,8,8,9,9,9,10,10,10,11,11,11,12,12,12,13,13,13,14,14,14,15,15,15,16,16,16,17,17,17,18,18,18,18,19,19,19,19,19,19,19,20,20,20,20,20,20,20,21,21,21,21,21,21,21,21,21,21,21,21,21,22,22,22,22,22,23,23,23,23,23,23,23,23,23,24,24,24,24,24,24,24,24,24,25,25,25,25,25,25,25,25,25,25,26,26,26,26,26,26,26,26,26,26,26,27,27,27,27,27,27,27,27,27,27,27,27,27,28,28,28,28,28,28,28,28,28,28,28,28,28,28,29,29,29,29,29,29,29,29,29,29,29,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,31,31,31,31,31,31,31,31,31,31,31,31,31,32,32,32,32,32,32,32,32,32,32,32,33,33,33,33,33,33,33,33,33,33,33,33,33,34,34,34,34,34,34,34,34,34,34,34,34,35,35,35,35,35,35,35,36,36,36,36,36,36,36,36,37,37,37,37,37,37,37,37,37,37,37,37,37,38,38,38,38,38,38,38,38,38,38,38,38,38,39,39,39,39,39,39,39,39,39,40,40,40,40,40,40,40,40,41,41,41,41,41,41,41,41,41,41,41,41,41,41,41,41,42,42,42,42,42,42,42,42,42,42,42,42,42,43,43,43,43,43,43,43,43,44,44,44,44,44,44,44,44,44,44,45,45,45,45,45,45,46,46,46,46,46,46,46,46,46,46,46,46,46,47,47,47,47,47,47,47,47,47,47,47,47,47,47,47,47,48,48,48,48,48,48,48,48,48,49,49,49,49,49,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,51,51,51,51,51,51,51,51,51,51,52,52,52,52,52,52,52,52,52,52,52,52,52,52,52,52,52,52,53,53,53,53,53,53,53,53,53,53,53,53,53,53,54,54,54,54,54,54,54,54,54,54,54,54,55,55,55,55,55,55,55,55,55,55,55,55,56,56,56,56,56,56,56,56,56,56,56,56,56,56,56,56,56,56,56,57,57,57,57,57,57,57,57,57,57,57,57,57,57,57,57,58,58,58,58,58,58,58,58,58,58,58,58,59,59,59,59,59,59,59,59,60,60,60,60,60,60,60,60,60,60,60,60,61,61,61,61,61,61,61,61,61,61,61,61,62,62,62,62,62,62,62,63,63,63,63,63,63,63,63,63,63,63,63,63,64,64,64,64,64,64,64,64,64,64,64,64,65,65,65,65,65,65,65,65,66,66,66,66,66,66,66,66,66,66,66,66,66,67,67,67,67,67,67,67,67,67,67,67,68,68,68,68,68,68,69,69,69,69,69,69,69,69,69,69,69,69,70,70,70,70,70,70,70,70,70,70,70,70,71,71,71,71,71,71,71,71,71,72,72,72,72,72,72,72,72,72,72,72,72,72,72,72,72,72,73,73,73,73,73,73,73,74,74,74,74,74,74,74,74,75,75,75,75,75,75,75,75,76,76,76,76,76,76,76,76,76,76,76,76,76,76,77,77,77,77,77,77,77,77,77,77,77,77,77,77,78,78,78,78,78,78,78,78,78,78,78,78,78,78,79,79,79,79,79,79,79,79,79,79,79,79,79,79,80,80,80,80,80,80,80,80,80,80,80,80,80,80,81,81,81,81,81,81,81,81,81,81,81,81,81,81,82,82,82,82,82,82,82,82,82,82,82,82,82,82,83,83,83,83,83,83,84,84,84,84,84,84,84,85,85,85,85,85,85,85,86,86,86,86,86,86,86,86,86,86,87,87,87,87,87,87,87,87,87,87,87,87,87,87,87,87,87,87,87,88,88,88,88,88,88,88,88,88,88,88,88,88,88,88,88,89,89,89,89,89,89,89,89,89,89,89,89,89,89,89,90,90,90,90,90,90,90,90,90,90,90,90,90,90,90,90,90,91,91,91,91,91,91,92,92,92,92,92,92,93,93,93,93,93,93,93,93,93,93,93,93,93,93,93,93,93,93,93,94,94,94,94,94,94,94,94,94,94,94,94,94,95,95,95,95,95,95,95,95,96,96,96,96,96,96,96,96,96,96,96,97,97,97,97,97,97,97,97,98,98,98,98,98,98,98,98,98,98,98,99,99,99,99,99,99,99,99,99,99,99,99,99,100,100,100,100,100,100,100,100,100,101,101,101,101,101,101,101,101,101,101,101,101,101,102,102,102,102,102,102,102,102,102,102,103,103,103,103,103,103,103,103,103,104,104,104,104,104,104,104,104,104,104,105,105,105,105,105,105,105,105,105,105,105,105,105,105,105,105,106,106,106,106,106,106,106,106,106,106,106,106,106,107,107,107,107,107,107,107,107,107,107,107,108,108,108,108,108,108,108,108,108,108,109,109,109,109,109,109,109,109,109,109,109,109,109,110,110,110,110,110,110,110,110,110,110,110,110,110,111,111,111,111,111,111,111,111,111,111,111,111,111,112,112,112,112,112,112,112,112,112,112,112,112,112,113,113,113,113,113,113,113,113,113,113,113,113,113,114,114,114,114,114,114,114,114,114,114,114,114,114,115,115,115,115,115,115,115,115,115,116,116,116,116,116,116,116,116,116,116,116,116,116,117,117,117,117,117,117,117,117,117,117,117,118,118,118,118,118,119,119,119,119,119,119,119,119,119,119,119,119,119,120,120,120,120,120,120,120,120,120,120,120,120,120,120,120,120,121,121,121,121,121,121,122,122,122,122,122,122,122,122,122,122,122,122,122,123,123,123,123,123,123,123,123,123,123,123,124,124,124,124,124,124,125,125,125,125,125,125,125,125,125,125,125,125,125,126,126,126,126,126,126,127,127,127,127,127,127,127,127,127,128,128,128,128,128,128,128,129,129,129,129,129,129,129,129,129,129,129,130,130,130,130,130,130,130,130,130,130,130,130,131,131,131,131,131,131,131,131,131,131,131,131,131,131,131,131,132,132,132,132,132,132,132,132,133,133,133,133,133,133,133,133,133,133,133,133,133,133,134,134,134,134,134,134,134,134,134,134,134,134,135,135,135,135,135,135,135,135,135,135,136,136,136,136,136,136,136,136,136,136,136,136,136,137,137,137,137,137,137,137,137,137,137,137,138,138,138,138,138,138,138,139,139,139,139,139,139,139,139,139,139,139,140,140,140,140,140,140,140,140,141,141,141,141,141,141,141,141,141,141,141,141,142,142,142,142,142,142,142,142,142,142,142,142,142,142,143,143,143,143,143,143,144,144,144,144,144,144,144,144,144,144,144,144,144,145,145,145,145,145,146,146,146,146,146,146,146,146,146,146,146,146,147,147,147,147,147,147,147,147,147,147,147,147,147,148,148,148,148,148,148,148,149,149,149,149,149,149,149,149,149,149,149,150,150,150,150,150,150,150,150,150,150,150,151,151,151,151,151,151,151,151,151,151,151,151,151,152,152,152,152,152,152,152,152,152,152,152,152,152,152,152,152,152,153,153,153,153,153,153,153,153,153,153,153,154,154,154,154,154,154,154,154,155,155,155,155,155,155,155,155,155,155,155,155,155,156,156,156,156,156,156,156,156,156,156,156,156,156,156,157,157,157,157,157,157,158,158,158,158,158,158,158,158,158,158,158,158,158,158,158,158,159,159,159,159,159,159,159,159,160,160,160,160,160,160,160,161,161,161,161,161,161,161,161,161,161,161,162,162,162,162,162,162,162,162,162,162,162,162,162,162,162,163,163,163,163,163,163,164,164,164,164,164,164,164,164,164,164,164,164,164,164,165,165,165,165,165,165,165,166,166,166,166,166,166,166,167,167,167,167,167,167,167,167,167,168,168,168,168,168,168,168,168,168,168,168,168,168,169,169,169,169,169,169,169,169,169,169,169,170,170,170,170,170,170,170,170,170,171,171,171,171,171,171,171,171,171,171,171,171,171,171,171,172,172,172,172,172,172,172,172,173,173,173,173,173,173,173,173,173,173,174,174,174,174,174,174,174,174,174,174,174,174,174,174,174,174,175,175,175,175,175,175,175,175,175,175,175,175,175,175,175,175,175,175,175,176,176,176,176,176,176,176,176,176,176,176,176,176,176,177,177,177,177,177,177,177,177,177,177,177,177,177,177,178,178,178,178,178,178,178,178,178,178,178,178,178,178,179,179,179,179,179,179,179,180,180,180,180,180,180,180,181,181,181,181,181,181,181,181,182,182,182,182,182,182,183,183,183,183,183,183,183,183,183,183,183,183,183,183,184,184,184,184,184,184,184,184,184,184,184,184,184,184,185,185,185,185,185,185,185,185,185,185,185,185,185,186,186,186,186,186,186,186,186,186,186,186,186,186,187,187,187,187,187,187,188,188,188,188,188,188,188,188,188,188,189,189,189,189,189,189,189,189,189,189,190,190,190,190,190,190,190,190,190,190,190,190,190,190,191,191,191,191,191,191,191,191,191,191,191,192,192,192,192,192,192,192,192,192,193,193,193,193,193,193,193,193,193,193,193,194,194,194,194,194,194,194,194,195,195,195,195,195,196,196,196,196,196,196,197,197,197,197,197,197,197,198,198,198,198,198,198,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,199,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,201,201,201,201,201,201,201,201,201,201,202,202,202,202,202,202,203,203,203,203,203,203,203,203,203,203,203,203,203,204,204,204,204,204,204,204,204,204,204,204,204,204,205,205,205,205,205,205,205,205,205,205,205,205,205,206,206,206,206,206,206,206,206,206,206,206,206,206,206,207,207,207,207,207,207,207,207,207,207,207,207,208,208,208,208,208,208,208,208,208,208,208,208,208,209,209,209,209,209,209,209,209,209,209,209,209,209,210,210,210,210,210,210,210,210,210,210,210,211,211,211,211,211,211,211,211,211,211,211,211,212,212,212,212,212,212,212,213,213,213,213,213,213,213,213,213,213,213,213,213,214,214,214,214,214,214,214,214,214,214,214,215,215,215,215,215,215,215,215,215,215,215,215,215,216,216,216,216,216,216,216,216,216,216,216,216,216,216,216,216,217,217,217,217,217,217,217,217,217,217,217,217,217,217,217,218,218,218,218,218,218,218,218,218,218,218,219,219,219,219,219,219,219,219,219,219,219,220,220,220,220,220,220,220,220,220,220,220,220,221,221,221,221,221,221,221,221,221,221,221,221,221,222,222,222,222,222,223,223,223,223,223,223,223,223,224,224,224,224,224,225,225,225,225,225,225,225,226,226,226,226,226,226,226,226,226,227,227,227,227,227,227,227,227,227,227,227,227,228,228,228,228,228,228,228,228,228,229,229,229,229,229,229,229,229,229,230,230,230,230,230,230,230,230,230,230,230,230,231,231,231,231,231,231,231,231,231,231,231,231,231,231,232,232,232,232,232,232,232,232,232,232,232,232,233,233,233,233,233,233,233,233,233,233,233,233,233,233,233,233,234,234,234,234,234,234,235,235,235,235,235,235,235,235,235,235,235,236,236,236,236,236,236,236,236,236,236,236,236,236,236,236,236,236,236,236,236,237,237,237,237,237,237,237,237,237,238,238,238,238,238,238,238,238,238,238,238,239,239,239,239,239,239,239,239,239,239,240,240,240,240,240,240,240,240,240,240,240,240,240,240,240,240,241,241,241,241,241,241,241,241,241,241,241,242,242,242,242,242,242,242,242,243,243,243,243,243,243,243,243,243,243,243,244,244,244,244,244,244,244,245,245,245,245,245,245,245,245,245,246,246,246,246,246,246,246,246,247,247,247,247,247,247,247,247,247,247,247,247,247,248,248,248,248,248,248,248,248,248,248,248,248,249,249,249,249,249,249,249,249,249,249,249,249,249,249,249,250,250,250,250,250,250,250,250,250,250,250,250,250,250,250,250,250,250,250,251,251,251,251,251,251,251,251,251,251,252,252,252,252,252,252,252,252,252,252,253,253,253,253,253,253,253,253,253,254,254,254,254,254,254,254,254,254,254,254,254,255,255,255,255,255,255,255,255,255,255,255,256,256,256,256,256,256,256,256,256,256,256,256,256,256,256,256,256,256,257,257,257,257,257,257,258,258,258,258,258,258,258,258,258,258,258,258,258,259,259,259,259,259,259,259,259,259,259,259,260,260,260,260,260,260,260,260,260,260,260,260,261,261,261,261,261,261,261,261,261,261,262,262,262,262,262,262,262,262,262,262,262,262,262,262,262,262,263,263,263,263,263,263,263,263,263,263,263,263,263,263,263,263,264,264,264,264,264,264,264,264,264,264,265,265,265,265,265,265,265,265,266,266,266,266,266,266,266,267,267,267,267,267,267,267,267,267,267,267,267,267,267,267,268,268,268,268,268,268,268,268,268,268,268,269,269,269,269,269,269,269,269,269,269,269,269,270,270,270,270,270,270,270,270,270,270,270,270,270,270,270,270,271,271,271,271,271,271,271,271,271,271,272,272,272,272,272,272,272,272,272,272,273,273,273,273,273,273,273,273,273,273,273,273,273,274,274,274,274,274,274,274,274,274,275,275,275,275,275,275,275,275,275,275,275,275,275,275,275,276,276,276,276,276,276,276,276,276,276,276,276,277,277,277,277,277,277,277,277,277,277,277,277,278,278,278,278,278,278,278,278,278,278,278,279,279,279,279,279,279,279,279,279,279,279,280,280,280,280,280,280,280,280,281,281,281,281,281,282,282,282,282,282,282,282,282,282,282,282,282,282,282,282,282,282,283,283,283,283,283,283,283,284,284,284,284,284,284,284,284,284,284,284,284,284,285,285,285,285,285,285,285,285,285,285,285,285,286,286,286,286,287,287,287,287,287,287,287,288,288,288,288,288,288,288,288,288,288,288,288,288,289,289,289,289,289,289,289,289,289,289,289,289,290,290,290,290,290,290,290,291,291,291,291,291,291,291,291,291,291,291,291,291,292,292,292,292,292,292,292,292,292,292,292,292,292,293,293,293,293,293,293,293,293,293,293,293,293,294,294,294,294,294,294,294,294,294,294,294,294,294,294,294,294,295,295,295,295,295,295,295,295,295,295,295,295,296,296,296,296,296,296,296,296,296,296,296,296,297,297,297,297,297,297,297,297,297,297,297,297,298,298,298,298,298,298,298,298,298,298,298,298,298,299,299,299,299,299,299,299,299,299,299,299,300,300,300,300,300,300,300,300,300,300,300,300,301,301,301,301,301,302,302,302,302,302,302,302,302,302,302,302,302,302,303,303,303,303,303,303,303,304,304,304,304,304,304,304,304,304,304,304,304,304,304,304,304,305,305,305,305,305,305,305,305,305,305,306,306,306,306,306,306,306,306,306,306,306,306,307,307,307,307,307,307,307,307,307,308,308,308,308,308,308,308,308,308,308,308,308,308,309,309,309,309,310,310,310,310,310,310,310,310,310,310,310,310,310,311,311,311,311,311,311,311,311,311,311,311,311,311,311,312,312,312,312,312,312,312,313,313,313,313,313,313,313,313,313,313,313,314,314,314,314,314,314,314,314,315,315,315,315,315,315,315,315,315,315,315,315,316,316,316,316,316,316,316,316,316,316,317,317,317,317,317,317,317,318,318,318,318,318,318,318,318,318,318,318,318,318,319,319,319,319,319,319,319,319,319,319,319,319,319,319,320,320,320,320,320,320,320,320,320,320,320,320,320,321,321,321,321,321,321,321,321,321,321,321,321,321,321,322,322,322,322,323,323,323,323,323,323,324,324,324,324,324,324,324,324,324,324,324,324,325,325,325,325,325,325,325,325,325,325,325,325,325,325,325,325,325,326,326,326,326,326,326,326,326,326,326,326,326,326,326,327,327,327,327,327,327,327,327,327,327,327,327,327,327,327,327,328,328,328,328,328,328,328,328,329,329,329,329,329,329,329,329,329,329,329,329,329,329,329,329,329,329,329,330,330,330,330,330,330,330,330,331,331,331,331,331,331,331,331,331,331,331,331,331,331,331,331,331,332,332,332,332,332,332,332,333,333,333,333,333,333,333,333,333,333,333,334,334,334,334,334,334,334,334,334,335,335,335,335,335,335,335,335,335,335,335,335,335,336,336,336,336,336,336,336,336,337,337,337,337,337,337,337,337,337,337,337,337,338,338,338,338,338,338,338,338,338,338,338,338,338,338,339,339,339,339,339,339,339,339,339,339,339,339,340,340,340,340,340,340,340,340,340,340,340,340,341,341,341,341,341,341,342,342,342,342,343,343,343,343,343,343,343,343,344,344,344,344,344,344,344,344,344,344,344,344,344,344,344,344,344,344,345,345,345,345,345,345,346,346,346,346,346,346,346,346,346,347,347,347,347,347,347,347,347,347,347,347,347,348,348,348,348,348,348,348,348,348,348,348,349,349,349,349,349,349,350,350,350,350,350,350,350,350,351,351,351,351,351,351,351,351,352,352,352,352,352,352,352,352,352,352,353,353,353,353,353,353,353,353,353,353,353,353,353,353,354,354,354,354,354,354,354,354,354,354,355,355,355,355,355,355,355,355,355,355,355,355,355,355,355,355,355,356,356,356,356,356,356,357,357,357,357,357,358,358,358,358,358,358,358,358,358,358,358,359,359,359,359,359,359,359,359,359,360,360,360,360,360,360,360,360,360,360,360,360,360,360,360,360,361,361,361,361,361,361,361,361,361,361,361,361,362,362,362,362,362,362,362,362,362,362,362,362,362,362,362,362,363,363,363,363,363,364,364,364,364,364,364,364,364,364,364,364,364,364,364,364,364,364,364,364,364,365,365,365,365,365,365,365,365,366,366,366,366,366,366,366,366,366,366,366,366,366,366,366,367,367,367,367,367,367,367,367,367,368,368,368,368,368,368,368,368,368,368,369,369,369,369,369,369,370,370,370,370,370,370,370,370,370,370,370,370,371,371,371,371,371,371,371,371,371,371,371,371,371,372,372,372,372,372,372,372,372,372,372,372,372,373,373,373,373,373,373,373,374,374,374,374,374,374,374,374,374,374,374,374,374,375,375,375,375,375,375,375,375,375,375,375,376,376,376,376,376,376,376,376,377,377,377,377,377,377,377,377,377,377,377,378,378,378,378,379,379,379,379,379,379,379,379,379,379,379,379,380,380,380,380,380,380,380,380,380,380,380,380,381,381,381,381,381,381,381,381,381,381,381,382,382,382,382,382,382,382,382,382,382,382,383,383,383,383,383,383,383,383,383,383,383,383,383,384,384,384,384,384,384,384,384,384,384,384,384,384,385,385,385,385,385,385,385,385,385,385,385,385,386,386,386,386,386,386,386,386,386,386,386,386,386,386,387,387,387,387,387,387,387,387,387,387,387,387,388,388,388,388,388,388,388,388,388,388,388,388,388,388,389,389,389,389,389,389,389,389,389,389,389,389,389,390,390,390,390,390,390,390,390,390,390,390,390,391,391,391,391,391,391,391,391,391,391,391,391,392,392,392,392,392,392,393,393,393,393,393,393,393,393,393,393,393,393,393,393,393,393,394,394,394,394,394,394,394,394,394,394,394,394,395,395,395,395,395,395,395,395,395,395,395,395,395,396,396,396,396,396,396,396,396,396,396,396,397,397,397,397,397,397,397,397,397,397,397,397,397,398,398,398,398,398,398,398,398,398,398,398,398,399,399,399,399,399,399,399,400,400,400,400,400,400,400,400,400,400,400,401,401,401,401,401,401,401,401,401,401,401,401,401,401,401,401,402,402,402,402,402,402,402,402,402,402,402,402,403,403,403,403,403,403,403,403,403,403,403,404,404,404,404,404,404,404,404,404,404,404,405,405,405,405,405,406,406,406,406,406,406,406,406,406,406,406,406,406,406,407,407,407,407,407,407,407,407,407,408,408,408,408,408,408,409,409,409,409,409,409,409,409,409,409,409,410,410,410,410,410,410,410,410,410,410,410,411,411,411,411,411,411,411,411,411,411,411,411,411,411,411,411,411,411,412,412,412,412,412,412,412,412,412,412,412,413,413,413,413,413,413,413,413,413,413,413,413,414,414,414,414,414,414,414,414,415,415,415,415,415,415,415,415,415,415,415,415,415,416,416,416,416,416,416,416,416,416,416,416,416,416,416,416,416,416,417,417,417,417,417,417,417,417,417,417,417,417,418,418,418,418,418,418,418,418,418,418,418,418,418,419,419,419,419,419,419,419,419,419,419,419,419,420,420,420,420,420,420,420,420,420,420,420,420,420,420,420,420,420,421,421,421,421,421,421,421,421,421,421,421,421,421,421,422,422,422,422,422,422,422,422,422,422,422,422,422,423,423,423,423,423,423,423,423,423,423,423,423,423,423,423,423,423,424,424,424,424,424,424,424,424,424,424,424,424,424,424,424,424,424,424,424,424,425,425,425,425,425,425,425,425,426,426,426,426,426,426,426,426,426,426,426,426,426,426,426,426,427,427,427,427,427,427,427,427,427,427,428,428,428,428,428,428,428,428,428,429,429,429,429,429,429,429,429,429,429,429,429,429,429,429,429,430,430,430,430,430,430,430,430,430,430,430,430,431,431,431,431,431,431,431,431,431,431,431,431,431,431,431,431,432,432,432,432,432,432,432,432,433,433,433,433,433,433,433,433,434,434,434,434,434,434,434,434,434,434,434,434,434,434,434,434,435,435,435,435,435,435,435,435,435,435,435,435,435,435,435,435,436,436,436,436,436,436,436,437,437,437,437,437,437,437,438,438,438,438,438,438,439,439,439,439,439,439,439,439,439,439,439,439,440,440,440,440,440,440,440,441,441,441,441,441,441,441,441,441,441,441,441,441,441,441,442,442,442,442,442,442,443,443,443,443,443,443,443,444,444,444,444,444,444,444,444,444,444,444,444,444,444,444,445,445,445,445,445,445,445,445,445,445,445,445,446,446,446,446,446,446,446,446,446,447,447,447,447,447,447,447,447,448,448,448,448,448,448,448,448,448,449,449,449,449,449,449,449,449,449,449,449,449,449,449,449],"depth":[3,2,1,3,2,1,3,2,1,3,2,1,3,2,1,3,2,1,3,2,1,3,2,1,3,2,1,3,2,1,3,2,1,3,2,1,3,2,1,3,2,1,3,2,1,3,2,1,3,2,1,4,3,2,1,7,6,5,4,3,2,1,7,6,5,4,3,2,1,13,12,11,10,9,8,7,6,5,4,3,2,1,5,4,3,2,1,9,8,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,10,9,8,7,6,5,4,3,2,1,11,10,9,8,7,6,5,4,3,2,1,13,12,11,10,9,8,7,6,5,4,3,2,1,14,13,12,11,10,9,8,7,6,5,4,3,2,1,11,10,9,8,7,6,5,4,3,2,1,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,13,12,11,10,9,8,7,6,5,4,3,2,1,11,10,9,8,7,6,5,4,3,2,1,13,12,11,10,9,8,7,6,5,4,3,2,1,12,11,10,9,8,7,6,5,4,3,2,1,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,13,12,11,10,9,8,7,6,5,4,3,2,1,13,12,11,10,9,8,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,16,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,13,12,11,10,9,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,10,9,8,7,6,5,4,3,2,1,6,5,4,3,2,1,13,12,11,10,9,8,7,6,5,4,3,2,1,16,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,5,4,3,2,1,19,18,17,16,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,10,9,8,7,6,5,4,3,2,1,18,17,16,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,14,13,12,11,10,9,8,7,6,5,4,3,2,1,12,11,10,9,8,7,6,5,4,3,2,1,12,11,10,9,8,7,6,5,4,3,2,1,19,18,17,16,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,16,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,12,11,10,9,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,12,11,10,9,8,7,6,5,4,3,2,1,12,11,10,9,8,7,6,5,4,3,2,1,7,6,5,4,3,2,1,13,12,11,10,9,8,7,6,5,4,3,2,1,12,11,10,9,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,13,12,11,10,9,8,7,6,5,4,3,2,1,11,10,9,8,7,6,5,4,3,2,1,6,5,4,3,2,1,12,11,10,9,8,7,6,5,4,3,2,1,12,11,10,9,8,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,17,16,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,14,13,12,11,10,9,8,7,6,5,4,3,2,1,14,13,12,11,10,9,8,7,6,5,4,3,2,1,14,13,12,11,10,9,8,7,6,5,4,3,2,1,14,13,12,11,10,9,8,7,6,5,4,3,2,1,14,13,12,11,10,9,8,7,6,5,4,3,2,1,14,13,12,11,10,9,8,7,6,5,4,3,2,1,14,13,12,11,10,9,8,7,6,5,4,3,2,1,6,5,4,3,2,1,7,6,5,4,3,2,1,7,6,5,4,3,2,1,10,9,8,7,6,5,4,3,2,1,19,18,17,16,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,16,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,17,16,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,6,5,4,3,2,1,6,5,4,3,2,1,19,18,17,16,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,13,12,11,10,9,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,11,10,9,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,11,10,9,8,7,6,5,4,3,2,1,13,12,11,10,9,8,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,13,12,11,10,9,8,7,6,5,4,3,2,1,10,9,8,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,10,9,8,7,6,5,4,3,2,1,16,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,13,12,11,10,9,8,7,6,5,4,3,2,1,11,10,9,8,7,6,5,4,3,2,1,10,9,8,7,6,5,4,3,2,1,13,12,11,10,9,8,7,6,5,4,3,2,1,13,12,11,10,9,8,7,6,5,4,3,2,1,13,12,11,10,9,8,7,6,5,4,3,2,1,13,12,11,10,9,8,7,6,5,4,3,2,1,13,12,11,10,9,8,7,6,5,4,3,2,1,13,12,11,10,9,8,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,13,12,11,10,9,8,7,6,5,4,3,2,1,11,10,9,8,7,6,5,4,3,2,1,5,4,3,2,1,13,12,11,10,9,8,7,6,5,4,3,2,1,16,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,6,5,4,3,2,1,13,12,11,10,9,8,7,6,5,4,3,2,1,11,10,9,8,7,6,5,4,3,2,1,6,5,4,3,2,1,13,12,11,10,9,8,7,6,5,4,3,2,1,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,7,6,5,4,3,2,1,11,10,9,8,7,6,5,4,3,2,1,12,11,10,9,8,7,6,5,4,3,2,1,16,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,14,13,12,11,10,9,8,7,6,5,4,3,2,1,12,11,10,9,8,7,6,5,4,3,2,1,10,9,8,7,6,5,4,3,2,1,13,12,11,10,9,8,7,6,5,4,3,2,1,11,10,9,8,7,6,5,4,3,2,1,7,6,5,4,3,2,1,11,10,9,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,12,11,10,9,8,7,6,5,4,3,2,1,14,13,12,11,10,9,8,7,6,5,4,3,2,1,6,5,4,3,2,1,13,12,11,10,9,8,7,6,5,4,3,2,1,5,4,3,2,1,12,11,10,9,8,7,6,5,4,3,2,1,13,12,11,10,9,8,7,6,5,4,3,2,1,7,6,5,4,3,2,1,11,10,9,8,7,6,5,4,3,2,1,11,10,9,8,7,6,5,4,3,2,1,13,12,11,10,9,8,7,6,5,4,3,2,1,17,16,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,11,10,9,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,13,12,11,10,9,8,7,6,5,4,3,2,1,14,13,12,11,10,9,8,7,6,5,4,3,2,1,6,5,4,3,2,1,16,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,7,6,5,4,3,2,1,11,10,9,8,7,6,5,4,3,2,1,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,6,5,4,3,2,1,14,13,12,11,10,9,8,7,6,5,4,3,2,1,7,6,5,4,3,2,1,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,13,12,11,10,9,8,7,6,5,4,3,2,1,11,10,9,8,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,10,9,8,7,6,5,4,3,2,1,16,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,19,18,17,16,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,14,13,12,11,10,9,8,7,6,5,4,3,2,1,14,13,12,11,10,9,8,7,6,5,4,3,2,1,14,13,12,11,10,9,8,7,6,5,4,3,2,1,7,6,5,4,3,2,1,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,6,5,4,3,2,1,14,13,12,11,10,9,8,7,6,5,4,3,2,1,14,13,12,11,10,9,8,7,6,5,4,3,2,1,13,12,11,10,9,8,7,6,5,4,3,2,1,13,12,11,10,9,8,7,6,5,4,3,2,1,6,5,4,3,2,1,10,9,8,7,6,5,4,3,2,1,10,9,8,7,6,5,4,3,2,1,14,13,12,11,10,9,8,7,6,5,4,3,2,1,11,10,9,8,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,11,10,9,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,5,4,3,2,1,6,5,4,3,2,1,7,6,5,4,3,2,1,6,5,4,3,2,1,16,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,10,9,8,7,6,5,4,3,2,1,6,5,4,3,2,1,13,12,11,10,9,8,7,6,5,4,3,2,1,13,12,11,10,9,8,7,6,5,4,3,2,1,13,12,11,10,9,8,7,6,5,4,3,2,1,14,13,12,11,10,9,8,7,6,5,4,3,2,1,12,11,10,9,8,7,6,5,4,3,2,1,13,12,11,10,9,8,7,6,5,4,3,2,1,13,12,11,10,9,8,7,6,5,4,3,2,1,11,10,9,8,7,6,5,4,3,2,1,12,11,10,9,8,7,6,5,4,3,2,1,7,6,5,4,3,2,1,13,12,11,10,9,8,7,6,5,4,3,2,1,11,10,9,8,7,6,5,4,3,2,1,13,12,11,10,9,8,7,6,5,4,3,2,1,16,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,11,10,9,8,7,6,5,4,3,2,1,11,10,9,8,7,6,5,4,3,2,1,12,11,10,9,8,7,6,5,4,3,2,1,13,12,11,10,9,8,7,6,5,4,3,2,1,5,4,3,2,1,8,7,6,5,4,3,2,1,5,4,3,2,1,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,12,11,10,9,8,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,12,11,10,9,8,7,6,5,4,3,2,1,14,13,12,11,10,9,8,7,6,5,4,3,2,1,12,11,10,9,8,7,6,5,4,3,2,1,16,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,6,5,4,3,2,1,11,10,9,8,7,6,5,4,3,2,1,20,19,18,17,16,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,11,10,9,8,7,6,5,4,3,2,1,10,9,8,7,6,5,4,3,2,1,16,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,11,10,9,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,11,10,9,8,7,6,5,4,3,2,1,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,13,12,11,10,9,8,7,6,5,4,3,2,1,12,11,10,9,8,7,6,5,4,3,2,1,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,19,18,17,16,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,10,9,8,7,6,5,4,3,2,1,10,9,8,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,12,11,10,9,8,7,6,5,4,3,2,1,11,10,9,8,7,6,5,4,3,2,1,18,17,16,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,6,5,4,3,2,1,13,12,11,10,9,8,7,6,5,4,3,2,1,11,10,9,8,7,6,5,4,3,2,1,12,11,10,9,8,7,6,5,4,3,2,1,10,9,8,7,6,5,4,3,2,1,16,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,16,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,10,9,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,7,6,5,4,3,2,1,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,11,10,9,8,7,6,5,4,3,2,1,12,11,10,9,8,7,6,5,4,3,2,1,16,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,10,9,8,7,6,5,4,3,2,1,10,9,8,7,6,5,4,3,2,1,13,12,11,10,9,8,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,12,11,10,9,8,7,6,5,4,3,2,1,12,11,10,9,8,7,6,5,4,3,2,1,11,10,9,8,7,6,5,4,3,2,1,11,10,9,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,5,4,3,2,1,17,16,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,7,6,5,4,3,2,1,13,12,11,10,9,8,7,6,5,4,3,2,1,12,11,10,9,8,7,6,5,4,3,2,1,4,3,2,1,7,6,5,4,3,2,1,13,12,11,10,9,8,7,6,5,4,3,2,1,12,11,10,9,8,7,6,5,4,3,2,1,7,6,5,4,3,2,1,13,12,11,10,9,8,7,6,5,4,3,2,1,13,12,11,10,9,8,7,6,5,4,3,2,1,12,11,10,9,8,7,6,5,4,3,2,1,16,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,12,11,10,9,8,7,6,5,4,3,2,1,12,11,10,9,8,7,6,5,4,3,2,1,12,11,10,9,8,7,6,5,4,3,2,1,13,12,11,10,9,8,7,6,5,4,3,2,1,11,10,9,8,7,6,5,4,3,2,1,12,11,10,9,8,7,6,5,4,3,2,1,5,4,3,2,1,13,12,11,10,9,8,7,6,5,4,3,2,1,7,6,5,4,3,2,1,16,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,10,9,8,7,6,5,4,3,2,1,12,11,10,9,8,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,13,12,11,10,9,8,7,6,5,4,3,2,1,4,3,2,1,13,12,11,10,9,8,7,6,5,4,3,2,1,14,13,12,11,10,9,8,7,6,5,4,3,2,1,7,6,5,4,3,2,1,11,10,9,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,12,11,10,9,8,7,6,5,4,3,2,1,10,9,8,7,6,5,4,3,2,1,7,6,5,4,3,2,1,13,12,11,10,9,8,7,6,5,4,3,2,1,14,13,12,11,10,9,8,7,6,5,4,3,2,1,13,12,11,10,9,8,7,6,5,4,3,2,1,14,13,12,11,10,9,8,7,6,5,4,3,2,1,4,3,2,1,6,5,4,3,2,1,12,11,10,9,8,7,6,5,4,3,2,1,17,16,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,14,13,12,11,10,9,8,7,6,5,4,3,2,1,16,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,19,18,17,16,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,17,16,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,7,6,5,4,3,2,1,11,10,9,8,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,13,12,11,10,9,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,12,11,10,9,8,7,6,5,4,3,2,1,14,13,12,11,10,9,8,7,6,5,4,3,2,1,12,11,10,9,8,7,6,5,4,3,2,1,12,11,10,9,8,7,6,5,4,3,2,1,6,5,4,3,2,1,4,3,2,1,8,7,6,5,4,3,2,1,18,17,16,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,12,11,10,9,8,7,6,5,4,3,2,1,11,10,9,8,7,6,5,4,3,2,1,6,5,4,3,2,1,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,10,9,8,7,6,5,4,3,2,1,14,13,12,11,10,9,8,7,6,5,4,3,2,1,10,9,8,7,6,5,4,3,2,1,17,16,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,6,5,4,3,2,1,5,4,3,2,1,11,10,9,8,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,16,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,12,11,10,9,8,7,6,5,4,3,2,1,16,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,5,4,3,2,1,20,19,18,17,16,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,10,9,8,7,6,5,4,3,2,1,6,5,4,3,2,1,12,11,10,9,8,7,6,5,4,3,2,1,13,12,11,10,9,8,7,6,5,4,3,2,1,12,11,10,9,8,7,6,5,4,3,2,1,7,6,5,4,3,2,1,13,12,11,10,9,8,7,6,5,4,3,2,1,11,10,9,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,11,10,9,8,7,6,5,4,3,2,1,4,3,2,1,12,11,10,9,8,7,6,5,4,3,2,1,12,11,10,9,8,7,6,5,4,3,2,1,11,10,9,8,7,6,5,4,3,2,1,11,10,9,8,7,6,5,4,3,2,1,13,12,11,10,9,8,7,6,5,4,3,2,1,13,12,11,10,9,8,7,6,5,4,3,2,1,12,11,10,9,8,7,6,5,4,3,2,1,14,13,12,11,10,9,8,7,6,5,4,3,2,1,12,11,10,9,8,7,6,5,4,3,2,1,14,13,12,11,10,9,8,7,6,5,4,3,2,1,13,12,11,10,9,8,7,6,5,4,3,2,1,12,11,10,9,8,7,6,5,4,3,2,1,12,11,10,9,8,7,6,5,4,3,2,1,6,5,4,3,2,1,16,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,12,11,10,9,8,7,6,5,4,3,2,1,13,12,11,10,9,8,7,6,5,4,3,2,1,11,10,9,8,7,6,5,4,3,2,1,13,12,11,10,9,8,7,6,5,4,3,2,1,12,11,10,9,8,7,6,5,4,3,2,1,7,6,5,4,3,2,1,11,10,9,8,7,6,5,4,3,2,1,16,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,12,11,10,9,8,7,6,5,4,3,2,1,11,10,9,8,7,6,5,4,3,2,1,11,10,9,8,7,6,5,4,3,2,1,5,4,3,2,1,14,13,12,11,10,9,8,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,6,5,4,3,2,1,11,10,9,8,7,6,5,4,3,2,1,11,10,9,8,7,6,5,4,3,2,1,18,17,16,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,11,10,9,8,7,6,5,4,3,2,1,12,11,10,9,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,13,12,11,10,9,8,7,6,5,4,3,2,1,17,16,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,12,11,10,9,8,7,6,5,4,3,2,1,13,12,11,10,9,8,7,6,5,4,3,2,1,12,11,10,9,8,7,6,5,4,3,2,1,17,16,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,14,13,12,11,10,9,8,7,6,5,4,3,2,1,13,12,11,10,9,8,7,6,5,4,3,2,1,17,16,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,20,19,18,17,16,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,16,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,10,9,8,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,16,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,12,11,10,9,8,7,6,5,4,3,2,1,16,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,16,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,16,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,7,6,5,4,3,2,1,7,6,5,4,3,2,1,6,5,4,3,2,1,12,11,10,9,8,7,6,5,4,3,2,1,7,6,5,4,3,2,1,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,6,5,4,3,2,1,7,6,5,4,3,2,1,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,12,11,10,9,8,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1],"label":["lazyLoadDBfetch",".Call","map","lazyLoadDBfetch",".Call","map","lazyLoadDBfetch",".Call","map","lazyLoadDBfetch",".Call","map","lazyLoadDBfetch",".Call","map","lazyLoadDBfetch",".Call","map","lazyLoadDBfetch",".Call","map","lazyLoadDBfetch",".Call","map","lazyLoadDBfetch",".Call","map","lazyLoadDBfetch",".Call","map","lazyLoadDBfetch",".Call","map","lazyLoadDBfetch",".Call","map","lazyLoadDBfetch",".Call","map","lazyLoadDBfetch",".Call","map","lazyLoadDBfetch",".Call","map","lazyLoadDBfetch",".Call","map","lazyLoadDBfetch",".Call","map","lm",".f",".Call","map",".External2","model.matrix.default","model.matrix","lm",".f",".Call","map",".External2","model.matrix.default","model.matrix","lm",".f",".Call","map","unique.default","unique","poly","eval","eval","model.frame.default","stats::model.frame","eval","eval","lm",".f",".Call","map","lm.fit","lm",".f",".Call","map","NextMethod","[.data.frame","[","lapply",".getXlevels","lm",".f",".Call","map","%in%","[[.data.frame","[[","model.matrix.default","model.matrix","lm",".f",".Call","map","match.fun","vapply","model.frame.default","stats::model.frame","eval","eval","lm",".f",".Call","map","%in%","deparse","paste","FUN","lapply","sapply",".getXlevels","lm",".f",".Call","map","<GC>","is.matrix","<Anonymous>","[[.data.frame","[[","model.frame.default","stats::model.frame","eval","eval","lm",".f",".Call","map","list_to_tibble","as_tibble.data.frame","[.tbl_df","[","as.data.frame.resample","as.data.frame","model.frame.default","stats::model.frame","eval","eval","lm",".f",".Call","map","FUN","lapply","sapply","model.frame.default","stats::model.frame","eval","eval","lm",".f",".Call","map","recycle_columns","list_to_tibble","as_tibble.data.frame","[.tbl_df","[","as.data.frame.resample","as.data.frame","model.frame.default","stats::model.frame","eval","eval","lm",".f",".Call","map","lapply","[.tbl_df","[","as.data.frame.resample","as.data.frame","model.frame.default","stats::model.frame","eval","eval","lm",".f",".Call","map","stats::complete.cases","<Anonymous>",".External2","model.frame.default","stats::model.frame","eval","eval","lm",".f",".Call","map",".deparseOpts","deparse","paste","FUN","lapply","sapply","match","model.matrix.default","model.matrix","lm",".f",".Call","map","na.exclude.data.frame","stats::na.exclude","<Anonymous>",".External2","model.frame.default","stats::model.frame","eval","eval","lm",".f",".Call","map",".External2","model.matrix.default","model.matrix","lm",".f",".Call","map","names","names","model.matrix.default","model.matrix","lm",".f",".Call","map","is.data.frame","colnames<-","poly","eval","eval","model.frame.default","stats::model.frame","eval","eval","lm",".f",".Call","map","unique.default","unique","poly","eval","eval","model.frame.default","stats::model.frame","eval","eval","lm",".f",".Call","map","names","names","match","model.matrix.default","model.matrix","lm",".f",".Call","map","<Anonymous>","[[.data.frame","[[","model.response","lm",".f",".Call","map","deparse","mode","match","%in%","deparse","paste","FUN","lapply","sapply","match","model.matrix.default","model.matrix","lm",".f",".Call","map",".deparseOpts","deparse","paste","FUN","lapply","sapply","match","model.matrix.default","model.matrix","lm",".f",".Call","map","as.character","::","eval","eval","lm",".f",".Call","map","as.list","vapply","model.frame.default","stats::model.frame","eval","eval","lm",".f",".Call","map","rep.int","lm.fit","lm",".f",".Call","map","is.data.frame","colSums","poly","eval","eval","model.frame.default","stats::model.frame","eval","eval","lm",".f",".Call","map","NextMethod","[.factor","FUN","lapply","[.tbl_df","[","as.data.frame.resample","as.data.frame","model.frame.default","stats::model.frame","eval","eval","lm",".f",".Call","map","is.data.frame","model.frame.default","stats::model.frame","eval","eval","lm",".f",".Call","map","is.empty.model","lm",".f",".Call","map","pmatch",".deparseOpts","deparse","mode","match","%in%","deparse","paste","FUN","lapply","sapply","model.frame.default","stats::model.frame","eval","eval","lm",".f",".Call","map","makepredictcall.poly","makepredictcall","model.frame.default","stats::model.frame","eval","eval","lm",".f",".Call","map","<GC>","match.fun","vapply","recycle_columns","list_to_tibble","as_tibble.data.frame","[.tbl_df","[","as.data.frame.resample","as.data.frame","model.frame.default","stats::model.frame","eval","eval","lm",".f",".Call","map",".Fortran","qr.default","qr","poly","eval","eval","model.frame.default","stats::model.frame","eval","eval","lm",".f",".Call","map","outer","poly","eval","eval","model.frame.default","stats::model.frame","eval","eval","lm",".f",".Call","map","getExportedValue","::","<Anonymous>",".External2","model.frame.default","stats::model.frame","eval","eval","lm",".f",".Call","map","sys.call","match","%in%","[[.data.frame","[[","[.data.frame","[","na.exclude.data.frame","stats::na.exclude","<Anonymous>",".External2","model.frame.default","stats::model.frame","eval","eval","lm",".f",".Call","map","anyDuplicated.default","anyDuplicated","[.data.frame","[","na.exclude.data.frame","stats::na.exclude","<Anonymous>",".External2","model.frame.default","stats::model.frame","eval","eval","lm",".f",".Call","map","na.exclude.data.frame","stats::na.exclude","<Anonymous>",".External2","model.frame.default","stats::model.frame","eval","eval","lm",".f",".Call","map","model.frame.default","stats::model.frame","eval","eval","lm",".f",".Call","map","length","length","dim.data.frame","dim","dim","ncol","model.matrix.default","model.matrix","lm",".f",".Call","map","na.exclude.data.frame","stats::na.exclude","<Anonymous>",".External2","model.frame.default","stats::model.frame","eval","eval","lm",".f",".Call","map",".External2","model.matrix.default","model.matrix","lm",".f",".Call","map","is.na","na.exclude.data.frame","stats::na.exclude","<Anonymous>",".External2","model.frame.default","stats::model.frame","eval","eval","lm",".f",".Call","map","structure","poly","eval","eval","model.frame.default","stats::model.frame","eval","eval","lm",".f",".Call","map","<GC>","any","model.matrix.default","model.matrix","lm",".f",".Call","map",".deparseOpts","deparse","paste","FUN","lapply","sapply","match","model.matrix.default","model.matrix","lm",".f",".Call","map",".deparseOpts","deparse","paste","FUN","lapply","sapply",".getXlevels","lm",".f",".Call","map","vapply",".getXlevels","lm",".f",".Call","map","paste","FUN","lapply","sapply","model.frame.default","stats::model.frame","eval","eval","lm",".f",".Call","map","structure","poly","eval","eval","model.frame.default","stats::model.frame","eval","eval","lm",".f",".Call","map","all","[.data.frame","[","lapply",".getXlevels","lm",".f",".Call","map","deparse","mode","match","%in%","deparse","paste","FUN","lapply","sapply","model.frame.default","stats::model.frame","eval","eval","lm",".f",".Call","map",".External2","model.matrix.default","model.matrix","lm",".f",".Call","map","vapply",".checkMFClasses","predict.lm","stats::predict","modelr:::residuals",".f",".Call","map2_dbl","model.matrix.default","model.matrix","predict.lm","stats::predict","modelr:::residuals",".f",".Call","map2_dbl","<GC>","list","formula.terms","formula","formula.lm","stats::formula","response_var.default","response_var","eval","response","modelr:::residuals",".f",".Call","map2_dbl","<GC>","list","formula.terms","formula","formula.lm","stats::formula","response_var.default","response_var","eval","response","modelr:::residuals",".f",".Call","map2_dbl","<GC>","list","formula.terms","formula","formula.lm","stats::formula","response_var.default","response_var","eval","response","modelr:::residuals",".f",".Call","map2_dbl","<GC>","list","formula.terms","formula","formula.lm","stats::formula","response_var.default","response_var","eval","response","modelr:::residuals",".f",".Call","map2_dbl","<GC>","list","formula.terms","formula","formula.lm","stats::formula","response_var.default","response_var","eval","response","modelr:::residuals",".f",".Call","map2_dbl","<GC>","list","formula.terms","formula","formula.lm","stats::formula","response_var.default","response_var","eval","response","modelr:::residuals",".f",".Call","map2_dbl","<GC>","list","formula.terms","formula","formula.lm","stats::formula","response_var.default","response_var","eval","response","modelr:::residuals",".f",".Call","map2_dbl","predict.lm","stats::predict","modelr:::residuals",".f",".Call","map2_dbl","$","predict.lm","stats::predict","modelr:::residuals",".f",".Call","map2_dbl","terms","predict.lm","stats::predict","modelr:::residuals",".f",".Call","map2_dbl","formula.lm","stats::formula","response_var.default","response_var","eval","response","modelr:::residuals",".f",".Call","map2_dbl","pmatch",".deparseOpts","deparse","mode","match","%in%","deparse","paste","FUN","lapply","sapply","model.frame.default","model.frame","predict.lm","stats::predict","modelr:::residuals",".f",".Call","map2_dbl","length","FUN","vapply","recycle_columns","list_to_tibble","as_tibble.data.frame","[.tbl_df","[","as.data.frame.resample","as.data.frame","eval","response","modelr:::residuals",".f",".Call","map2_dbl","integer","vapply","recycle_columns","list_to_tibble","as_tibble.data.frame","[.tbl_df","[","as.data.frame.resample","as.data.frame","eval","response","modelr:::residuals",".f",".Call","map2_dbl","FUN","vapply","recycle_columns","list_to_tibble","as_tibble.data.frame","[.tbl_df","[","as.data.frame.resample","as.data.frame","model.frame.default","model.frame","predict.lm","stats::predict","modelr:::residuals",".f",".Call","map2_dbl","predict.lm","stats::predict","modelr:::residuals",".f",".Call","map2_dbl","predict.lm","stats::predict","modelr:::residuals",".f",".Call","map2_dbl","dim","dim","FUN","vapply","recycle_columns","list_to_tibble","as_tibble.data.frame","[.tbl_df","[","as.data.frame.resample","as.data.frame","model.frame.default","model.frame","predict.lm","stats::predict","modelr:::residuals",".f",".Call","map2_dbl","[.factor","FUN","lapply","[.tbl_df","[","as.data.frame.resample","as.data.frame","eval","response","modelr:::residuals",".f",".Call","map2_dbl","model.matrix.default","model.matrix","predict.lm","stats::predict","modelr:::residuals",".f",".Call","map2_dbl","poly","eval","eval","model.frame.default","model.frame","predict.lm","stats::predict","modelr:::residuals",".f",".Call","map2_dbl","as.data.frame.resample","as.data.frame","eval","response","modelr:::residuals",".f",".Call","map2_dbl","as.list","lapply","sapply","model.frame.default","model.frame","predict.lm","stats::predict","modelr:::residuals",".f",".Call","map2_dbl","deparse","paste","FUN","lapply","sapply","model.frame.default","model.frame","predict.lm","stats::predict","modelr:::residuals",".f",".Call","map2_dbl","attr","model.matrix.default","model.matrix","predict.lm","stats::predict","modelr:::residuals",".f",".Call","map2_dbl","deparse","paste","FUN","lapply","sapply","model.frame.default","model.frame","predict.lm","stats::predict","modelr:::residuals",".f",".Call","map2_dbl","[.tbl_df","[","as.data.frame.resample","as.data.frame","eval","response","modelr:::residuals",".f",".Call","map2_dbl",".External2","model.matrix.default","model.matrix","predict.lm","stats::predict","modelr:::residuals",".f",".Call","map2_dbl","[.tbl_df","[","as.data.frame.resample","as.data.frame","eval","response","modelr:::residuals",".f",".Call","map2_dbl","match","%in%",".deparseOpts","deparse","paste","FUN","lapply","sapply","model.frame.default","model.frame","predict.lm","stats::predict","modelr:::residuals",".f",".Call","map2_dbl","paste","FUN","lapply","sapply","match","model.matrix.default","model.matrix","predict.lm","stats::predict","modelr:::residuals",".f",".Call","map2_dbl","poly","eval","eval","model.frame.default","stats::model.frame","eval","eval","lm",".f",".Call","map","terms.formula","terms","model.frame.default","stats::model.frame","eval","eval","lm",".f",".Call","map","qr.default","qr","poly","eval","eval","model.frame.default","stats::model.frame","eval","eval","lm",".f",".Call","map","lapply","[.tbl_df","[","as.data.frame.resample","as.data.frame","model.frame.default","stats::model.frame","eval","eval","lm",".f",".Call","map","unique.default","unique","poly","eval","eval","model.frame.default","stats::model.frame","eval","eval","lm",".f",".Call","map","lapply","[.tbl_df","[","as.data.frame.resample","as.data.frame","model.frame.default","stats::model.frame","eval","eval","lm",".f",".Call","map","lapply","[.tbl_df","[","as.data.frame.resample","as.data.frame","model.frame.default","stats::model.frame","eval","eval","lm",".f",".Call","map","unique.default","unique","poly","eval","eval","model.frame.default","stats::model.frame","eval","eval","lm",".f",".Call","map","unique.default","unique","simplify2array","sapply",".getXlevels","lm",".f",".Call","map","unique.default","unique","poly","eval","eval","model.frame.default","stats::model.frame","eval","eval","lm",".f",".Call","map","!","<Anonymous>",".External2","model.frame.default","stats::model.frame","eval","eval","lm",".f",".Call","map","model.response","lm",".f",".Call","map","<GC>","structure","poly","eval","eval","model.frame.default","stats::model.frame","eval","eval","lm",".f",".Call","map","match","%in%","deparse","mode","match","%in%","deparse","paste","FUN","lapply","sapply",".getXlevels","lm",".f",".Call","map","as.character","model.response","lm",".f",".Call","map","prod","colSums","poly","eval","eval","model.frame.default","stats::model.frame","eval","eval","lm",".f",".Call","map","paste","FUN","lapply","sapply","match","model.matrix.default","model.matrix","lm",".f",".Call","map","as.character","model.response","lm",".f",".Call","map","is.na","na.exclude.data.frame","stats::na.exclude","<Anonymous>",".External2","model.frame.default","stats::model.frame","eval","eval","lm",".f",".Call","map","as.character","model.response","lm",".f",".Call","map","match","%in%","[[.data.frame","[[","model.response","lm",".f",".Call","map",".External2","model.matrix.default","model.matrix","lm",".f",".Call","map","poly","eval","eval","model.frame.default","stats::model.frame","eval","eval","lm",".f",".Call","map","na.exclude.data.frame","stats::na.exclude","<Anonymous>",".External2","model.frame.default","stats::model.frame","eval","eval","lm",".f",".Call","map","match","%in%","[[.data.frame","[[","na.exclude.data.frame","stats::na.exclude","<Anonymous>",".External2","model.frame.default","stats::model.frame","eval","eval","lm",".f",".Call","map","model.frame.default","stats::model.frame","eval","eval","lm",".f",".Call","map","[.data.frame","[","na.exclude.data.frame","stats::na.exclude","<Anonymous>",".External2","model.frame.default","stats::model.frame","eval","eval","lm",".f",".Call","map","na.exclude.data.frame","stats::na.exclude","<Anonymous>",".External2","model.frame.default","stats::model.frame","eval","eval","lm",".f",".Call","map","lapply","sapply","model.frame.default","stats::model.frame","eval","eval","lm",".f",".Call","map","lapply","[.tbl_df","[","as.data.frame.resample","as.data.frame","model.frame.default","stats::model.frame","eval","eval","lm",".f",".Call","map","!","<Anonymous>",".External2","model.frame.default","stats::model.frame","eval","eval","lm",".f",".Call","map","vapply","model.matrix.default","model.matrix","lm",".f",".Call","map","stats::complete.cases","<Anonymous>",".External2","model.frame.default","stats::model.frame","eval","eval","lm",".f",".Call","map","<Anonymous>","[[.data.frame","[[","model.response","lm",".f",".Call","map","structure","poly","eval","eval","model.frame.default","stats::model.frame","eval","eval","lm",".f",".Call","map","is.na","is.na","na.exclude.data.frame","stats::na.exclude","<Anonymous>",".External2","model.frame.default","stats::model.frame","eval","eval","lm",".f",".Call","map","as.character","model.response","lm",".f",".Call","map","qr.default","qr","poly","eval","eval","model.frame.default","stats::model.frame","eval","eval","lm",".f",".Call","map","lm.fit","lm",".f",".Call","map","na.exclude.data.frame","stats::na.exclude","<Anonymous>",".External2","model.frame.default","stats::model.frame","eval","eval","lm",".f",".Call","map","lapply","[.tbl_df","[","as.data.frame.resample","as.data.frame","model.frame.default","stats::model.frame","eval","eval","lm",".f",".Call","map",".External2","model.matrix.default","model.matrix","lm",".f",".Call","map",".deparseOpts","deparse","paste","FUN","lapply","sapply",".getXlevels","lm",".f",".Call","map",".External","terms.formula","terms","model.frame.default","stats::model.frame","eval","eval","lm",".f",".Call","map","lapply","[.tbl_df","[","as.data.frame.resample","as.data.frame","model.frame.default","stats::model.frame","eval","eval","lm",".f",".Call","map","as.list","vapply","recycle_columns","list_to_tibble","as_tibble.data.frame","[.tbl_df","[","as.data.frame.resample","as.data.frame","model.frame.default","stats::model.frame","eval","eval","lm",".f",".Call","map","paste","FUN","lapply","sapply","match","model.matrix.default","model.matrix","lm",".f",".Call","map","is.factor","FUN","lapply",".getXlevels","lm",".f",".Call","map","lapply","[.tbl_df","[","as.data.frame.resample","as.data.frame","model.frame.default","stats::model.frame","eval","eval","lm",".f",".Call","map","match","%in%","deparse","paste","FUN","lapply","sapply","match","model.matrix.default","model.matrix","lm",".f",".Call","map","as.character","model.response","lm",".f",".Call","map","pmatch",".deparseOpts","deparse","mode","match","%in%","deparse","paste","FUN","lapply","sapply",".getXlevels","lm",".f",".Call","map","model.frame.default","stats::model.frame","eval","eval","lm",".f",".Call","map","lapply","sapply",".getXlevels","lm",".f",".Call","map","poly","eval","eval","model.frame.default","model.frame","predict.lm","stats::predict","modelr:::residuals",".f",".Call","map2_dbl",".deparseOpts","deparse","paste","FUN","lapply","sapply","match","model.matrix.default","model.matrix","predict.lm","stats::predict","modelr:::residuals",".f",".Call","map2_dbl","predict.lm","stats::predict","modelr:::residuals",".f",".Call","map2_dbl","deparse","paste","FUN","lapply","sapply","match","model.matrix.default","model.matrix","predict.lm","stats::predict","modelr:::residuals",".f",".Call","map2_dbl","delete.response","predict.lm","stats::predict","modelr:::residuals",".f",".Call","map2_dbl","qr.lm","predict.lm","stats::predict","modelr:::residuals",".f",".Call","map2_dbl","eval","model.frame.default","model.frame","predict.lm","stats::predict","modelr:::residuals",".f",".Call","map2_dbl","c","structure","poly","eval","eval","model.frame.default","model.frame","predict.lm","stats::predict","modelr:::residuals",".f",".Call","map2_dbl","FUN","lapply","sapply","model.frame.default","model.frame","predict.lm","stats::predict","modelr:::residuals",".f",".Call","map2_dbl",".External2","model.frame.default","model.frame","predict.lm","stats::predict","modelr:::residuals",".f",".Call","map2_dbl",".deparseOpts","deparse","paste","FUN","lapply","sapply","match","model.matrix.default","model.matrix","predict.lm","stats::predict","modelr:::residuals",".f",".Call","map2_dbl","model.frame.default","model.frame","predict.lm","stats::predict","modelr:::residuals",".f",".Call","map2_dbl","lapply","sapply","model.frame.default","model.frame","predict.lm","stats::predict","modelr:::residuals",".f",".Call","map2_dbl","vapply","recycle_columns","list_to_tibble","as_tibble.data.frame","[.tbl_df","[","as.data.frame.resample","as.data.frame","model.frame.default","model.frame","predict.lm","stats::predict","modelr:::residuals",".f",".Call","map2_dbl","pmatch",".deparseOpts","deparse","mode","match","%in%","deparse","paste","FUN","lapply","sapply","model.frame.default","model.frame","predict.lm","stats::predict","modelr:::residuals",".f",".Call","map2_dbl","attr","[.factor","FUN","lapply","[.tbl_df","[","as.data.frame.resample","as.data.frame","eval","response","modelr:::residuals",".f",".Call","map2_dbl","vapply","recycle_columns","list_to_tibble","as_tibble.data.frame","[.tbl_df","[","as.data.frame.resample","as.data.frame","eval","response","modelr:::residuals",".f",".Call","map2_dbl","deparse","paste","FUN","lapply","sapply","match","model.matrix.default","model.matrix","predict.lm","stats::predict","modelr:::residuals",".f",".Call","map2_dbl","delete.response","predict.lm","stats::predict","modelr:::residuals",".f",".Call","map2_dbl","getNamespace","asNamespace","get",":::",".f",".Call","map2_dbl","model.frame.default","model.frame","predict.lm","stats::predict","modelr:::residuals",".f",".Call","map2_dbl","predict.lm","stats::predict","modelr:::residuals",".f",".Call","map2_dbl","dim","dim","ncol","paste","FUN","vapply","model.frame.default","model.frame","predict.lm","stats::predict","modelr:::residuals",".f",".Call","map2_dbl","match","%in%","structure","poly","eval","eval","model.frame.default","model.frame","predict.lm","stats::predict","modelr:::residuals",".f",".Call","map2_dbl","as.list.default","as.list","lapply","sapply","match","model.matrix.default","model.matrix","predict.lm","stats::predict","modelr:::residuals",".f",".Call","map2_dbl","recycle_columns","list_to_tibble","as_tibble.data.frame","[.tbl_df","[","as.data.frame.resample","as.data.frame","eval","response","modelr:::residuals",".f",".Call","map2_dbl","as.character","model.response","lm",".f",".Call","map","<GC>","as.list.data.frame","as.list","vapply","model.matrix.default","model.matrix","lm",".f",".Call","map","as.list","lapply","sapply","match","model.matrix.default","model.matrix","lm",".f",".Call","map","getNamespace","asNamespace","getExportedValue","::","<Anonymous>",".External2","model.frame.default","stats::model.frame","eval","eval","lm",".f",".Call","map","%in%","[[.data.frame","[[","$.data.frame","$","model.weights","as.vector","lm",".f",".Call","map","eval","model.frame.default","stats::model.frame","eval","eval","lm",".f",".Call","map","poly","eval","eval","model.frame.default","stats::model.frame","eval","eval","lm",".f",".Call","map","model.frame.default","stats::model.frame","eval","eval","lm",".f",".Call","map","model.response","lm",".f",".Call","map","as.character","model.response","lm",".f",".Call","map","match","model.matrix.default","model.matrix","lm",".f",".Call","map","as.character","model.response","lm",".f",".Call","map","&","recycle_columns","list_to_tibble","as_tibble.data.frame","[.tbl_df","[","as.data.frame.resample","as.data.frame","model.frame.default","stats::model.frame","eval","eval","lm",".f",".Call","map","match","%in%",".deparseOpts","deparse","paste","FUN","lapply","sapply","match","model.matrix.default","model.matrix","lm",".f",".Call","map","[[.data.frame","[[","model.frame.default","stats::model.frame","eval","eval","lm",".f",".Call","map","c","lm.fit","lm",".f",".Call","map","unique.default","unique","poly","eval","eval","model.frame.default","stats::model.frame","eval","eval","lm",".f",".Call","map","lapply","[.tbl_df","[","as.data.frame.resample","as.data.frame","model.frame.default","stats::model.frame","eval","eval","lm",".f",".Call","map","unique.default","unique","poly","eval","eval","model.frame.default","stats::model.frame","eval","eval","lm",".f",".Call","map","[.data.frame","[","na.exclude.data.frame","stats::na.exclude","<Anonymous>",".External2","model.frame.default","stats::model.frame","eval","eval","lm",".f",".Call","map","getExportedValue","::","<Anonymous>",".External2","model.frame.default","stats::model.frame","eval","eval","lm",".f",".Call","map","qr.default","qr","poly","eval","eval","model.frame.default","stats::model.frame","eval","eval","lm",".f",".Call","map","is.data.frame","colSums","poly","eval","eval","model.frame.default","stats::model.frame","eval","eval","lm",".f",".Call","map","poly","eval","eval","model.frame.default","stats::model.frame","eval","eval","lm",".f",".Call","map","anyNA",".deparseOpts","deparse","paste","FUN","lapply","sapply",".getXlevels","lm",".f",".Call","map","[[.data.frame","[[","model.response","lm",".f",".Call","map","lapply","[.tbl_df","[","as.data.frame.resample","as.data.frame","model.frame.default","stats::model.frame","eval","eval","lm",".f",".Call","map","$","as.data.frame.resample","as.data.frame","model.frame.default","stats::model.frame","eval","eval","lm",".f",".Call","map","mode","match","%in%","deparse","paste","FUN","lapply","sapply",".getXlevels","lm",".f",".Call","map","NextMethod","[.factor","FUN","lapply","[.tbl_df","[","as.data.frame.resample","as.data.frame","model.frame.default","stats::model.frame","eval","eval","lm",".f",".Call","map","raw_rownames","list_to_tibble","as_tibble.data.frame","[.tbl_df","[","as.data.frame.resample","as.data.frame","model.frame.default","stats::model.frame","eval","eval","lm",".f",".Call","map","poly","eval","eval","model.frame.default","stats::model.frame","eval","eval","lm",".f",".Call","map","poly","eval","eval","model.frame.default","stats::model.frame","eval","eval","lm",".f",".Call","map","qr","poly","eval","eval","model.frame.default","stats::model.frame","eval","eval","lm",".f",".Call","map","match","structure","poly","eval","eval","model.frame.default","stats::model.frame","eval","eval","lm",".f",".Call","map","$<-","lm",".f",".Call","map","model.frame.default","stats::model.frame","eval","eval","lm",".f",".Call","map","lm.fit","lm",".f",".Call","map",".External2","model.matrix.default","model.matrix","lm",".f",".Call","map","is.factor","FUN","vapply","model.matrix.default","model.matrix","lm",".f",".Call","map","outer","poly","eval","eval","model.frame.default","stats::model.frame","eval","eval","lm",".f",".Call","map",".row_names_info","[.data.frame","[","lapply",".getXlevels","lm",".f",".Call","map","NextMethod","[.data.frame","[","lapply",".getXlevels","lm",".f",".Call","map","na.exclude.data.frame","stats::na.exclude","<Anonymous>",".External2","model.frame.default","stats::model.frame","eval","eval","lm",".f",".Call","map","[[.data.frame","[[","na.exclude.data.frame","stats::na.exclude","<Anonymous>",".External2","model.frame.default","stats::model.frame","eval","eval","lm",".f",".Call","map","outer","poly","eval","eval","model.frame.default","stats::model.frame","eval","eval","lm",".f",".Call","map","match","%in%","[[.data.frame","[[","na.exclude.data.frame","stats::na.exclude","<Anonymous>",".External2","model.frame.default","stats::model.frame","eval","eval","lm",".f",".Call","map","attr","lm.fit","lm",".f",".Call","map",".deparseOpts","deparse","paste","FUN","lapply","sapply",".getXlevels","lm",".f",".Call","map","c","match","%in%","deparse","mode","match","%in%","deparse","paste","FUN","lapply","sapply","model.frame.default","stats::model.frame","eval","eval","lm",".f",".Call","map","dim","dim","ncol","model.matrix.default","model.matrix","lm",".f",".Call","map","poly","eval","eval","model.frame.default","stats::model.frame","eval","eval","lm",".f",".Call","map","is.factor","FUN","vapply",".checkMFClasses","predict.lm","stats::predict","modelr:::residuals",".f",".Call","map2_dbl","==","recycle_columns","list_to_tibble","as_tibble.data.frame","[.tbl_df","[","as.data.frame.resample","as.data.frame","model.frame.default","model.frame","predict.lm","stats::predict","modelr:::residuals",".f",".Call","map2_dbl","poly","eval","eval","model.frame.default","model.frame","predict.lm","stats::predict","modelr:::residuals",".f",".Call","map2_dbl","model.frame.default","model.frame","predict.lm","stats::predict","modelr:::residuals",".f",".Call","map2_dbl","lapply","[.tbl_df","[","as.data.frame.resample","as.data.frame","eval","response","modelr:::residuals",".f",".Call","map2_dbl","terms","predict.lm","stats::predict","modelr:::residuals",".f",".Call","map2_dbl","match","%in%",".checkMFClasses","predict.lm","stats::predict","modelr:::residuals",".f",".Call","map2_dbl","model.frame.default","model.frame","predict.lm","stats::predict","modelr:::residuals",".f",".Call","map2_dbl","lapply","[.tbl_df","[","as.data.frame.resample","as.data.frame","model.frame.default","model.frame","predict.lm","stats::predict","modelr:::residuals",".f",".Call","map2_dbl","colnames<-","poly","eval","eval","model.frame.default","model.frame","predict.lm","stats::predict","modelr:::residuals",".f",".Call","map2_dbl","pmatch",".deparseOpts","deparse","paste","FUN","lapply","sapply","model.frame.default","model.frame","predict.lm","stats::predict","modelr:::residuals",".f",".Call","map2_dbl","pmatch",".deparseOpts","deparse","mode","match","%in%","deparse","paste","FUN","lapply","sapply","model.frame.default","model.frame","predict.lm","stats::predict","modelr:::residuals",".f",".Call","map2_dbl","names","names","model.matrix.default","model.matrix","predict.lm","stats::predict","modelr:::residuals",".f",".Call","map2_dbl","match.fun","vapply","model.matrix.default","model.matrix","predict.lm","stats::predict","modelr:::residuals",".f",".Call","map2_dbl","as.data.frame","model.frame.default","model.frame","predict.lm","stats::predict","modelr:::residuals",".f",".Call","map2_dbl","list_to_tibble","as_tibble.data.frame","[.tbl_df","[","as.data.frame.resample","as.data.frame","eval","response","modelr:::residuals",".f",".Call","map2_dbl","poly","eval","eval","model.frame.default","model.frame","predict.lm","stats::predict","modelr:::residuals",".f",".Call","map2_dbl","length","length","dim.data.frame","dim","dim","nrow","[.tbl_df","[","as.data.frame.resample","as.data.frame","model.frame.default","model.frame","predict.lm","stats::predict","modelr:::residuals",".f",".Call","map2_dbl","predict.lm","stats::predict","modelr:::residuals",".f",".Call","map2_dbl","environment","formula.terms","formula","formula.lm","stats::formula","response_var.default","response_var","eval","response","modelr:::residuals",".f",".Call","map2_dbl","paste","FUN","vapply","model.frame.default","model.frame","predict.lm","stats::predict","modelr:::residuals",".f",".Call","map2_dbl","matrix","poly","eval","eval","model.frame.default","model.frame","predict.lm","stats::predict","modelr:::residuals",".f",".Call","map2_dbl","sapply","match","model.matrix.default","model.matrix","predict.lm","stats::predict","modelr:::residuals",".f",".Call","map2_dbl","<GC>","%in%",".deparseOpts","deparse","paste","FUN","lapply","sapply","model.frame.default","model.frame","predict.lm","stats::predict","modelr:::residuals",".f",".Call","map2_dbl","<GC>","%in%",".deparseOpts","deparse","paste","FUN","lapply","sapply","model.frame.default","model.frame","predict.lm","stats::predict","modelr:::residuals",".f",".Call","map2_dbl","match.fun","vapply","model.matrix.default","model.matrix","predict.lm","stats::predict","modelr:::residuals",".f",".Call","map2_dbl","model.matrix.default","model.matrix","predict.lm","stats::predict","modelr:::residuals",".f",".Call","map2_dbl","getNamespace","asNamespace","get",":::",".f",".Call","map2_dbl","pmatch",".deparseOpts","deparse","paste","FUN","lapply","sapply","model.frame.default","model.frame","predict.lm","stats::predict","modelr:::residuals",".f",".Call","map2_dbl",".External","terms.formula","terms","model.frame.default","stats::model.frame","eval","eval","lm",".f",".Call","map","outer","poly","eval","eval","model.frame.default","stats::model.frame","eval","eval","lm",".f",".Call","map","dim.data.frame","dim","dim","nrow","[.tbl_df","[","as.data.frame.resample","as.data.frame","model.frame.default","stats::model.frame","eval","eval","lm",".f",".Call","map","FUN","lapply","sapply","match","model.matrix.default","model.matrix","lm",".f",".Call","map","unique","simplify2array","sapply","match","model.matrix.default","model.matrix","lm",".f",".Call","map","is.data.frame","colSums","poly","eval","eval","model.frame.default","stats::model.frame","eval","eval","lm",".f",".Call","map","unique.default","unique","simplify2array","sapply",".getXlevels","lm",".f",".Call","map","pmatch",".deparseOpts","deparse","paste","FUN","lapply","sapply","model.frame.default","stats::model.frame","eval","eval","lm",".f",".Call","map","as.list.default","as.list","lapply","sapply","model.frame.default","stats::model.frame","eval","eval","lm",".f",".Call","map","deparse","paste","FUN","lapply","sapply","match","model.matrix.default","model.matrix","lm",".f",".Call","map","poly","eval","eval","model.frame.default","stats::model.frame","eval","eval","lm",".f",".Call","map","poly","eval","eval","model.frame.default","stats::model.frame","eval","eval","lm",".f",".Call","map","model.frame.default","stats::model.frame","eval","eval","lm",".f",".Call","map","lm.fit","lm",".f",".Call","map","FUN","vapply","recycle_columns","list_to_tibble","as_tibble.data.frame","[.tbl_df","[","as.data.frame.resample","as.data.frame","model.frame.default","stats::model.frame","eval","eval","lm",".f",".Call","map",".External2","model.matrix.default","model.matrix","lm",".f",".Call","map",".Fortran","qr.qy","poly","eval","eval","model.frame.default","stats::model.frame","eval","eval","lm",".f",".Call","map","qr.qy","poly","eval","eval","model.frame.default","stats::model.frame","eval","eval","lm",".f",".Call","map","lm",".f",".Call","map","dim","nrow","lm.fit","lm",".f",".Call","map",".getNamespaceInfo","getExportedValue","::","<Anonymous>",".External2","model.frame.default","stats::model.frame","eval","eval","lm",".f",".Call","map","sum",".deparseOpts","deparse","paste","FUN","lapply","sapply",".getXlevels","lm",".f",".Call","map","[[","model.matrix.default","model.matrix","lm",".f",".Call","map","qr.default","qr","poly","eval","eval","model.frame.default","stats::model.frame","eval","eval","lm",".f",".Call","map","unique.default","unique","poly","eval","eval","model.frame.default","stats::model.frame","eval","eval","lm",".f",".Call","map","pmatch",".deparseOpts","deparse","paste","FUN","lapply","sapply",".getXlevels","lm",".f",".Call","map","max","recycle_columns","list_to_tibble","as_tibble.data.frame","[.tbl_df","[","as.data.frame.resample","as.data.frame","model.frame.default","stats::model.frame","eval","eval","lm",".f",".Call","map","qr","poly","eval","eval","model.frame.default","stats::model.frame","eval","eval","lm",".f",".Call","map","outer","poly","eval","eval","model.frame.default","stats::model.frame","eval","eval","lm",".f",".Call","map","is.numeric","is.numeric","FUN","vapply","model.frame.default","stats::model.frame","eval","eval","lm",".f",".Call","map","isTRUE","qr.qy","poly","eval","eval","model.frame.default","stats::model.frame","eval","eval","lm",".f",".Call","map","$","as.data.frame.resample","as.data.frame","model.frame.default","stats::model.frame","eval","eval","lm",".f",".Call","map","na.exclude.data.frame","stats::na.exclude","<Anonymous>",".External2","model.frame.default","stats::model.frame","eval","eval","lm",".f",".Call","map","lm.fit","lm",".f",".Call","map","unique.default","unique","poly","eval","eval","model.frame.default","stats::model.frame","eval","eval","lm",".f",".Call","map",".External2","model.matrix.default","model.matrix","lm",".f",".Call","map","deparse","mode","match","%in%","deparse","paste","FUN","lapply","sapply","match","model.matrix.default","model.matrix","lm",".f",".Call","map","deparse","paste","FUN","lapply","sapply",".getXlevels","lm",".f",".Call","map","na.exclude.data.frame","stats::na.exclude","<Anonymous>",".External2","model.frame.default","stats::model.frame","eval","eval","lm",".f",".Call","map","eval","model.frame.default","stats::model.frame","eval","eval","lm",".f",".Call","map","unique.default","unique","poly","eval","eval","model.frame.default","stats::model.frame","eval","eval","lm",".f",".Call","map","lm",".f",".Call","map","mean.default","mean","poly","eval","eval","model.frame.default","stats::model.frame","eval","eval","lm",".f",".Call","map","[.data.frame","[","na.exclude.data.frame","stats::na.exclude","<Anonymous>",".External2","model.frame.default","stats::model.frame","eval","eval","lm",".f",".Call","map",".External2","model.matrix.default","model.matrix","lm",".f",".Call","map","poly","eval","eval","model.frame.default","stats::model.frame","eval","eval","lm",".f",".Call","map","[.data.frame","[","lapply",".getXlevels","lm",".f",".Call","map","structure","poly","eval","eval","model.frame.default","stats::model.frame","eval","eval","lm",".f",".Call","map","deparse","paste","FUN","lapply","sapply",".getXlevels","lm",".f",".Call","map",".External2","model.matrix.default","model.matrix","lm",".f",".Call","map","lapply","[.tbl_df","[","as.data.frame.resample","as.data.frame","model.frame.default","stats::model.frame","eval","eval","lm",".f",".Call","map","getNamespace","asNamespace","getExportedValue","::","<Anonymous>",".External2","model.frame.default","stats::model.frame","eval","eval","lm",".f",".Call","map","lapply","[.tbl_df","[","as.data.frame.resample","as.data.frame","model.frame.default","stats::model.frame","eval","eval","lm",".f",".Call","map",".Fortran","qr.default","qr","poly","eval","eval","model.frame.default","stats::model.frame","eval","eval","lm",".f",".Call","map","lm",".f",".Call","map","sys.call","match.call","lm",".f",".Call","map","pmatch",".deparseOpts","deparse","paste","FUN","lapply","sapply",".getXlevels","lm",".f",".Call","map","%in%","deparse","mode","match","%in%","deparse","paste","FUN","lapply","sapply","match","model.matrix.default","model.matrix","lm",".f",".Call","map","vapply","recycle_columns","list_to_tibble","as_tibble.data.frame","[.tbl_df","[","as.data.frame.resample","as.data.frame","eval","response","modelr:::residuals",".f",".Call","map2_dbl","mode","match","%in%","deparse","paste","FUN","lapply","sapply","model.frame.default","model.frame","predict.lm","stats::predict","modelr:::residuals",".f",".Call","map2_dbl","model.frame.default","model.frame","predict.lm","stats::predict","modelr:::residuals",".f",".Call","map2_dbl",".deparseOpts","deparse","mode","match","%in%","deparse","paste","FUN","lapply","sapply","match","model.matrix.default","model.matrix","predict.lm","stats::predict","modelr:::residuals",".f",".Call","map2_dbl","eval","match.arg","predict.lm","stats::predict","modelr:::residuals",".f",".Call","map2_dbl","as.list","vapply","recycle_columns","list_to_tibble","as_tibble.data.frame","[.tbl_df","[","as.data.frame.resample","as.data.frame","model.frame.default","model.frame","predict.lm","stats::predict","modelr:::residuals",".f",".Call","map2_dbl","match.arg","predict.lm","stats::predict","modelr:::residuals",".f",".Call","map2_dbl","unlist","simplify2array","sapply","model.frame.default","model.frame","predict.lm","stats::predict","modelr:::residuals",".f",".Call","map2_dbl","is.list","eval","match.arg","predict.lm","stats::predict","modelr:::residuals",".f",".Call","map2_dbl","%in%","structure","poly","eval","eval","model.frame.default","model.frame","predict.lm","stats::predict","modelr:::residuals",".f",".Call","map2_dbl","model.frame.default","model.frame","predict.lm","stats::predict","modelr:::residuals",".f",".Call","map2_dbl","unique","simplify2array","sapply","match","model.matrix.default","model.matrix","predict.lm","stats::predict","modelr:::residuals",".f",".Call","map2_dbl","max","recycle_columns","list_to_tibble","as_tibble.data.frame","[.tbl_df","[","as.data.frame.resample","as.data.frame","eval","response","modelr:::residuals",".f",".Call","map2_dbl","colnames<-","poly","eval","eval","model.frame.default","model.frame","predict.lm","stats::predict","modelr:::residuals",".f",".Call","map2_dbl","unique","simplify2array","sapply","match","model.matrix.default","model.matrix","predict.lm","stats::predict","modelr:::residuals",".f",".Call","map2_dbl","predict.lm","stats::predict","modelr:::residuals",".f",".Call","map2_dbl","mean",".f",".Call","map2_dbl","model.matrix.default","model.matrix","predict.lm","stats::predict","modelr:::residuals",".f",".Call","map2_dbl","deparse","mode","match","%in%","deparse","paste","FUN","lapply","sapply","match","model.matrix.default","model.matrix","predict.lm","stats::predict","modelr:::residuals",".f",".Call","map2_dbl","predict.lm","stats::predict","modelr:::residuals",".f",".Call","map2_dbl","as.character","eval","match.arg","predict.lm","stats::predict","modelr:::residuals",".f",".Call","map2_dbl","unique.default","unique","simplify2array","sapply","model.frame.default","model.frame","predict.lm","stats::predict","modelr:::residuals",".f",".Call","map2_dbl","unique","simplify2array","sapply","model.frame.default","model.frame","predict.lm","stats::predict","modelr:::residuals",".f",".Call","map2_dbl","predict.lm","stats::predict","modelr:::residuals",".f",".Call","map2_dbl","eval","match.arg","predict.lm","stats::predict","modelr:::residuals",".f",".Call","map2_dbl","model.matrix.default","model.matrix","predict.lm","stats::predict","modelr:::residuals",".f",".Call","map2_dbl","c","eval","eval","match.arg","predict.lm","stats::predict","modelr:::residuals",".f",".Call","map2_dbl","%in%","deparse","paste","FUN","lapply","sapply","model.frame.default","model.frame","predict.lm","stats::predict","modelr:::residuals",".f",".Call","map2_dbl","[.tbl_df","[","as.data.frame.resample","as.data.frame","eval","response","modelr:::residuals",".f",".Call","map2_dbl","c","pmatch",".deparseOpts","deparse","mode","match","%in%","deparse","paste","FUN","lapply","sapply",".getXlevels","lm",".f",".Call","map","terms","is.empty.model","lm",".f",".Call","map","$<-","lm",".f",".Call","map","stats::complete.cases","<Anonymous>",".External2","model.frame.default","stats::model.frame","eval","eval","lm",".f",".Call","map","as.list.data.frame","as.list","vapply","model.matrix.default","model.matrix","lm",".f",".Call","map","anyDuplicated.default","anyDuplicated","[.data.frame","[","na.exclude.data.frame","stats::na.exclude","<Anonymous>",".External2","model.frame.default","stats::model.frame","eval","eval","lm",".f",".Call","map","outer","poly","eval","eval","model.frame.default","stats::model.frame","eval","eval","lm",".f",".Call","map","mode","match","%in%","deparse","paste","FUN","lapply","sapply","model.frame.default","stats::model.frame","eval","eval","lm",".f",".Call","map","lm.fit","lm",".f",".Call","map","match","%in%",".deparseOpts","deparse","mode","match","%in%","deparse","paste","FUN","lapply","sapply","model.frame.default","stats::model.frame","eval","eval","lm",".f",".Call","map","names","names","model.matrix.default","model.matrix","lm",".f",".Call","map","as.matrix.default","as.matrix","qr.default","qr","poly","eval","eval","model.frame.default","stats::model.frame","eval","eval","lm",".f",".Call","map","makepredictcall","model.frame.default","stats::model.frame","eval","eval","lm",".f",".Call","map","[[.data.frame","[[","$.data.frame","$","model.weights","as.vector","lm",".f",".Call","map","attr",".getXlevels","lm",".f",".Call","map","match","%in%","[[.data.frame","[[","$.data.frame","$","model.weights","as.vector","lm",".f",".Call","map","unique.default","unique","poly","eval","eval","model.frame.default","stats::model.frame","eval","eval","lm",".f",".Call","map","structure","poly","eval","eval","model.frame.default","stats::model.frame","eval","eval","lm",".f",".Call","map","sys.parent","sys.function","match.call","lm",".f",".Call","map","unique.default","unique","poly","eval","eval","model.frame.default","stats::model.frame","eval","eval","lm",".f",".Call","map","poly","eval","eval","model.frame.default","stats::model.frame","eval","eval","lm",".f",".Call","map","sapply","match","model.matrix.default","model.matrix","lm",".f",".Call","map","as.character","makepredictcall.poly","makepredictcall","model.frame.default","stats::model.frame","eval","eval","lm",".f",".Call","map","lm",".f",".Call","map","c","poly","eval","eval","model.frame.default","stats::model.frame","eval","eval","lm",".f",".Call","map","qr","poly","eval","eval","model.frame.default","stats::model.frame","eval","eval","lm",".f",".Call","map","poly","eval","eval","model.frame.default","stats::model.frame","eval","eval","lm",".f",".Call","map",".deparseOpts","deparse","paste","FUN","lapply","sapply",".getXlevels","lm",".f",".Call","map","unique.default","unique","poly","eval","eval","model.frame.default","stats::model.frame","eval","eval","lm",".f",".Call","map","get0","getExportedValue","::","<Anonymous>",".External2","model.frame.default","stats::model.frame","eval","eval","lm",".f",".Call","map","na.exclude.data.frame","stats::na.exclude","<Anonymous>",".External2","model.frame.default","stats::model.frame","eval","eval","lm",".f",".Call","map","deparse","mode","match","%in%","deparse","paste","FUN","lapply","sapply",".getXlevels","lm",".f",".Call","map","colnames<-","poly","eval","eval","model.frame.default","stats::model.frame","eval","eval","lm",".f",".Call","map","$","lapply","[.tbl_df","[","as.data.frame.resample","as.data.frame","model.frame.default","stats::model.frame","eval","eval","lm",".f",".Call","map","lapply","[.tbl_df","[","as.data.frame.resample","as.data.frame","model.frame.default","stats::model.frame","eval","eval","lm",".f",".Call","map","na.exclude.data.frame","stats::na.exclude","<Anonymous>",".External2","model.frame.default","stats::model.frame","eval","eval","lm",".f",".Call","map","qr.qy","poly","eval","eval","model.frame.default","stats::model.frame","eval","eval","lm",".f",".Call","map","as.character","model.response","lm",".f",".Call","map","deparse","mode","match","%in%","deparse","paste","FUN","lapply","sapply","match","model.matrix.default","model.matrix","lm",".f",".Call","map","outer","poly","eval","eval","model.frame.default","stats::model.frame","eval","eval","lm",".f",".Call","map","asNamespace","getExportedValue","::","<Anonymous>",".External2","model.frame.default","stats::model.frame","eval","eval","lm",".f",".Call","map","is.function","match.fun","sapply","model.frame.default","stats::model.frame","eval","eval","lm",".f",".Call","map","<GC>","paste","FUN","lapply","sapply","model.frame.default","stats::model.frame","eval","eval","lm",".f",".Call","map","na.exclude.data.frame","stats::na.exclude","<Anonymous>",".External2","model.frame.default","stats::model.frame","eval","eval","lm",".f",".Call","map",".External2","model.matrix.default","model.matrix","lm",".f",".Call","map","poly","eval","eval","model.frame.default","stats::model.frame","eval","eval","lm",".f",".Call","map","match","%in%","[[.data.frame","[[","na.exclude.data.frame","stats::na.exclude","<Anonymous>",".External2","model.frame.default","stats::model.frame","eval","eval","lm",".f",".Call","map","outer","poly","eval","eval","model.frame.default","stats::model.frame","eval","eval","lm",".f",".Call","map","poly","eval","eval","model.frame.default","stats::model.frame","eval","eval","lm",".f",".Call","map","poly","eval","eval","model.frame.default","stats::model.frame","eval","eval","lm",".f",".Call","map","lm.fit","lm",".f",".Call","map",".deparseOpts","deparse","paste","FUN","lapply","sapply","model.frame.default","stats::model.frame","eval","eval","lm",".f",".Call","map","<GC>","getExportedValue","::","eval","eval","lm",".f",".Call","map","c","lm.fit","lm",".f",".Call","map","poly","eval","eval","model.frame.default","stats::model.frame","eval","eval","lm",".f",".Call","map","poly","eval","eval","model.frame.default","stats::model.frame","eval","eval","lm",".f",".Call","map","length","length","dim.data.frame","dim","dim","nrow","[.tbl_df","[","as.data.frame.resample","as.data.frame","model.frame.default","stats::model.frame","eval","eval","lm",".f",".Call","map",".External","terms.formula","terms","model.frame.default","stats::model.frame","eval","eval","lm",".f",".Call","map","structure","poly","eval","eval","model.frame.default","stats::model.frame","eval","eval","lm",".f",".Call","map","model.frame.default","stats::model.frame","eval","eval","lm",".f",".Call","map","lapply","[.tbl_df","[","as.data.frame.resample","as.data.frame","model.frame.default","stats::model.frame","eval","eval","lm",".f",".Call","map","integer","vapply","recycle_columns","list_to_tibble","as_tibble.data.frame","[.tbl_df","[","as.data.frame.resample","as.data.frame","model.frame.default","stats::model.frame","eval","eval","lm",".f",".Call","map","[.tbl_df","[","as.data.frame.resample","as.data.frame","model.frame.default","stats::model.frame","eval","eval","lm",".f",".Call","map","unique.default","unique","poly","eval","eval","model.frame.default","stats::model.frame","eval","eval","lm",".f",".Call","map","col","poly","eval","eval","model.frame.default","stats::model.frame","eval","eval","lm",".f",".Call","map","FUN","vapply","recycle_columns","list_to_tibble","as_tibble.data.frame","[.tbl_df","[","as.data.frame.resample","as.data.frame","model.frame.default","stats::model.frame","eval","eval","lm",".f",".Call","map","deparse","paste","FUN","lapply","sapply","match","model.matrix.default","model.matrix","predict.lm","stats::predict","modelr:::residuals",".f",".Call","map2_dbl","lapply","[.tbl_df","[","as.data.frame.resample","as.data.frame","model.frame.default","model.frame","predict.lm","stats::predict","modelr:::residuals",".f",".Call","map2_dbl","c","pmatch",".deparseOpts","deparse","paste","FUN","lapply","sapply","match","model.matrix.default","model.matrix","predict.lm","stats::predict","modelr:::residuals",".f",".Call","map2_dbl","%in%",".deparseOpts","deparse","mode","match","%in%","deparse","paste","FUN","lapply","sapply","match","model.matrix.default","model.matrix","predict.lm","stats::predict","modelr:::residuals",".f",".Call","map2_dbl","model.frame.default","model.frame","predict.lm","stats::predict","modelr:::residuals",".f",".Call","map2_dbl","vapply","recycle_columns","list_to_tibble","as_tibble.data.frame","[.tbl_df","[","as.data.frame.resample","as.data.frame","model.frame.default","model.frame","predict.lm","stats::predict","modelr:::residuals",".f",".Call","map2_dbl","sapply","match","model.matrix.default","model.matrix","predict.lm","stats::predict","modelr:::residuals",".f",".Call","map2_dbl","vapply","model.matrix.default","model.matrix","predict.lm","stats::predict","modelr:::residuals",".f",".Call","map2_dbl","NextMethod","[.factor","FUN","lapply","[.tbl_df","[","as.data.frame.resample","as.data.frame","model.frame.default","model.frame","predict.lm","stats::predict","modelr:::residuals",".f",".Call","map2_dbl","paste","FUN","lapply","sapply","model.frame.default","model.frame","predict.lm","stats::predict","modelr:::residuals",".f",".Call","map2_dbl","pmatch",".deparseOpts","deparse","paste","FUN","lapply","sapply","match","model.matrix.default","model.matrix","predict.lm","stats::predict","modelr:::residuals",".f",".Call","map2_dbl","model.frame.default","model.frame","predict.lm","stats::predict","modelr:::residuals",".f",".Call","map2_dbl","eval","match.arg","predict.lm","stats::predict","modelr:::residuals",".f",".Call","map2_dbl","vapply","recycle_columns","list_to_tibble","as_tibble.data.frame","[.tbl_df","[","as.data.frame.resample","as.data.frame","model.frame.default","model.frame","predict.lm","stats::predict","modelr:::residuals",".f",".Call","map2_dbl","length","length","dim.data.frame","dim","dim","nrow","[.tbl_df","[","as.data.frame.resample","as.data.frame","eval","response","modelr:::residuals",".f",".Call","map2_dbl","response_var","eval","response","modelr:::residuals",".f",".Call","map2_dbl","delete.response","predict.lm","stats::predict","modelr:::residuals",".f",".Call","map2_dbl","predict.lm","stats::predict","modelr:::residuals",".f",".Call","map2_dbl","[.tbl_df","[","as.data.frame.resample","as.data.frame","model.frame.default","model.frame","predict.lm","stats::predict","modelr:::residuals",".f",".Call","map2_dbl","match.arg","predict.lm","stats::predict","modelr:::residuals",".f",".Call","map2_dbl","attr<-","list_to_tibble","as_tibble.data.frame","[.tbl_df","[","as.data.frame.resample","as.data.frame","model.frame.default","model.frame","predict.lm","stats::predict","modelr:::residuals",".f",".Call","map2_dbl","predict.lm","stats::predict","modelr:::residuals",".f",".Call","map2_dbl","$","predict.lm","stats::predict","modelr:::residuals",".f",".Call","map2_dbl","[.factor","FUN","lapply","[.tbl_df","[","as.data.frame.resample","as.data.frame","model.frame.default","model.frame","predict.lm","stats::predict","modelr:::residuals",".f",".Call","map2_dbl","<GC>","lapply","[.tbl_df","[","as.data.frame.resample","as.data.frame","eval","response","modelr:::residuals",".f",".Call","map2_dbl","$","model.matrix.default","model.matrix","predict.lm","stats::predict","modelr:::residuals",".f",".Call","map2_dbl","formals","match.arg","predict.lm","stats::predict","modelr:::residuals",".f",".Call","map2_dbl","match","%in%",".checkMFClasses","predict.lm","stats::predict","modelr:::residuals",".f",".Call","map2_dbl",".row_names_info","dim.data.frame","dim","dim","nrow","[.tbl_df","[","as.data.frame.resample","as.data.frame","eval","response","modelr:::residuals",".f",".Call","map2_dbl"],"filenum":[null,null,1,null,null,1,null,null,1,null,null,1,null,null,1,null,null,1,null,null,1,null,null,1,null,null,1,null,null,1,null,null,1,null,null,1,null,null,1,null,null,1,null,null,1,null,null,1,null,null,1,null,null,null,1,null,null,null,null,null,null,1,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,1,null,null,null,null,null,null,1,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,1,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,1,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,1,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,1,null,null,null,null,null,1,null,null,null,null,null,null,1,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,1,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,1,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,1,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,1,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,1,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,1,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,1,null,null,null,null,null,null,1,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,1,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1],"linenum":[null,null,8,null,null,8,null,null,8,null,null,8,null,null,8,null,null,8,null,null,8,null,null,8,null,null,8,null,null,8,null,null,8,null,null,8,null,null,8,null,null,8,null,null,8,null,null,8,null,null,8,null,null,null,8,null,null,null,null,null,null,8,null,null,null,null,null,null,8,null,null,null,null,null,null,null,null,null,null,null,null,8,null,null,null,null,8,null,null,null,null,null,null,null,null,8,null,null,null,null,null,null,null,null,8,null,null,null,null,null,null,null,null,null,8,null,null,null,null,null,null,null,null,null,null,8,null,null,null,null,null,null,null,null,null,null,null,null,8,null,null,null,null,null,null,null,null,null,null,null,null,null,8,null,null,null,null,null,null,null,null,null,null,8,null,null,null,null,null,null,null,null,null,null,null,null,null,null,8,null,null,null,null,null,null,null,null,null,null,null,null,8,null,null,null,null,null,null,null,null,null,null,8,null,null,null,null,null,null,null,null,null,null,null,null,8,null,null,null,null,null,null,null,null,null,null,null,8,null,null,null,null,null,null,8,null,null,null,null,null,null,null,8,null,null,null,null,null,null,null,null,null,null,null,null,8,null,null,null,null,null,null,null,null,null,null,null,null,8,null,null,null,null,null,null,null,null,8,null,null,null,null,null,null,null,8,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,8,null,null,null,null,null,null,null,null,null,null,null,null,8,null,null,null,null,null,null,null,8,null,null,null,null,null,null,null,null,null,8,null,null,null,null,null,8,null,null,null,null,null,null,null,null,null,null,null,null,8,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,8,null,null,null,null,null,null,null,null,8,null,null,null,null,8,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,8,null,null,null,null,null,null,null,null,null,8,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,8,null,null,null,null,null,null,null,null,null,null,null,null,null,8,null,null,null,null,null,null,null,null,null,null,null,8,null,null,null,null,null,null,null,null,null,null,null,8,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,8,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,8,null,null,null,null,null,null,null,null,null,null,null,8,null,null,null,null,null,null,null,8,null,null,null,null,null,null,null,null,null,null,null,8,null,null,null,null,null,null,null,null,null,null,null,8,null,null,null,null,null,null,8,null,null,null,null,null,null,null,null,null,null,null,null,8,null,null,null,null,null,null,null,null,null,null,null,8,null,null,null,null,null,null,null,8,null,null,null,null,null,null,null,null,null,null,null,null,8,null,null,null,null,null,null,null,null,null,null,8,null,null,null,null,null,8,null,null,null,null,null,null,null,null,null,null,null,8,null,null,null,null,null,null,null,null,null,null,null,8,null,null,null,null,null,null,null,null,8,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,8,null,null,null,null,null,null,8,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,9,null,null,null,null,null,null,9,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,9,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,8,null,null,null,null,null,null,null,null,null,8,null,null,null,null,null,null,null,null,null,null,null,null,8,null,null,null,null,null,null,null,null,null,null,null,null,8,null,null,null,null,null,null,null,null,null,null,null,null,8,null,null,null,null,null,null,null,null,null,null,null,null,8,null,null,null,null,null,null,null,null,null,null,null,null,8,null,null,null,null,null,null,null,null,null,null,null,null,8,null,null,null,null,null,null,null,null,8,null,null,null,null,null,null,null,null,null,null,null,null,8,null,null,null,null,null,null,null,null,null,null,8,null,null,null,null,8,null,null,null,null,null,null,null,null,null,null,null,null,8,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,8,null,null,null,null,null,8,null,null,null,null,null,null,null,null,null,null,null,null,8,null,null,null,null,null,null,null,null,null,null,8,null,null,null,null,null,8,null,null,null,null,null,null,null,null,null,null,null,null,8,null,null,null,null,null,8,null,null,null,null,null,null,null,null,8,null,null,null,null,null,null,8,null,null,null,null,null,null,null,null,null,null,8,null,null,null,null,null,null,null,null,null,null,null,8,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,8,null,null,null,null,null,null,null,8,null,null,null,null,null,null,null,null,null,null,null,null,null,8,null,null,null,null,null,null,null,null,null,null,null,8,null,null,null,null,null,null,null,null,null,8,null,null,null,null,null,null,null,null,null,null,null,null,8,null,null,null,null,null,null,null,null,null,null,8,null,null,null,null,null,null,8,null,null,null,null,null,null,null,null,null,null,8,null,null,null,null,null,null,null,8,null,null,null,null,null,null,null,null,null,null,null,8,null,null,null,null,null,null,null,null,null,null,null,null,null,8,null,null,null,null,null,8,null,null,null,null,null,null,null,null,null,null,null,null,8,null,null,null,null,8,null,null,null,null,null,null,null,null,null,null,null,8,null,null,null,null,null,null,null,null,null,null,null,null,8,null,null,null,null,null,null,8,null,null,null,null,null,null,null,null,null,null,8,null,null,null,null,null,null,null,null,null,null,8,null,null,null,null,null,null,null,null,null,null,null,null,8,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,8,null,null,null,null,null,null,null,null,null,null,8,null,null,null,null,null,null,null,8,null,null,null,null,null,null,null,null,null,null,null,null,8,null,null,null,null,null,null,null,null,null,null,null,null,null,8,null,null,null,null,null,8,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,8,null,null,null,null,null,null,null,8,null,null,null,null,null,null,8,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,9,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,9,null,null,null,null,null,null,9,null,null,null,null,null,null,null,9,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,8,null,null,null,null,null,null,null,null,null,8,null,null,null,null,null,null,null,null,null,8,null,null,null,null,null,null,null,null,null,null,null,null,null,8,null,null,null,null,null,null,null,null,null,null,8,null,null,null,null,null,null,null,null,8,null,null,null,null,null,null,null,null,null,null,8,null,null,null,null,null,null,null,8,null,null,null,null,8,null,null,null,null,null,8,null,null,null,null,null,null,8,null,null,null,null,null,8,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,8,null,null,null,null,null,null,null,null,null,null,null,null,null,null,8,null,null,null,null,null,null,null,null,null,8,null,null,null,null,null,8,null,null,null,null,null,null,null,null,null,null,null,null,8,null,null,null,null,null,null,null,null,null,null,null,null,8,null,null,null,null,null,null,null,null,null,null,null,null,8,null,null,null,null,null,null,null,null,null,null,null,null,null,8,null,null,null,null,null,null,null,null,null,null,null,8,null,null,null,null,null,null,null,null,null,null,null,null,8,null,null,null,null,null,null,null,null,null,null,null,null,8,null,null,null,null,null,null,null,null,null,null,8,null,null,null,null,null,null,null,null,null,null,null,8,null,null,null,null,null,null,8,null,null,null,null,null,null,null,null,null,null,null,null,8,null,null,null,null,null,null,null,null,null,null,8,null,null,null,null,null,null,null,null,null,null,null,null,8,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,8,null,null,null,null,null,null,null,null,null,null,null,null,null,null,8,null,null,null,null,null,null,null,null,null,null,8,null,null,null,null,null,null,null,null,null,null,8,null,null,null,null,null,null,null,null,null,null,null,8,null,null,null,null,null,null,null,null,null,null,null,null,8,null,null,null,null,8,null,null,null,null,null,null,null,8,null,null,null,null,8,null,null,null,null,null,null,8,null,null,null,null,null,null,null,null,8,null,null,null,null,null,null,null,null,null,null,null,8,null,null,null,null,null,null,null,null,8,null,null,null,null,null,null,null,null,8,null,null,null,null,null,null,null,null,null,null,null,8,null,null,null,null,null,null,null,null,null,null,null,null,null,8,null,null,null,null,null,null,null,null,null,null,null,8,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,8,null,null,null,null,null,8,null,null,null,null,null,null,null,null,null,null,8,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,8,null,null,null,null,null,null,null,null,8,null,null,null,null,null,null,null,null,null,null,8,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,9,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,8,null,null,null,null,null,null,null,null,null,null,null,8,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,8,null,null,null,null,null,null,null,null,null,8,null,null,null,null,null,null,null,null,null,8,null,null,null,null,null,null,null,null,null,null,null,null,8,null,null,null,null,null,null,null,null,8,null,null,null,null,null,null,null,null,null,null,null,null,null,null,8,null,null,null,null,null,null,null,null,null,null,null,8,null,null,null,null,null,null,null,null,null,null,null,8,null,null,null,null,null,null,null,null,null,null,8,null,null,null,null,null,null,null,null,null,null,8,null,null,null,null,null,null,null,8,null,null,null,null,8,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,8,null,null,null,null,null,null,8,null,null,null,null,null,null,null,null,null,null,null,null,8,null,null,null,null,null,null,null,null,null,null,null,8,null,null,null,8,null,null,null,null,null,null,8,null,null,null,null,null,null,null,null,null,null,null,null,8,null,null,null,null,null,null,null,null,null,null,null,8,null,null,null,null,null,null,8,null,null,null,null,null,null,null,null,null,null,null,null,8,null,null,null,null,null,null,null,null,null,null,null,null,8,null,null,null,null,null,null,null,null,null,null,null,8,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,8,null,null,null,null,null,null,null,null,null,null,null,8,null,null,null,null,null,null,null,null,null,null,null,8,null,null,null,null,null,null,null,null,null,null,null,8,null,null,null,null,null,null,null,null,null,null,null,null,8,null,null,null,null,null,null,null,null,null,null,8,null,null,null,null,null,null,null,null,null,null,null,8,null,null,null,null,8,null,null,null,null,null,null,null,null,null,null,null,null,8,null,null,null,null,null,null,8,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,8,null,null,null,null,null,null,null,null,null,8,null,null,null,null,null,null,null,null,null,null,null,8,null,null,null,null,null,null,null,null,8,null,null,null,null,null,null,null,null,null,null,null,null,8,null,null,null,8,null,null,null,null,null,null,null,null,null,null,null,null,8,null,null,null,null,null,null,null,null,null,null,null,null,null,8,null,null,null,null,null,null,8,null,null,null,null,null,null,null,null,null,null,8,null,null,null,null,null,null,null,8,null,null,null,null,null,null,null,null,null,null,null,8,null,null,null,null,null,null,null,null,null,8,null,null,null,null,null,null,8,null,null,null,null,null,null,null,null,null,null,null,null,8,null,null,null,null,null,null,null,null,null,null,null,null,null,8,null,null,null,null,null,null,null,null,null,null,null,null,8,null,null,null,null,null,null,null,null,null,null,null,null,null,8,null,null,null,8,null,null,null,null,null,8,null,null,null,null,null,null,null,null,null,null,null,8,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,8,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,9,null,null,null,9,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,9,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,9,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,8,null,null,null,null,null,8,null,null,null,null,8,null,null,null,null,null,null,null,null,null,null,8,null,null,null,null,null,null,null,null,8,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,8,null,null,null,null,null,null,null,null,null,null,null,8,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,8,null,null,null,null,8,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,8,null,null,null,null,null,null,null,8,null,null,null,null,null,null,null,null,null,null,null,null,null,null,8,null,null,null,null,null,null,null,null,8,null,null,null,null,null,null,null,null,null,8,null,null,null,null,null,8,null,null,null,null,null,null,null,null,null,null,null,8,null,null,null,null,null,null,null,null,null,null,null,null,8,null,null,null,null,null,null,null,null,null,null,null,8,null,null,null,null,null,null,8,null,null,null,null,null,null,null,null,null,null,null,null,8,null,null,null,null,null,null,null,null,null,null,8,null,null,null,null,null,null,null,8,null,null,null,null,null,null,null,null,null,null,8,null,null,null,8,null,null,null,null,null,null,null,null,null,null,null,8,null,null,null,null,null,null,null,null,null,null,null,8,null,null,null,null,null,null,null,null,null,null,8,null,null,null,null,null,null,null,null,null,null,8,null,null,null,null,null,null,null,null,null,null,null,null,8,null,null,null,null,null,null,null,null,null,null,null,null,8,null,null,null,null,null,null,null,null,null,null,null,8,null,null,null,null,null,null,null,null,null,null,null,null,null,8,null,null,null,null,null,null,null,null,null,null,null,8,null,null,null,null,null,null,null,null,null,null,null,null,null,8,null,null,null,null,null,null,null,null,null,null,null,null,8,null,null,null,null,null,null,null,null,null,null,null,8,null,null,null,null,null,null,null,null,null,null,null,8,null,null,null,null,null,8,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,8,null,null,null,null,null,null,null,null,null,null,null,8,null,null,null,null,null,null,null,null,null,null,null,null,8,null,null,null,null,null,null,null,null,null,null,8,null,null,null,null,null,null,null,null,null,null,null,null,8,null,null,null,null,null,null,null,null,null,null,null,8,null,null,null,null,null,null,8,null,null,null,null,null,null,null,null,null,null,8,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,8,null,null,null,null,null,null,null,null,null,null,null,8,null,null,null,null,null,null,null,null,null,null,8,null,null,null,null,null,null,null,null,null,null,8,null,null,null,null,8,null,null,null,null,null,null,null,null,null,null,null,null,null,8,null,null,null,null,null,null,null,null,8,null,null,null,null,null,8,null,null,null,null,null,null,null,null,null,null,8,null,null,null,null,null,null,null,null,null,null,8,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,8,null,null,null,null,null,null,null,null,null,null,8,null,null,null,null,null,null,null,null,null,null,null,8,null,null,null,null,null,null,null,8,null,null,null,null,null,null,null,null,null,null,null,null,8,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,8,null,null,null,null,null,null,null,null,null,null,null,8,null,null,null,null,null,null,null,null,null,null,null,null,8,null,null,null,null,null,null,null,null,null,null,null,8,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,8,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,9,null,null,null,null,null,null,9,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,9,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,null,9],"memalloc":[132.510009765625,132.510009765625,132.510009765625,162.126831054688,162.126831054688,162.126831054688,162.126831054688,162.126831054688,162.126831054688,162.126831054688,162.126831054688,162.126831054688,162.126831054688,162.126831054688,162.126831054688,162.126831054688,162.126831054688,162.126831054688,162.126831054688,162.126831054688,162.126831054688,191.74365234375,191.74365234375,191.74365234375,191.74365234375,191.74365234375,191.74365234375,194.748092651367,194.748092651367,194.748092651367,197.837066650391,197.837066650391,197.837066650391,200.989448547363,200.989448547363,200.989448547363,204.146537780762,204.146537780762,204.146537780762,207.024673461914,207.024673461914,207.024673461914,209.422546386719,209.422546386719,209.422546386719,212.24275970459,212.24275970459,212.24275970459,214.885787963867,214.885787963867,214.885787963867,217.056350708008,217.056350708008,217.056350708008,217.056350708008,218.564361572266,218.564361572266,218.564361572266,218.564361572266,218.564361572266,218.564361572266,218.564361572266,220.116310119629,220.116310119629,220.116310119629,220.116310119629,220.116310119629,220.116310119629,220.116310119629,221.480499267578,221.480499267578,221.480499267578,221.480499267578,221.480499267578,221.480499267578,221.480499267578,221.480499267578,221.480499267578,221.480499267578,221.480499267578,221.480499267578,221.480499267578,223.214286804199,223.214286804199,223.214286804199,223.214286804199,223.214286804199,224.77766418457,224.77766418457,224.77766418457,224.77766418457,224.77766418457,224.77766418457,224.77766418457,224.77766418457,224.77766418457,226.028396606445,226.028396606445,226.028396606445,226.028396606445,226.028396606445,226.028396606445,226.028396606445,226.028396606445,226.028396606445,227.55834197998,227.55834197998,227.55834197998,227.55834197998,227.55834197998,227.55834197998,227.55834197998,227.55834197998,227.55834197998,227.55834197998,228.893753051758,228.893753051758,228.893753051758,228.893753051758,228.893753051758,228.893753051758,228.893753051758,228.893753051758,228.893753051758,228.893753051758,228.893753051758,163.263458251953,163.263458251953,163.263458251953,163.263458251953,163.263458251953,163.263458251953,163.263458251953,163.263458251953,163.263458251953,163.263458251953,163.263458251953,163.263458251953,163.263458251953,153.218688964844,153.218688964844,153.218688964844,153.218688964844,153.218688964844,153.218688964844,153.218688964844,153.218688964844,153.218688964844,153.218688964844,153.218688964844,153.218688964844,153.218688964844,153.218688964844,156.051803588867,156.051803588867,156.051803588867,156.051803588867,156.051803588867,156.051803588867,156.051803588867,156.051803588867,156.051803588867,156.051803588867,156.051803588867,157.59481048584,157.59481048584,157.59481048584,157.59481048584,157.59481048584,157.59481048584,157.59481048584,157.59481048584,157.59481048584,157.59481048584,157.59481048584,157.59481048584,157.59481048584,157.59481048584,157.59481048584,159.366188049316,159.366188049316,159.366188049316,159.366188049316,159.366188049316,159.366188049316,159.366188049316,159.366188049316,159.366188049316,159.366188049316,159.366188049316,159.366188049316,159.366188049316,160.801788330078,160.801788330078,160.801788330078,160.801788330078,160.801788330078,160.801788330078,160.801788330078,160.801788330078,160.801788330078,160.801788330078,160.801788330078,162.405166625977,162.405166625977,162.405166625977,162.405166625977,162.405166625977,162.405166625977,162.405166625977,162.405166625977,162.405166625977,162.405166625977,162.405166625977,162.405166625977,162.405166625977,164.420593261719,164.420593261719,164.420593261719,164.420593261719,164.420593261719,164.420593261719,164.420593261719,164.420593261719,164.420593261719,164.420593261719,164.420593261719,164.420593261719,155.201530456543,155.201530456543,155.201530456543,155.201530456543,155.201530456543,155.201530456543,155.201530456543,158.021850585938,158.021850585938,158.021850585938,158.021850585938,158.021850585938,158.021850585938,158.021850585938,158.021850585938,159.741882324219,159.741882324219,159.741882324219,159.741882324219,159.741882324219,159.741882324219,159.741882324219,159.741882324219,159.741882324219,159.741882324219,159.741882324219,159.741882324219,159.741882324219,161.453170776367,161.453170776367,161.453170776367,161.453170776367,161.453170776367,161.453170776367,161.453170776367,161.453170776367,161.453170776367,161.453170776367,161.453170776367,161.453170776367,161.453170776367,163.169540405273,163.169540405273,163.169540405273,163.169540405273,163.169540405273,163.169540405273,163.169540405273,163.169540405273,163.169540405273,165.214775085449,165.214775085449,165.214775085449,165.214775085449,165.214775085449,165.214775085449,165.214775085449,165.214775085449,156.095596313477,156.095596313477,156.095596313477,156.095596313477,156.095596313477,156.095596313477,156.095596313477,156.095596313477,156.095596313477,156.095596313477,156.095596313477,156.095596313477,156.095596313477,156.095596313477,156.095596313477,156.095596313477,158.153022766113,158.153022766113,158.153022766113,158.153022766113,158.153022766113,158.153022766113,158.153022766113,158.153022766113,158.153022766113,158.153022766113,158.153022766113,158.153022766113,158.153022766113,159.995384216309,159.995384216309,159.995384216309,159.995384216309,159.995384216309,159.995384216309,159.995384216309,159.995384216309,162.517044067383,162.517044067383,162.517044067383,162.517044067383,162.517044067383,162.517044067383,162.517044067383,162.517044067383,162.517044067383,162.517044067383,164.359497070312,164.359497070312,164.359497070312,164.359497070312,164.359497070312,164.359497070312,166.819053649902,166.819053649902,166.819053649902,166.819053649902,166.819053649902,166.819053649902,166.819053649902,166.819053649902,166.819053649902,166.819053649902,166.819053649902,166.819053649902,166.819053649902,157.807884216309,157.807884216309,157.807884216309,157.807884216309,157.807884216309,157.807884216309,157.807884216309,157.807884216309,157.807884216309,157.807884216309,157.807884216309,157.807884216309,157.807884216309,157.807884216309,157.807884216309,157.807884216309,159.83073425293,159.83073425293,159.83073425293,159.83073425293,159.83073425293,159.83073425293,159.83073425293,159.83073425293,159.83073425293,163.135765075684,163.135765075684,163.135765075684,163.135765075684,163.135765075684,164.763069152832,164.763069152832,164.763069152832,164.763069152832,164.763069152832,164.763069152832,164.763069152832,164.763069152832,164.763069152832,164.763069152832,164.763069152832,164.763069152832,164.763069152832,164.763069152832,164.763069152832,164.763069152832,164.763069152832,164.763069152832,164.763069152832,166.68074798584,166.68074798584,166.68074798584,166.68074798584,166.68074798584,166.68074798584,166.68074798584,166.68074798584,166.68074798584,166.68074798584,168.364013671875,168.364013671875,168.364013671875,168.364013671875,168.364013671875,168.364013671875,168.364013671875,168.364013671875,168.364013671875,168.364013671875,168.364013671875,168.364013671875,168.364013671875,168.364013671875,168.364013671875,168.364013671875,168.364013671875,168.364013671875,159.301483154297,159.301483154297,159.301483154297,159.301483154297,159.301483154297,159.301483154297,159.301483154297,159.301483154297,159.301483154297,159.301483154297,159.301483154297,159.301483154297,159.301483154297,159.301483154297,161.09073638916,161.09073638916,161.09073638916,161.09073638916,161.09073638916,161.09073638916,161.09073638916,161.09073638916,161.09073638916,161.09073638916,161.09073638916,161.09073638916,162.98609161377,162.98609161377,162.98609161377,162.98609161377,162.98609161377,162.98609161377,162.98609161377,162.98609161377,162.98609161377,162.98609161377,162.98609161377,162.98609161377,165.575675964355,165.575675964355,165.575675964355,165.575675964355,165.575675964355,165.575675964355,165.575675964355,165.575675964355,165.575675964355,165.575675964355,165.575675964355,165.575675964355,165.575675964355,165.575675964355,165.575675964355,165.575675964355,165.575675964355,165.575675964355,165.575675964355,167.402435302734,167.402435302734,167.402435302734,167.402435302734,167.402435302734,167.402435302734,167.402435302734,167.402435302734,167.402435302734,167.402435302734,167.402435302734,167.402435302734,167.402435302734,167.402435302734,167.402435302734,167.402435302734,169.171295166016,169.171295166016,169.171295166016,169.171295166016,169.171295166016,169.171295166016,169.171295166016,169.171295166016,169.171295166016,169.171295166016,169.171295166016,169.171295166016,160.370162963867,160.370162963867,160.370162963867,160.370162963867,160.370162963867,160.370162963867,160.370162963867,160.370162963867,162.391105651855,162.391105651855,162.391105651855,162.391105651855,162.391105651855,162.391105651855,162.391105651855,162.391105651855,162.391105651855,162.391105651855,162.391105651855,162.391105651855,164.4013671875,164.4013671875,164.4013671875,164.4013671875,164.4013671875,164.4013671875,164.4013671875,164.4013671875,164.4013671875,164.4013671875,164.4013671875,164.4013671875,166.260063171387,166.260063171387,166.260063171387,166.260063171387,166.260063171387,166.260063171387,166.260063171387,167.998741149902,167.998741149902,167.998741149902,167.998741149902,167.998741149902,167.998741149902,167.998741149902,167.998741149902,167.998741149902,167.998741149902,167.998741149902,167.998741149902,167.998741149902,169.533538818359,169.533538818359,169.533538818359,169.533538818359,169.533538818359,169.533538818359,169.533538818359,169.533538818359,169.533538818359,169.533538818359,169.533538818359,169.533538818359,166.696228027344,166.696228027344,166.696228027344,166.696228027344,166.696228027344,166.696228027344,166.696228027344,166.696228027344,162.336585998535,162.336585998535,162.336585998535,162.336585998535,162.336585998535,162.336585998535,162.336585998535,162.336585998535,162.336585998535,162.336585998535,162.336585998535,162.336585998535,162.336585998535,165.71720123291,165.71720123291,165.71720123291,165.71720123291,165.71720123291,165.71720123291,165.71720123291,165.71720123291,165.71720123291,165.71720123291,165.71720123291,168.55094909668,168.55094909668,168.55094909668,168.55094909668,168.55094909668,168.55094909668,170.396049499512,170.396049499512,170.396049499512,170.396049499512,170.396049499512,170.396049499512,170.396049499512,170.396049499512,170.396049499512,170.396049499512,170.396049499512,170.396049499512,172.049896240234,172.049896240234,172.049896240234,172.049896240234,172.049896240234,172.049896240234,172.049896240234,172.049896240234,172.049896240234,172.049896240234,172.049896240234,172.049896240234,162.961906433105,162.961906433105,162.961906433105,162.961906433105,162.961906433105,162.961906433105,162.961906433105,162.961906433105,162.961906433105,164.293037414551,164.293037414551,164.293037414551,164.293037414551,164.293037414551,164.293037414551,164.293037414551,164.293037414551,164.293037414551,164.293037414551,164.293037414551,164.293037414551,164.293037414551,164.293037414551,164.293037414551,164.293037414551,164.293037414551,166.025344848633,166.025344848633,166.025344848633,166.025344848633,166.025344848633,166.025344848633,166.025344848633,167.871971130371,167.871971130371,167.871971130371,167.871971130371,167.871971130371,167.871971130371,167.871971130371,167.871971130371,168.085296630859,168.085296630859,168.085296630859,168.085296630859,168.085296630859,168.085296630859,168.085296630859,168.085296630859,168.230812072754,168.230812072754,168.230812072754,168.230812072754,168.230812072754,168.230812072754,168.230812072754,168.230812072754,168.230812072754,168.230812072754,168.230812072754,168.230812072754,168.230812072754,168.230812072754,168.230812072754,168.230812072754,168.230812072754,168.230812072754,168.230812072754,168.230812072754,168.230812072754,168.230812072754,168.230812072754,168.230812072754,168.230812072754,168.230812072754,168.230812072754,168.230812072754,168.230812072754,168.230812072754,168.230812072754,168.230812072754,168.230812072754,168.230812072754,168.230812072754,168.230812072754,168.230812072754,168.230812072754,168.230812072754,168.230812072754,168.230812072754,168.230812072754,168.230812072754,168.230812072754,168.230812072754,168.230812072754,168.230812072754,168.230812072754,168.230812072754,168.230812072754,168.230812072754,168.230812072754,168.230812072754,168.230812072754,168.230812072754,168.230812072754,168.230812072754,168.230812072754,168.230812072754,168.230812072754,168.230812072754,168.230812072754,168.230812072754,168.230812072754,168.230812072754,168.230812072754,168.230812072754,168.230812072754,168.230812072754,168.230812072754,168.230812072754,168.230812072754,168.230812072754,168.230812072754,168.230812072754,168.230812072754,168.230812072754,168.230812072754,168.230812072754,168.230812072754,168.230812072754,168.230812072754,168.230812072754,168.230812072754,162.598472595215,162.598472595215,162.598472595215,162.598472595215,162.598472595215,162.598472595215,162.598472595215,162.598472595215,162.598472595215,162.598472595215,162.598472595215,162.598472595215,162.598472595215,162.598472595215,162.611793518066,162.611793518066,162.611793518066,162.611793518066,162.611793518066,162.611793518066,162.774787902832,162.774787902832,162.774787902832,162.774787902832,162.774787902832,162.774787902832,162.774787902832,162.932487487793,162.932487487793,162.932487487793,162.932487487793,162.932487487793,162.932487487793,162.932487487793,163.086196899414,163.086196899414,163.086196899414,163.086196899414,163.086196899414,163.086196899414,163.086196899414,163.086196899414,163.086196899414,163.086196899414,163.268531799316,163.268531799316,163.268531799316,163.268531799316,163.268531799316,163.268531799316,163.268531799316,163.268531799316,163.268531799316,163.268531799316,163.268531799316,163.268531799316,163.268531799316,163.268531799316,163.268531799316,163.268531799316,163.268531799316,163.268531799316,163.268531799316,163.430679321289,163.430679321289,163.430679321289,163.430679321289,163.430679321289,163.430679321289,163.430679321289,163.430679321289,163.430679321289,163.430679321289,163.430679321289,163.430679321289,163.430679321289,163.430679321289,163.430679321289,163.430679321289,163.652236938477,163.652236938477,163.652236938477,163.652236938477,163.652236938477,163.652236938477,163.652236938477,163.652236938477,163.652236938477,163.652236938477,163.652236938477,163.652236938477,163.652236938477,163.652236938477,163.652236938477,163.787651062012,163.787651062012,163.787651062012,163.787651062012,163.787651062012,163.787651062012,163.787651062012,163.787651062012,163.787651062012,163.787651062012,163.787651062012,163.787651062012,163.787651062012,163.787651062012,163.787651062012,163.787651062012,163.787651062012,163.916595458984,163.916595458984,163.916595458984,163.916595458984,163.916595458984,163.916595458984,162.749588012695,162.749588012695,162.749588012695,162.749588012695,162.749588012695,162.749588012695,162.895370483398,162.895370483398,162.895370483398,162.895370483398,162.895370483398,162.895370483398,162.895370483398,162.895370483398,162.895370483398,162.895370483398,162.895370483398,162.895370483398,162.895370483398,162.895370483398,162.895370483398,162.895370483398,162.895370483398,162.895370483398,162.895370483398,163.147743225098,163.147743225098,163.147743225098,163.147743225098,163.147743225098,163.147743225098,163.147743225098,163.147743225098,163.147743225098,163.147743225098,163.147743225098,163.147743225098,163.147743225098,163.345390319824,163.345390319824,163.345390319824,163.345390319824,163.345390319824,163.345390319824,163.345390319824,163.345390319824,163.474548339844,163.474548339844,163.474548339844,163.474548339844,163.474548339844,163.474548339844,163.474548339844,163.474548339844,163.474548339844,163.474548339844,163.474548339844,163.691780090332,163.691780090332,163.691780090332,163.691780090332,163.691780090332,163.691780090332,163.691780090332,163.691780090332,163.860412597656,163.860412597656,163.860412597656,163.860412597656,163.860412597656,163.860412597656,163.860412597656,163.860412597656,163.860412597656,163.860412597656,163.860412597656,162.669883728027,162.669883728027,162.669883728027,162.669883728027,162.669883728027,162.669883728027,162.669883728027,162.669883728027,162.669883728027,162.669883728027,162.669883728027,162.669883728027,162.669883728027,162.897178649902,162.897178649902,162.897178649902,162.897178649902,162.897178649902,162.897178649902,162.897178649902,162.897178649902,162.897178649902,163.036178588867,163.036178588867,163.036178588867,163.036178588867,163.036178588867,163.036178588867,163.036178588867,163.036178588867,163.036178588867,163.036178588867,163.036178588867,163.036178588867,163.036178588867,163.209732055664,163.209732055664,163.209732055664,163.209732055664,163.209732055664,163.209732055664,163.209732055664,163.209732055664,163.209732055664,163.209732055664,163.352104187012,163.352104187012,163.352104187012,163.352104187012,163.352104187012,163.352104187012,163.352104187012,163.352104187012,163.352104187012,163.49747467041,163.49747467041,163.49747467041,163.49747467041,163.49747467041,163.49747467041,163.49747467041,163.49747467041,163.49747467041,163.49747467041,163.635925292969,163.635925292969,163.635925292969,163.635925292969,163.635925292969,163.635925292969,163.635925292969,163.635925292969,163.635925292969,163.635925292969,163.635925292969,163.635925292969,163.635925292969,163.635925292969,163.635925292969,163.635925292969,163.80591583252,163.80591583252,163.80591583252,163.80591583252,163.80591583252,163.80591583252,163.80591583252,163.80591583252,163.80591583252,163.80591583252,163.80591583252,163.80591583252,163.80591583252,163.262489318848,163.262489318848,163.262489318848,163.262489318848,163.262489318848,163.262489318848,163.262489318848,163.262489318848,163.262489318848,163.262489318848,163.262489318848,166.087554931641,166.087554931641,166.087554931641,166.087554931641,166.087554931641,166.087554931641,166.087554931641,166.087554931641,166.087554931641,166.087554931641,168.776039123535,168.776039123535,168.776039123535,168.776039123535,168.776039123535,168.776039123535,168.776039123535,168.776039123535,168.776039123535,168.776039123535,168.776039123535,168.776039123535,168.776039123535,170.331321716309,170.331321716309,170.331321716309,170.331321716309,170.331321716309,170.331321716309,170.331321716309,170.331321716309,170.331321716309,170.331321716309,170.331321716309,170.331321716309,170.331321716309,172.352813720703,172.352813720703,172.352813720703,172.352813720703,172.352813720703,172.352813720703,172.352813720703,172.352813720703,172.352813720703,172.352813720703,172.352813720703,172.352813720703,172.352813720703,174.286911010742,174.286911010742,174.286911010742,174.286911010742,174.286911010742,174.286911010742,174.286911010742,174.286911010742,174.286911010742,174.286911010742,174.286911010742,174.286911010742,174.286911010742,176.263214111328,176.263214111328,176.263214111328,176.263214111328,176.263214111328,176.263214111328,176.263214111328,176.263214111328,176.263214111328,176.263214111328,176.263214111328,176.263214111328,176.263214111328,177.952339172363,177.952339172363,177.952339172363,177.952339172363,177.952339172363,177.952339172363,177.952339172363,177.952339172363,177.952339172363,177.952339172363,177.952339172363,177.952339172363,177.952339172363,179.879356384277,179.879356384277,179.879356384277,179.879356384277,179.879356384277,179.879356384277,179.879356384277,179.879356384277,179.879356384277,181.901039123535,181.901039123535,181.901039123535,181.901039123535,181.901039123535,181.901039123535,181.901039123535,181.901039123535,181.901039123535,181.901039123535,181.901039123535,181.901039123535,181.901039123535,184.049072265625,184.049072265625,184.049072265625,184.049072265625,184.049072265625,184.049072265625,184.049072265625,184.049072265625,184.049072265625,184.049072265625,184.049072265625,186.08910369873,186.08910369873,186.08910369873,186.08910369873,186.08910369873,187.656753540039,187.656753540039,187.656753540039,187.656753540039,187.656753540039,187.656753540039,187.656753540039,187.656753540039,187.656753540039,187.656753540039,187.656753540039,187.656753540039,187.656753540039,168.630241394043,168.630241394043,168.630241394043,168.630241394043,168.630241394043,168.630241394043,168.630241394043,168.630241394043,168.630241394043,168.630241394043,168.630241394043,168.630241394043,168.630241394043,168.630241394043,168.630241394043,168.630241394043,171.550331115723,171.550331115723,171.550331115723,171.550331115723,171.550331115723,171.550331115723,173.735481262207,173.735481262207,173.735481262207,173.735481262207,173.735481262207,173.735481262207,173.735481262207,173.735481262207,173.735481262207,173.735481262207,173.735481262207,173.735481262207,173.735481262207,175.835441589355,175.835441589355,175.835441589355,175.835441589355,175.835441589355,175.835441589355,175.835441589355,175.835441589355,175.835441589355,175.835441589355,175.835441589355,178.467376708984,178.467376708984,178.467376708984,178.467376708984,178.467376708984,178.467376708984,180.384719848633,180.384719848633,180.384719848633,180.384719848633,180.384719848633,180.384719848633,180.384719848633,180.384719848633,180.384719848633,180.384719848633,180.384719848633,180.384719848633,180.384719848633,182.749359130859,182.749359130859,182.749359130859,182.749359130859,182.749359130859,182.749359130859,185.373908996582,185.373908996582,185.373908996582,185.373908996582,185.373908996582,185.373908996582,185.373908996582,185.373908996582,185.373908996582,187.375259399414,187.375259399414,187.375259399414,187.375259399414,187.375259399414,187.375259399414,187.375259399414,189.491775512695,189.491775512695,189.491775512695,189.491775512695,189.491775512695,189.491775512695,189.491775512695,189.491775512695,189.491775512695,189.491775512695,189.491775512695,169.509963989258,169.509963989258,169.509963989258,169.509963989258,169.509963989258,169.509963989258,169.509963989258,169.509963989258,169.509963989258,169.509963989258,169.509963989258,169.509963989258,171.474128723145,171.474128723145,171.474128723145,171.474128723145,171.474128723145,171.474128723145,171.474128723145,171.474128723145,171.474128723145,171.474128723145,171.474128723145,171.474128723145,171.474128723145,171.474128723145,171.474128723145,171.474128723145,174.104393005371,174.104393005371,174.104393005371,174.104393005371,174.104393005371,174.104393005371,174.104393005371,174.104393005371,177.0966796875,177.0966796875,177.0966796875,177.0966796875,177.0966796875,177.0966796875,177.0966796875,177.0966796875,177.0966796875,177.0966796875,177.0966796875,177.0966796875,177.0966796875,177.0966796875,180.038017272949,180.038017272949,180.038017272949,180.038017272949,180.038017272949,180.038017272949,180.038017272949,180.038017272949,180.038017272949,180.038017272949,180.038017272949,180.038017272949,182.498237609863,182.498237609863,182.498237609863,182.498237609863,182.498237609863,182.498237609863,182.498237609863,182.498237609863,182.498237609863,182.498237609863,184.767127990723,184.767127990723,184.767127990723,184.767127990723,184.767127990723,184.767127990723,184.767127990723,184.767127990723,184.767127990723,184.767127990723,184.767127990723,184.767127990723,184.767127990723,186.624084472656,186.624084472656,186.624084472656,186.624084472656,186.624084472656,186.624084472656,186.624084472656,186.624084472656,186.624084472656,186.624084472656,186.624084472656,188.670799255371,188.670799255371,188.670799255371,188.670799255371,188.670799255371,188.670799255371,188.670799255371,190.245796203613,190.245796203613,190.245796203613,190.245796203613,190.245796203613,190.245796203613,190.245796203613,190.245796203613,190.245796203613,190.245796203613,190.245796203613,191.950401306152,191.950401306152,191.950401306152,191.950401306152,191.950401306152,191.950401306152,191.950401306152,191.950401306152,172.278747558594,172.278747558594,172.278747558594,172.278747558594,172.278747558594,172.278747558594,172.278747558594,172.278747558594,172.278747558594,172.278747558594,172.278747558594,172.278747558594,174.927932739258,174.927932739258,174.927932739258,174.927932739258,174.927932739258,174.927932739258,174.927932739258,174.927932739258,174.927932739258,174.927932739258,174.927932739258,174.927932739258,174.927932739258,174.927932739258,177.624931335449,177.624931335449,177.624931335449,177.624931335449,177.624931335449,177.624931335449,180.062324523926,180.062324523926,180.062324523926,180.062324523926,180.062324523926,180.062324523926,180.062324523926,180.062324523926,180.062324523926,180.062324523926,180.062324523926,180.062324523926,180.062324523926,182.607666015625,182.607666015625,182.607666015625,182.607666015625,182.607666015625,185.474220275879,185.474220275879,185.474220275879,185.474220275879,185.474220275879,185.474220275879,185.474220275879,185.474220275879,185.474220275879,185.474220275879,185.474220275879,185.474220275879,187.559074401855,187.559074401855,187.559074401855,187.559074401855,187.559074401855,187.559074401855,187.559074401855,187.559074401855,187.559074401855,187.559074401855,187.559074401855,187.559074401855,187.559074401855,190.159034729004,190.159034729004,190.159034729004,190.159034729004,190.159034729004,190.159034729004,190.159034729004,192.821937561035,192.821937561035,192.821937561035,192.821937561035,192.821937561035,192.821937561035,192.821937561035,192.821937561035,192.821937561035,192.821937561035,192.821937561035,174.561363220215,174.561363220215,174.561363220215,174.561363220215,174.561363220215,174.561363220215,174.561363220215,174.561363220215,174.561363220215,174.561363220215,174.561363220215,177.846817016602,177.846817016602,177.846817016602,177.846817016602,177.846817016602,177.846817016602,177.846817016602,177.846817016602,177.846817016602,177.846817016602,177.846817016602,177.846817016602,177.846817016602,180.489433288574,180.489433288574,180.489433288574,180.489433288574,180.489433288574,180.489433288574,180.489433288574,180.489433288574,180.489433288574,180.489433288574,180.489433288574,180.489433288574,180.489433288574,180.489433288574,180.489433288574,180.489433288574,180.489433288574,183.698120117188,183.698120117188,183.698120117188,183.698120117188,183.698120117188,183.698120117188,183.698120117188,183.698120117188,183.698120117188,183.698120117188,183.698120117188,186.047912597656,186.047912597656,186.047912597656,186.047912597656,186.047912597656,186.047912597656,186.047912597656,186.047912597656,190.004127502441,190.004127502441,190.004127502441,190.004127502441,190.004127502441,190.004127502441,190.004127502441,190.004127502441,190.004127502441,190.004127502441,190.004127502441,190.004127502441,190.004127502441,191.93334197998,191.93334197998,191.93334197998,191.93334197998,191.93334197998,191.93334197998,191.93334197998,191.93334197998,191.93334197998,191.93334197998,191.93334197998,191.93334197998,191.93334197998,191.93334197998,194.565826416016,194.565826416016,194.565826416016,194.565826416016,194.565826416016,194.565826416016,196.916152954102,196.916152954102,196.916152954102,196.916152954102,196.916152954102,196.916152954102,196.916152954102,196.916152954102,196.916152954102,196.916152954102,196.916152954102,196.916152954102,196.916152954102,196.916152954102,196.916152954102,196.916152954102,178.71174621582,178.71174621582,178.71174621582,178.71174621582,178.71174621582,178.71174621582,178.71174621582,178.71174621582,182.660423278809,182.660423278809,182.660423278809,182.660423278809,182.660423278809,182.660423278809,182.660423278809,183.684532165527,183.684532165527,183.684532165527,183.684532165527,183.684532165527,183.684532165527,183.684532165527,183.684532165527,183.684532165527,183.684532165527,183.684532165527,183.802490234375,183.802490234375,183.802490234375,183.802490234375,183.802490234375,183.802490234375,183.802490234375,183.802490234375,183.802490234375,183.802490234375,183.802490234375,183.802490234375,183.802490234375,183.802490234375,183.802490234375,183.986854553223,183.986854553223,183.986854553223,183.986854553223,183.986854553223,183.986854553223,184.146362304688,184.146362304688,184.146362304688,184.146362304688,184.146362304688,184.146362304688,184.146362304688,184.146362304688,184.146362304688,184.146362304688,184.146362304688,184.146362304688,184.146362304688,184.146362304688,184.310539245605,184.310539245605,184.310539245605,184.310539245605,184.310539245605,184.310539245605,184.310539245605,184.491004943848,184.491004943848,184.491004943848,184.491004943848,184.491004943848,184.491004943848,184.491004943848,177.340812683105,177.340812683105,177.340812683105,177.340812683105,177.340812683105,177.340812683105,177.340812683105,177.340812683105,177.340812683105,177.51261138916,177.51261138916,177.51261138916,177.51261138916,177.51261138916,177.51261138916,177.51261138916,177.51261138916,177.51261138916,177.51261138916,177.51261138916,177.51261138916,177.51261138916,177.681365966797,177.681365966797,177.681365966797,177.681365966797,177.681365966797,177.681365966797,177.681365966797,177.681365966797,177.681365966797,177.681365966797,177.681365966797,177.810661315918,177.810661315918,177.810661315918,177.810661315918,177.810661315918,177.810661315918,177.810661315918,177.810661315918,177.810661315918,177.973388671875,177.973388671875,177.973388671875,177.973388671875,177.973388671875,177.973388671875,177.973388671875,177.973388671875,177.973388671875,177.973388671875,177.973388671875,177.973388671875,177.973388671875,177.973388671875,177.973388671875,178.184097290039,178.184097290039,178.184097290039,178.184097290039,178.184097290039,178.184097290039,178.184097290039,178.184097290039,178.334548950195,178.334548950195,178.334548950195,178.334548950195,178.334548950195,178.334548950195,178.334548950195,178.334548950195,178.334548950195,178.334548950195,178.494194030762,178.494194030762,178.494194030762,178.494194030762,178.494194030762,178.494194030762,178.494194030762,178.494194030762,178.494194030762,178.494194030762,178.494194030762,178.494194030762,178.494194030762,178.494194030762,178.494194030762,178.494194030762,177.374801635742,177.374801635742,177.374801635742,177.374801635742,177.374801635742,177.374801635742,177.374801635742,177.374801635742,177.374801635742,177.374801635742,177.374801635742,177.374801635742,177.374801635742,177.374801635742,177.374801635742,177.374801635742,177.374801635742,177.374801635742,177.374801635742,177.565589904785,177.565589904785,177.565589904785,177.565589904785,177.565589904785,177.565589904785,177.565589904785,177.565589904785,177.565589904785,177.565589904785,177.565589904785,177.565589904785,177.565589904785,177.565589904785,177.726196289062,177.726196289062,177.726196289062,177.726196289062,177.726196289062,177.726196289062,177.726196289062,177.726196289062,177.726196289062,177.726196289062,177.726196289062,177.726196289062,177.726196289062,177.726196289062,177.883880615234,177.883880615234,177.883880615234,177.883880615234,177.883880615234,177.883880615234,177.883880615234,177.883880615234,177.883880615234,177.883880615234,177.883880615234,177.883880615234,177.883880615234,177.883880615234,178.048553466797,178.048553466797,178.048553466797,178.048553466797,178.048553466797,178.048553466797,178.048553466797,178.22957611084,178.22957611084,178.22957611084,178.22957611084,178.22957611084,178.22957611084,178.22957611084,178.415351867676,178.415351867676,178.415351867676,178.415351867676,178.415351867676,178.415351867676,178.415351867676,178.415351867676,177.283561706543,177.283561706543,177.283561706543,177.283561706543,177.283561706543,177.283561706543,177.472618103027,177.472618103027,177.472618103027,177.472618103027,177.472618103027,177.472618103027,177.472618103027,177.472618103027,177.472618103027,177.472618103027,177.472618103027,177.472618103027,177.472618103027,177.472618103027,177.644065856934,177.644065856934,177.644065856934,177.644065856934,177.644065856934,177.644065856934,177.644065856934,177.644065856934,177.644065856934,177.644065856934,177.644065856934,177.644065856934,177.644065856934,177.644065856934,177.931465148926,177.931465148926,177.931465148926,177.931465148926,177.931465148926,177.931465148926,177.931465148926,177.931465148926,177.931465148926,177.931465148926,177.931465148926,177.931465148926,177.931465148926,178.119476318359,178.119476318359,178.119476318359,178.119476318359,178.119476318359,178.119476318359,178.119476318359,178.119476318359,178.119476318359,178.119476318359,178.119476318359,178.119476318359,178.119476318359,180.258728027344,180.258728027344,180.258728027344,180.258728027344,180.258728027344,180.258728027344,182.669692993164,182.669692993164,182.669692993164,182.669692993164,182.669692993164,182.669692993164,182.669692993164,182.669692993164,182.669692993164,182.669692993164,180.081756591797,180.081756591797,180.081756591797,180.081756591797,180.081756591797,180.081756591797,180.081756591797,180.081756591797,180.081756591797,180.081756591797,183.209045410156,183.209045410156,183.209045410156,183.209045410156,183.209045410156,183.209045410156,183.209045410156,183.209045410156,183.209045410156,183.209045410156,183.209045410156,183.209045410156,183.209045410156,183.209045410156,186.099304199219,186.099304199219,186.099304199219,186.099304199219,186.099304199219,186.099304199219,186.099304199219,186.099304199219,186.099304199219,186.099304199219,186.099304199219,189.405754089355,189.405754089355,189.405754089355,189.405754089355,189.405754089355,189.405754089355,189.405754089355,189.405754089355,189.405754089355,192.697174072266,192.697174072266,192.697174072266,192.697174072266,192.697174072266,192.697174072266,192.697174072266,192.697174072266,192.697174072266,192.697174072266,192.697174072266,196.850006103516,196.850006103516,196.850006103516,196.850006103516,196.850006103516,196.850006103516,196.850006103516,196.850006103516,198.930335998535,198.930335998535,198.930335998535,198.930335998535,198.930335998535,201.746269226074,201.746269226074,201.746269226074,201.746269226074,201.746269226074,201.746269226074,204.156875610352,204.156875610352,204.156875610352,204.156875610352,204.156875610352,204.156875610352,204.156875610352,181.086563110352,181.086563110352,181.086563110352,181.086563110352,181.086563110352,181.086563110352,183.988456726074,183.988456726074,183.988456726074,183.988456726074,183.988456726074,183.988456726074,183.988456726074,183.988456726074,183.988456726074,183.988456726074,183.988456726074,183.988456726074,183.988456726074,183.988456726074,183.988456726074,183.988456726074,187.105895996094,187.105895996094,187.105895996094,187.105895996094,187.105895996094,187.105895996094,187.105895996094,187.105895996094,187.105895996094,187.105895996094,187.105895996094,187.105895996094,187.105895996094,187.105895996094,187.105895996094,192.71061706543,192.71061706543,192.71061706543,192.71061706543,192.71061706543,192.71061706543,192.71061706543,192.71061706543,192.71061706543,192.71061706543,196.381278991699,196.381278991699,196.381278991699,196.381278991699,196.381278991699,196.381278991699,198.439292907715,198.439292907715,198.439292907715,198.439292907715,198.439292907715,198.439292907715,198.439292907715,198.439292907715,198.439292907715,198.439292907715,198.439292907715,198.439292907715,198.439292907715,200.831199645996,200.831199645996,200.831199645996,200.831199645996,200.831199645996,200.831199645996,200.831199645996,200.831199645996,200.831199645996,200.831199645996,200.831199645996,200.831199645996,200.831199645996,203.253776550293,203.253776550293,203.253776550293,203.253776550293,203.253776550293,203.253776550293,203.253776550293,203.253776550293,203.253776550293,203.253776550293,203.253776550293,203.253776550293,203.253776550293,205.52271270752,205.52271270752,205.52271270752,205.52271270752,205.52271270752,205.52271270752,205.52271270752,205.52271270752,205.52271270752,205.52271270752,205.52271270752,205.52271270752,205.52271270752,205.52271270752,207.888702392578,207.888702392578,207.888702392578,207.888702392578,207.888702392578,207.888702392578,207.888702392578,207.888702392578,207.888702392578,207.888702392578,207.888702392578,207.888702392578,185.020523071289,185.020523071289,185.020523071289,185.020523071289,185.020523071289,185.020523071289,185.020523071289,185.020523071289,185.020523071289,185.020523071289,185.020523071289,185.020523071289,185.020523071289,187.530227661133,187.530227661133,187.530227661133,187.530227661133,187.530227661133,187.530227661133,187.530227661133,187.530227661133,187.530227661133,187.530227661133,187.530227661133,187.530227661133,187.530227661133,190.363159179688,190.363159179688,190.363159179688,190.363159179688,190.363159179688,190.363159179688,190.363159179688,190.363159179688,190.363159179688,190.363159179688,190.363159179688,193.748527526855,193.748527526855,193.748527526855,193.748527526855,193.748527526855,193.748527526855,193.748527526855,193.748527526855,193.748527526855,193.748527526855,193.748527526855,193.748527526855,197.700469970703,197.700469970703,197.700469970703,197.700469970703,197.700469970703,197.700469970703,197.700469970703,201.380508422852,201.380508422852,201.380508422852,201.380508422852,201.380508422852,201.380508422852,201.380508422852,201.380508422852,201.380508422852,201.380508422852,201.380508422852,201.380508422852,201.380508422852,203.782867431641,203.782867431641,203.782867431641,203.782867431641,203.782867431641,203.782867431641,203.782867431641,203.782867431641,203.782867431641,203.782867431641,203.782867431641,206.587860107422,206.587860107422,206.587860107422,206.587860107422,206.587860107422,206.587860107422,206.587860107422,206.587860107422,206.587860107422,206.587860107422,206.587860107422,206.587860107422,206.587860107422,210.639617919922,210.639617919922,210.639617919922,210.639617919922,210.639617919922,210.639617919922,210.639617919922,210.639617919922,210.639617919922,210.639617919922,210.639617919922,210.639617919922,210.639617919922,210.639617919922,210.639617919922,210.639617919922,189.185958862305,189.185958862305,189.185958862305,189.185958862305,189.185958862305,189.185958862305,189.185958862305,189.185958862305,189.185958862305,189.185958862305,189.185958862305,189.185958862305,189.185958862305,189.185958862305,189.185958862305,192.175811767578,192.175811767578,192.175811767578,192.175811767578,192.175811767578,192.175811767578,192.175811767578,192.175811767578,192.175811767578,192.175811767578,192.175811767578,195.411087036133,195.411087036133,195.411087036133,195.411087036133,195.411087036133,195.411087036133,195.411087036133,195.411087036133,195.411087036133,195.411087036133,195.411087036133,198.456130981445,198.456130981445,198.456130981445,198.456130981445,198.456130981445,198.456130981445,198.456130981445,198.456130981445,198.456130981445,198.456130981445,198.456130981445,198.456130981445,201.429382324219,201.429382324219,201.429382324219,201.429382324219,201.429382324219,201.429382324219,201.429382324219,201.429382324219,201.429382324219,201.429382324219,201.429382324219,201.429382324219,201.429382324219,204.390731811523,204.390731811523,204.390731811523,204.390731811523,204.390731811523,206.438285827637,206.438285827637,206.438285827637,206.438285827637,206.438285827637,206.438285827637,206.438285827637,206.438285827637,209.9755859375,209.9755859375,209.9755859375,209.9755859375,209.9755859375,212.783966064453,212.783966064453,212.783966064453,212.783966064453,212.783966064453,212.783966064453,212.783966064453,193.804092407227,193.804092407227,193.804092407227,193.804092407227,193.804092407227,193.804092407227,193.804092407227,193.804092407227,193.804092407227,197.146087646484,197.146087646484,197.146087646484,197.146087646484,197.146087646484,197.146087646484,197.146087646484,197.146087646484,197.146087646484,197.146087646484,197.146087646484,197.146087646484,200.271606445312,200.271606445312,200.271606445312,200.271606445312,200.271606445312,200.271606445312,200.271606445312,200.271606445312,200.271606445312,204.283592224121,204.283592224121,204.283592224121,204.283592224121,204.283592224121,204.283592224121,204.283592224121,204.283592224121,204.283592224121,208.596534729004,208.596534729004,208.596534729004,208.596534729004,208.596534729004,208.596534729004,208.596534729004,208.596534729004,208.596534729004,208.596534729004,208.596534729004,208.596534729004,211.777534484863,211.777534484863,211.777534484863,211.777534484863,211.777534484863,211.777534484863,211.777534484863,211.777534484863,211.777534484863,211.777534484863,211.777534484863,211.777534484863,211.777534484863,211.777534484863,214.374046325684,214.374046325684,214.374046325684,214.374046325684,214.374046325684,214.374046325684,214.374046325684,214.374046325684,214.374046325684,214.374046325684,214.374046325684,214.374046325684,192.668296813965,192.668296813965,192.668296813965,192.668296813965,192.668296813965,192.668296813965,192.668296813965,192.668296813965,192.668296813965,192.668296813965,192.668296813965,192.668296813965,192.668296813965,192.668296813965,192.668296813965,192.668296813965,196.404586791992,196.404586791992,196.404586791992,196.404586791992,196.404586791992,196.404586791992,199.214881896973,199.214881896973,199.214881896973,199.214881896973,199.214881896973,199.214881896973,199.214881896973,199.214881896973,199.214881896973,199.214881896973,199.214881896973,201.667343139648,201.667343139648,201.667343139648,201.667343139648,201.667343139648,201.667343139648,201.667343139648,201.667343139648,201.667343139648,201.667343139648,201.667343139648,201.667343139648,201.667343139648,201.667343139648,201.667343139648,201.667343139648,201.667343139648,201.667343139648,201.667343139648,201.667343139648,204.78491973877,204.78491973877,204.78491973877,204.78491973877,204.78491973877,204.78491973877,204.78491973877,204.78491973877,204.78491973877,207.463813781738,207.463813781738,207.463813781738,207.463813781738,207.463813781738,207.463813781738,207.463813781738,207.463813781738,207.463813781738,207.463813781738,207.463813781738,209.270637512207,209.270637512207,209.270637512207,209.270637512207,209.270637512207,209.270637512207,209.270637512207,209.270637512207,209.270637512207,209.270637512207,209.441162109375,209.441162109375,209.441162109375,209.441162109375,209.441162109375,209.441162109375,209.441162109375,209.441162109375,209.441162109375,209.441162109375,209.441162109375,209.441162109375,209.441162109375,209.441162109375,209.441162109375,209.441162109375,209.629905700684,209.629905700684,209.629905700684,209.629905700684,209.629905700684,209.629905700684,209.629905700684,209.629905700684,209.629905700684,209.629905700684,209.629905700684,194.295555114746,194.295555114746,194.295555114746,194.295555114746,194.295555114746,194.295555114746,194.295555114746,194.295555114746,194.568092346191,194.568092346191,194.568092346191,194.568092346191,194.568092346191,194.568092346191,194.568092346191,194.568092346191,194.568092346191,194.568092346191,194.568092346191,194.70873260498,194.70873260498,194.70873260498,194.70873260498,194.70873260498,194.70873260498,194.70873260498,194.889450073242,194.889450073242,194.889450073242,194.889450073242,194.889450073242,194.889450073242,194.889450073242,194.889450073242,194.889450073242,195.075424194336,195.075424194336,195.075424194336,195.075424194336,195.075424194336,195.075424194336,195.075424194336,195.075424194336,195.257118225098,195.257118225098,195.257118225098,195.257118225098,195.257118225098,195.257118225098,195.257118225098,195.257118225098,195.257118225098,195.257118225098,195.257118225098,195.257118225098,195.257118225098,195.377235412598,195.377235412598,195.377235412598,195.377235412598,195.377235412598,195.377235412598,195.377235412598,195.377235412598,195.377235412598,195.377235412598,195.377235412598,195.377235412598,194.416473388672,194.416473388672,194.416473388672,194.416473388672,194.416473388672,194.416473388672,194.416473388672,194.416473388672,194.416473388672,194.416473388672,194.416473388672,194.416473388672,194.416473388672,194.416473388672,194.416473388672,194.545532226562,194.545532226562,194.545532226562,194.545532226562,194.545532226562,194.545532226562,194.545532226562,194.545532226562,194.545532226562,194.545532226562,194.545532226562,194.545532226562,194.545532226562,194.545532226562,194.545532226562,194.545532226562,194.545532226562,194.545532226562,194.545532226562,194.748489379883,194.748489379883,194.748489379883,194.748489379883,194.748489379883,194.748489379883,194.748489379883,194.748489379883,194.748489379883,194.748489379883,194.923271179199,194.923271179199,194.923271179199,194.923271179199,194.923271179199,194.923271179199,194.923271179199,194.923271179199,194.923271179199,194.923271179199,195.031936645508,195.031936645508,195.031936645508,195.031936645508,195.031936645508,195.031936645508,195.031936645508,195.031936645508,195.031936645508,195.228103637695,195.228103637695,195.228103637695,195.228103637695,195.228103637695,195.228103637695,195.228103637695,195.228103637695,195.228103637695,195.228103637695,195.228103637695,195.228103637695,195.40796661377,195.40796661377,195.40796661377,195.40796661377,195.40796661377,195.40796661377,195.40796661377,195.40796661377,195.40796661377,195.40796661377,195.40796661377,194.366500854492,194.366500854492,194.366500854492,194.366500854492,194.366500854492,194.366500854492,194.366500854492,194.366500854492,194.366500854492,194.366500854492,194.366500854492,194.366500854492,194.366500854492,194.366500854492,194.366500854492,194.366500854492,194.366500854492,194.366500854492,194.549133300781,194.549133300781,194.549133300781,194.549133300781,194.549133300781,194.549133300781,194.701698303223,194.701698303223,194.701698303223,194.701698303223,194.701698303223,194.701698303223,194.701698303223,194.701698303223,194.701698303223,194.701698303223,194.701698303223,194.701698303223,194.701698303223,194.988487243652,194.988487243652,194.988487243652,194.988487243652,194.988487243652,194.988487243652,194.988487243652,194.988487243652,194.988487243652,194.988487243652,194.988487243652,195.150016784668,195.150016784668,195.150016784668,195.150016784668,195.150016784668,195.150016784668,195.150016784668,195.150016784668,195.150016784668,195.150016784668,195.150016784668,195.150016784668,195.291595458984,195.291595458984,195.291595458984,195.291595458984,195.291595458984,195.291595458984,195.291595458984,195.291595458984,195.291595458984,195.291595458984,195.439483642578,195.439483642578,195.439483642578,195.439483642578,195.439483642578,195.439483642578,195.439483642578,195.439483642578,195.439483642578,195.439483642578,195.439483642578,195.439483642578,195.439483642578,195.439483642578,195.439483642578,195.439483642578,182.698661804199,182.698661804199,182.698661804199,182.698661804199,182.698661804199,182.698661804199,182.698661804199,182.698661804199,182.698661804199,182.698661804199,182.698661804199,182.698661804199,182.698661804199,182.698661804199,182.698661804199,182.698661804199,179.384071350098,179.384071350098,179.384071350098,179.384071350098,179.384071350098,179.384071350098,179.384071350098,179.384071350098,179.384071350098,179.384071350098,179.533065795898,179.533065795898,179.533065795898,179.533065795898,179.533065795898,179.533065795898,179.533065795898,179.533065795898,179.75806427002,179.75806427002,179.75806427002,179.75806427002,179.75806427002,179.75806427002,179.75806427002,179.913421630859,179.913421630859,179.913421630859,179.913421630859,179.913421630859,179.913421630859,179.913421630859,179.913421630859,179.913421630859,179.913421630859,179.913421630859,179.913421630859,179.913421630859,179.913421630859,179.913421630859,182.806625366211,182.806625366211,182.806625366211,182.806625366211,182.806625366211,182.806625366211,182.806625366211,182.806625366211,182.806625366211,182.806625366211,182.806625366211,186.170425415039,186.170425415039,186.170425415039,186.170425415039,186.170425415039,186.170425415039,186.170425415039,186.170425415039,186.170425415039,186.170425415039,186.170425415039,186.170425415039,190.341743469238,190.341743469238,190.341743469238,190.341743469238,190.341743469238,190.341743469238,190.341743469238,190.341743469238,190.341743469238,190.341743469238,190.341743469238,190.341743469238,190.341743469238,190.341743469238,190.341743469238,190.341743469238,193.597267150879,193.597267150879,193.597267150879,193.597267150879,193.597267150879,193.597267150879,193.597267150879,193.597267150879,193.597267150879,193.597267150879,196.439849853516,196.439849853516,196.439849853516,196.439849853516,196.439849853516,196.439849853516,196.439849853516,196.439849853516,196.439849853516,196.439849853516,182.467254638672,182.467254638672,182.467254638672,182.467254638672,182.467254638672,182.467254638672,182.467254638672,182.467254638672,182.467254638672,182.467254638672,182.467254638672,182.467254638672,182.467254638672,185.531394958496,185.531394958496,185.531394958496,185.531394958496,185.531394958496,185.531394958496,185.531394958496,185.531394958496,185.531394958496,188.888946533203,188.888946533203,188.888946533203,188.888946533203,188.888946533203,188.888946533203,188.888946533203,188.888946533203,188.888946533203,188.888946533203,188.888946533203,188.888946533203,188.888946533203,188.888946533203,188.888946533203,192.675811767578,192.675811767578,192.675811767578,192.675811767578,192.675811767578,192.675811767578,192.675811767578,192.675811767578,192.675811767578,192.675811767578,192.675811767578,192.675811767578,196.36450958252,196.36450958252,196.36450958252,196.36450958252,196.36450958252,196.36450958252,196.36450958252,196.36450958252,196.36450958252,196.36450958252,196.36450958252,196.36450958252,198.833473205566,198.833473205566,198.833473205566,198.833473205566,198.833473205566,198.833473205566,198.833473205566,198.833473205566,198.833473205566,198.833473205566,198.833473205566,201.421028137207,201.421028137207,201.421028137207,201.421028137207,201.421028137207,201.421028137207,201.421028137207,201.421028137207,201.421028137207,201.421028137207,201.421028137207,203.524757385254,203.524757385254,203.524757385254,203.524757385254,203.524757385254,203.524757385254,203.524757385254,203.524757385254,206.824226379395,206.824226379395,206.824226379395,206.824226379395,206.824226379395,209.719253540039,209.719253540039,209.719253540039,209.719253540039,209.719253540039,209.719253540039,209.719253540039,209.719253540039,209.719253540039,209.719253540039,209.719253540039,209.719253540039,209.719253540039,209.719253540039,209.719253540039,209.719253540039,209.719253540039,213.413063049316,213.413063049316,213.413063049316,213.413063049316,213.413063049316,213.413063049316,213.413063049316,187.660057067871,187.660057067871,187.660057067871,187.660057067871,187.660057067871,187.660057067871,187.660057067871,187.660057067871,187.660057067871,187.660057067871,187.660057067871,187.660057067871,187.660057067871,191.921165466309,191.921165466309,191.921165466309,191.921165466309,191.921165466309,191.921165466309,191.921165466309,191.921165466309,191.921165466309,191.921165466309,191.921165466309,191.921165466309,195.012710571289,195.012710571289,195.012710571289,195.012710571289,197.821792602539,197.821792602539,197.821792602539,197.821792602539,197.821792602539,197.821792602539,197.821792602539,201.495246887207,201.495246887207,201.495246887207,201.495246887207,201.495246887207,201.495246887207,201.495246887207,201.495246887207,201.495246887207,201.495246887207,201.495246887207,201.495246887207,201.495246887207,204.482513427734,204.482513427734,204.482513427734,204.482513427734,204.482513427734,204.482513427734,204.482513427734,204.482513427734,204.482513427734,204.482513427734,204.482513427734,204.482513427734,207.74439239502,207.74439239502,207.74439239502,207.74439239502,207.74439239502,207.74439239502,207.74439239502,210.731430053711,210.731430053711,210.731430053711,210.731430053711,210.731430053711,210.731430053711,210.731430053711,210.731430053711,210.731430053711,210.731430053711,210.731430053711,210.731430053711,210.731430053711,213.525459289551,213.525459289551,213.525459289551,213.525459289551,213.525459289551,213.525459289551,213.525459289551,213.525459289551,213.525459289551,213.525459289551,213.525459289551,213.525459289551,213.525459289551,188.535232543945,188.535232543945,188.535232543945,188.535232543945,188.535232543945,188.535232543945,188.535232543945,188.535232543945,188.535232543945,188.535232543945,188.535232543945,188.535232543945,191.419448852539,191.419448852539,191.419448852539,191.419448852539,191.419448852539,191.419448852539,191.419448852539,191.419448852539,191.419448852539,191.419448852539,191.419448852539,191.419448852539,191.419448852539,191.419448852539,191.419448852539,191.419448852539,194.310493469238,194.310493469238,194.310493469238,194.310493469238,194.310493469238,194.310493469238,194.310493469238,194.310493469238,194.310493469238,194.310493469238,194.310493469238,194.310493469238,198.556686401367,198.556686401367,198.556686401367,198.556686401367,198.556686401367,198.556686401367,198.556686401367,198.556686401367,198.556686401367,198.556686401367,198.556686401367,198.556686401367,201.251846313477,201.251846313477,201.251846313477,201.251846313477,201.251846313477,201.251846313477,201.251846313477,201.251846313477,201.251846313477,201.251846313477,201.251846313477,201.251846313477,204.321113586426,204.321113586426,204.321113586426,204.321113586426,204.321113586426,204.321113586426,204.321113586426,204.321113586426,204.321113586426,204.321113586426,204.321113586426,204.321113586426,204.321113586426,207.950897216797,207.950897216797,207.950897216797,207.950897216797,207.950897216797,207.950897216797,207.950897216797,207.950897216797,207.950897216797,207.950897216797,207.950897216797,211.61368560791,211.61368560791,211.61368560791,211.61368560791,211.61368560791,211.61368560791,211.61368560791,211.61368560791,211.61368560791,211.61368560791,211.61368560791,211.61368560791,215.520446777344,215.520446777344,215.520446777344,215.520446777344,215.520446777344,218.883575439453,218.883575439453,218.883575439453,218.883575439453,218.883575439453,218.883575439453,218.883575439453,218.883575439453,218.883575439453,218.883575439453,218.883575439453,218.883575439453,218.883575439453,191.872459411621,191.872459411621,191.872459411621,191.872459411621,191.872459411621,191.872459411621,191.872459411621,194.218734741211,194.218734741211,194.218734741211,194.218734741211,194.218734741211,194.218734741211,194.218734741211,194.218734741211,194.218734741211,194.218734741211,194.218734741211,194.218734741211,194.218734741211,194.218734741211,194.218734741211,194.218734741211,197.114639282227,197.114639282227,197.114639282227,197.114639282227,197.114639282227,197.114639282227,197.114639282227,197.114639282227,197.114639282227,197.114639282227,200.778198242188,200.778198242188,200.778198242188,200.778198242188,200.778198242188,200.778198242188,200.778198242188,200.778198242188,200.778198242188,200.778198242188,200.778198242188,200.778198242188,203.788436889648,203.788436889648,203.788436889648,203.788436889648,203.788436889648,203.788436889648,203.788436889648,203.788436889648,203.788436889648,206.629241943359,206.629241943359,206.629241943359,206.629241943359,206.629241943359,206.629241943359,206.629241943359,206.629241943359,206.629241943359,206.629241943359,206.629241943359,206.629241943359,206.629241943359,208.948532104492,208.948532104492,208.948532104492,208.948532104492,211.843421936035,211.843421936035,211.843421936035,211.843421936035,211.843421936035,211.843421936035,211.843421936035,211.843421936035,211.843421936035,211.843421936035,211.843421936035,211.843421936035,211.843421936035,215.001815795898,215.001815795898,215.001815795898,215.001815795898,215.001815795898,215.001815795898,215.001815795898,215.001815795898,215.001815795898,215.001815795898,215.001815795898,215.001815795898,215.001815795898,215.001815795898,219.332954406738,219.332954406738,219.332954406738,219.332954406738,219.332954406738,219.332954406738,219.332954406738,222.473526000977,222.473526000977,222.473526000977,222.473526000977,222.473526000977,222.473526000977,222.473526000977,222.473526000977,222.473526000977,222.473526000977,222.473526000977,196.508636474609,196.508636474609,196.508636474609,196.508636474609,196.508636474609,196.508636474609,196.508636474609,196.508636474609,199.646553039551,199.646553039551,199.646553039551,199.646553039551,199.646553039551,199.646553039551,199.646553039551,199.646553039551,199.646553039551,199.646553039551,199.646553039551,199.646553039551,202.188537597656,202.188537597656,202.188537597656,202.188537597656,202.188537597656,202.188537597656,202.188537597656,202.188537597656,202.188537597656,202.188537597656,206.416702270508,206.416702270508,206.416702270508,206.416702270508,206.416702270508,206.416702270508,206.416702270508,211.661560058594,211.661560058594,211.661560058594,211.661560058594,211.661560058594,211.661560058594,211.661560058594,211.661560058594,211.661560058594,211.661560058594,211.661560058594,211.661560058594,211.661560058594,215.295150756836,215.295150756836,215.295150756836,215.295150756836,215.295150756836,215.295150756836,215.295150756836,215.295150756836,215.295150756836,215.295150756836,215.295150756836,215.295150756836,215.295150756836,215.295150756836,218.294418334961,218.294418334961,218.294418334961,218.294418334961,218.294418334961,218.294418334961,218.294418334961,218.294418334961,218.294418334961,218.294418334961,218.294418334961,218.294418334961,218.294418334961,221.250617980957,221.250617980957,221.250617980957,221.250617980957,221.250617980957,221.250617980957,221.250617980957,221.250617980957,221.250617980957,221.250617980957,221.250617980957,221.250617980957,221.250617980957,221.250617980957,224.439659118652,224.439659118652,224.439659118652,224.439659118652,199.422370910645,199.422370910645,199.422370910645,199.422370910645,199.422370910645,199.422370910645,201.787559509277,201.787559509277,201.787559509277,201.787559509277,201.787559509277,201.787559509277,201.787559509277,201.787559509277,201.787559509277,201.787559509277,201.787559509277,201.787559509277,204.573036193848,204.573036193848,204.573036193848,204.573036193848,204.573036193848,204.573036193848,204.573036193848,204.573036193848,204.573036193848,204.573036193848,204.573036193848,204.573036193848,204.573036193848,204.573036193848,204.573036193848,204.573036193848,204.573036193848,205.679130554199,205.679130554199,205.679130554199,205.679130554199,205.679130554199,205.679130554199,205.679130554199,205.679130554199,205.679130554199,205.679130554199,205.679130554199,205.679130554199,205.679130554199,205.679130554199,205.853851318359,205.853851318359,205.853851318359,205.853851318359,205.853851318359,205.853851318359,205.853851318359,205.853851318359,205.853851318359,205.853851318359,205.853851318359,205.853851318359,205.853851318359,205.853851318359,205.853851318359,205.853851318359,206.021553039551,206.021553039551,206.021553039551,206.021553039551,206.021553039551,206.021553039551,206.021553039551,206.021553039551,206.186912536621,206.186912536621,206.186912536621,206.186912536621,206.186912536621,206.186912536621,206.186912536621,206.186912536621,206.186912536621,206.186912536621,206.186912536621,206.186912536621,206.186912536621,206.186912536621,206.186912536621,206.186912536621,206.186912536621,206.186912536621,206.186912536621,206.420471191406,206.420471191406,206.420471191406,206.420471191406,206.420471191406,206.420471191406,206.420471191406,206.420471191406,198.837600708008,198.837600708008,198.837600708008,198.837600708008,198.837600708008,198.837600708008,198.837600708008,198.837600708008,198.837600708008,198.837600708008,198.837600708008,198.837600708008,198.837600708008,198.837600708008,198.837600708008,198.837600708008,198.837600708008,199.004570007324,199.004570007324,199.004570007324,199.004570007324,199.004570007324,199.004570007324,199.004570007324,199.193519592285,199.193519592285,199.193519592285,199.193519592285,199.193519592285,199.193519592285,199.193519592285,199.193519592285,199.193519592285,199.193519592285,199.193519592285,199.357620239258,199.357620239258,199.357620239258,199.357620239258,199.357620239258,199.357620239258,199.357620239258,199.357620239258,199.357620239258,199.596778869629,199.596778869629,199.596778869629,199.596778869629,199.596778869629,199.596778869629,199.596778869629,199.596778869629,199.596778869629,199.596778869629,199.596778869629,199.596778869629,199.596778869629,199.828514099121,199.828514099121,199.828514099121,199.828514099121,199.828514099121,199.828514099121,199.828514099121,199.828514099121,199.989212036133,199.989212036133,199.989212036133,199.989212036133,199.989212036133,199.989212036133,199.989212036133,199.989212036133,199.989212036133,199.989212036133,199.989212036133,199.989212036133,198.867980957031,198.867980957031,198.867980957031,198.867980957031,198.867980957031,198.867980957031,198.867980957031,198.867980957031,198.867980957031,198.867980957031,198.867980957031,198.867980957031,198.867980957031,198.867980957031,199.019752502441,199.019752502441,199.019752502441,199.019752502441,199.019752502441,199.019752502441,199.019752502441,199.019752502441,199.019752502441,199.019752502441,199.019752502441,199.019752502441,199.181488037109,199.181488037109,199.181488037109,199.181488037109,199.181488037109,199.181488037109,199.181488037109,199.181488037109,199.181488037109,199.181488037109,199.181488037109,199.181488037109,199.353317260742,199.353317260742,199.353317260742,199.353317260742,199.353317260742,199.353317260742,199.487899780273,199.487899780273,199.487899780273,199.487899780273,199.654441833496,199.654441833496,199.654441833496,199.654441833496,199.654441833496,199.654441833496,199.654441833496,199.654441833496,199.814056396484,199.814056396484,199.814056396484,199.814056396484,199.814056396484,199.814056396484,199.814056396484,199.814056396484,199.814056396484,199.814056396484,199.814056396484,199.814056396484,199.814056396484,199.814056396484,199.814056396484,199.814056396484,199.814056396484,199.814056396484,199.998390197754,199.998390197754,199.998390197754,199.998390197754,199.998390197754,199.998390197754,198.886268615723,198.886268615723,198.886268615723,198.886268615723,198.886268615723,198.886268615723,198.886268615723,198.886268615723,198.886268615723,199.026626586914,199.026626586914,199.026626586914,199.026626586914,199.026626586914,199.026626586914,199.026626586914,199.026626586914,199.026626586914,199.026626586914,199.026626586914,199.026626586914,199.184906005859,199.184906005859,199.184906005859,199.184906005859,199.184906005859,199.184906005859,199.184906005859,199.184906005859,199.184906005859,199.184906005859,199.184906005859,199.373031616211,199.373031616211,199.373031616211,199.373031616211,199.373031616211,199.373031616211,199.531745910645,199.531745910645,199.531745910645,199.531745910645,199.531745910645,199.531745910645,199.531745910645,199.531745910645,199.701156616211,199.701156616211,199.701156616211,199.701156616211,199.701156616211,199.701156616211,199.701156616211,199.701156616211,199.83617401123,199.83617401123,199.83617401123,199.83617401123,199.83617401123,199.83617401123,199.83617401123,199.83617401123,199.83617401123,199.83617401123,199.975158691406,199.975158691406,199.975158691406,199.975158691406,199.975158691406,199.975158691406,199.975158691406,199.975158691406,199.975158691406,199.975158691406,199.975158691406,199.975158691406,199.975158691406,199.975158691406,198.859710693359,198.859710693359,198.859710693359,198.859710693359,198.859710693359,198.859710693359,198.859710693359,198.859710693359,198.859710693359,198.859710693359,200.593437194824,200.593437194824,200.593437194824,200.593437194824,200.593437194824,200.593437194824,200.593437194824,200.593437194824,200.593437194824,200.593437194824,200.593437194824,200.593437194824,200.593437194824,200.593437194824,200.593437194824,200.593437194824,200.593437194824,203.802825927734,203.802825927734,203.802825927734,203.802825927734,203.802825927734,203.802825927734,207.134864807129,207.134864807129,207.134864807129,207.134864807129,207.134864807129,210.786209106445,210.786209106445,210.786209106445,210.786209106445,210.786209106445,210.786209106445,210.786209106445,210.786209106445,210.786209106445,210.786209106445,210.786209106445,214.165596008301,214.165596008301,214.165596008301,214.165596008301,214.165596008301,214.165596008301,214.165596008301,214.165596008301,214.165596008301,217.419281005859,217.419281005859,217.419281005859,217.419281005859,217.419281005859,217.419281005859,217.419281005859,217.419281005859,217.419281005859,217.419281005859,217.419281005859,217.419281005859,217.419281005859,217.419281005859,217.419281005859,217.419281005859,220.30485534668,220.30485534668,220.30485534668,220.30485534668,220.30485534668,220.30485534668,220.30485534668,220.30485534668,220.30485534668,220.30485534668,220.30485534668,220.30485534668,224.082649230957,224.082649230957,224.082649230957,224.082649230957,224.082649230957,224.082649230957,224.082649230957,224.082649230957,224.082649230957,224.082649230957,224.082649230957,224.082649230957,224.082649230957,224.082649230957,224.082649230957,224.082649230957,227.274513244629,227.274513244629,227.274513244629,227.274513244629,227.274513244629,230.082038879395,230.082038879395,230.082038879395,230.082038879395,230.082038879395,230.082038879395,230.082038879395,230.082038879395,230.082038879395,230.082038879395,230.082038879395,230.082038879395,230.082038879395,230.082038879395,230.082038879395,230.082038879395,230.082038879395,230.082038879395,230.082038879395,230.082038879395,204.054290771484,204.054290771484,204.054290771484,204.054290771484,204.054290771484,204.054290771484,204.054290771484,204.054290771484,207.485466003418,207.485466003418,207.485466003418,207.485466003418,207.485466003418,207.485466003418,207.485466003418,207.485466003418,207.485466003418,207.485466003418,207.485466003418,207.485466003418,207.485466003418,207.485466003418,207.485466003418,211.032867431641,211.032867431641,211.032867431641,211.032867431641,211.032867431641,211.032867431641,211.032867431641,211.032867431641,211.032867431641,214.410453796387,214.410453796387,214.410453796387,214.410453796387,214.410453796387,214.410453796387,214.410453796387,214.410453796387,214.410453796387,214.410453796387,217.745727539062,217.745727539062,217.745727539062,217.745727539062,217.745727539062,217.745727539062,220.953353881836,220.953353881836,220.953353881836,220.953353881836,220.953353881836,220.953353881836,220.953353881836,220.953353881836,220.953353881836,220.953353881836,220.953353881836,220.953353881836,223.792488098145,223.792488098145,223.792488098145,223.792488098145,223.792488098145,223.792488098145,223.792488098145,223.792488098145,223.792488098145,223.792488098145,223.792488098145,223.792488098145,223.792488098145,226.829917907715,226.829917907715,226.829917907715,226.829917907715,226.829917907715,226.829917907715,226.829917907715,226.829917907715,226.829917907715,226.829917907715,226.829917907715,226.829917907715,229.742408752441,229.742408752441,229.742408752441,229.742408752441,229.742408752441,229.742408752441,229.742408752441,231.971115112305,231.971115112305,231.971115112305,231.971115112305,231.971115112305,231.971115112305,231.971115112305,231.971115112305,231.971115112305,231.971115112305,231.971115112305,231.971115112305,231.971115112305,235.500679016113,235.500679016113,235.500679016113,235.500679016113,235.500679016113,235.500679016113,235.500679016113,235.500679016113,235.500679016113,235.500679016113,235.500679016113,206.041831970215,206.041831970215,206.041831970215,206.041831970215,206.041831970215,206.041831970215,206.041831970215,206.041831970215,209.204292297363,209.204292297363,209.204292297363,209.204292297363,209.204292297363,209.204292297363,209.204292297363,209.204292297363,209.204292297363,209.204292297363,209.204292297363,212.101631164551,212.101631164551,212.101631164551,212.101631164551,215.660804748535,215.660804748535,215.660804748535,215.660804748535,215.660804748535,215.660804748535,215.660804748535,215.660804748535,215.660804748535,215.660804748535,215.660804748535,215.660804748535,219.324371337891,219.324371337891,219.324371337891,219.324371337891,219.324371337891,219.324371337891,219.324371337891,219.324371337891,219.324371337891,219.324371337891,219.324371337891,219.324371337891,223.294425964355,223.294425964355,223.294425964355,223.294425964355,223.294425964355,223.294425964355,223.294425964355,223.294425964355,223.294425964355,223.294425964355,223.294425964355,225.73095703125,225.73095703125,225.73095703125,225.73095703125,225.73095703125,225.73095703125,225.73095703125,225.73095703125,225.73095703125,225.73095703125,225.73095703125,228.506614685059,228.506614685059,228.506614685059,228.506614685059,228.506614685059,228.506614685059,228.506614685059,228.506614685059,228.506614685059,228.506614685059,228.506614685059,228.506614685059,228.506614685059,232.104583740234,232.104583740234,232.104583740234,232.104583740234,232.104583740234,232.104583740234,232.104583740234,232.104583740234,232.104583740234,232.104583740234,232.104583740234,232.104583740234,232.104583740234,235.424293518066,235.424293518066,235.424293518066,235.424293518066,235.424293518066,235.424293518066,235.424293518066,235.424293518066,235.424293518066,235.424293518066,235.424293518066,235.424293518066,238.270431518555,238.270431518555,238.270431518555,238.270431518555,238.270431518555,238.270431518555,238.270431518555,238.270431518555,238.270431518555,238.270431518555,238.270431518555,238.270431518555,238.270431518555,238.270431518555,209.468566894531,209.468566894531,209.468566894531,209.468566894531,209.468566894531,209.468566894531,209.468566894531,209.468566894531,209.468566894531,209.468566894531,209.468566894531,209.468566894531,212.435150146484,212.435150146484,212.435150146484,212.435150146484,212.435150146484,212.435150146484,212.435150146484,212.435150146484,212.435150146484,212.435150146484,212.435150146484,212.435150146484,212.435150146484,212.435150146484,215.721534729004,215.721534729004,215.721534729004,215.721534729004,215.721534729004,215.721534729004,215.721534729004,215.721534729004,215.721534729004,215.721534729004,215.721534729004,215.721534729004,215.721534729004,218.831787109375,218.831787109375,218.831787109375,218.831787109375,218.831787109375,218.831787109375,218.831787109375,218.831787109375,218.831787109375,218.831787109375,218.831787109375,218.831787109375,221.373245239258,221.373245239258,221.373245239258,221.373245239258,221.373245239258,221.373245239258,221.373245239258,221.373245239258,221.373245239258,221.373245239258,221.373245239258,221.373245239258,224.909370422363,224.909370422363,224.909370422363,224.909370422363,224.909370422363,224.909370422363,228.728019714355,228.728019714355,228.728019714355,228.728019714355,228.728019714355,228.728019714355,228.728019714355,228.728019714355,228.728019714355,228.728019714355,228.728019714355,228.728019714355,228.728019714355,228.728019714355,228.728019714355,228.728019714355,232.160743713379,232.160743713379,232.160743713379,232.160743713379,232.160743713379,232.160743713379,232.160743713379,232.160743713379,232.160743713379,232.160743713379,232.160743713379,232.160743713379,236.253753662109,236.253753662109,236.253753662109,236.253753662109,236.253753662109,236.253753662109,236.253753662109,236.253753662109,236.253753662109,236.253753662109,236.253753662109,236.253753662109,236.253753662109,239.737205505371,239.737205505371,239.737205505371,239.737205505371,239.737205505371,239.737205505371,239.737205505371,239.737205505371,239.737205505371,239.737205505371,239.737205505371,212.456596374512,212.456596374512,212.456596374512,212.456596374512,212.456596374512,212.456596374512,212.456596374512,212.456596374512,212.456596374512,212.456596374512,212.456596374512,212.456596374512,212.456596374512,216.075859069824,216.075859069824,216.075859069824,216.075859069824,216.075859069824,216.075859069824,216.075859069824,216.075859069824,216.075859069824,216.075859069824,216.075859069824,216.075859069824,221.099998474121,221.099998474121,221.099998474121,221.099998474121,221.099998474121,221.099998474121,221.099998474121,224.026435852051,224.026435852051,224.026435852051,224.026435852051,224.026435852051,224.026435852051,224.026435852051,224.026435852051,224.026435852051,224.026435852051,224.026435852051,227.518402099609,227.518402099609,227.518402099609,227.518402099609,227.518402099609,227.518402099609,227.518402099609,227.518402099609,227.518402099609,227.518402099609,227.518402099609,227.518402099609,227.518402099609,227.518402099609,227.518402099609,227.518402099609,230.505805969238,230.505805969238,230.505805969238,230.505805969238,230.505805969238,230.505805969238,230.505805969238,230.505805969238,230.505805969238,230.505805969238,230.505805969238,230.505805969238,233.43994140625,233.43994140625,233.43994140625,233.43994140625,233.43994140625,233.43994140625,233.43994140625,233.43994140625,233.43994140625,233.43994140625,233.43994140625,235.904052734375,235.904052734375,235.904052734375,235.904052734375,235.904052734375,235.904052734375,235.904052734375,235.904052734375,235.904052734375,235.904052734375,235.904052734375,239.111175537109,239.111175537109,239.111175537109,239.111175537109,239.111175537109,242.991851806641,242.991851806641,242.991851806641,242.991851806641,242.991851806641,242.991851806641,242.991851806641,242.991851806641,242.991851806641,242.991851806641,242.991851806641,242.991851806641,242.991851806641,242.991851806641,245.675094604492,245.675094604492,245.675094604492,245.675094604492,245.675094604492,245.675094604492,245.675094604492,245.675094604492,245.675094604492,218.56258392334,218.56258392334,218.56258392334,218.56258392334,218.56258392334,218.56258392334,222.144073486328,222.144073486328,222.144073486328,222.144073486328,222.144073486328,222.144073486328,222.144073486328,222.144073486328,222.144073486328,222.144073486328,222.144073486328,226.506004333496,226.506004333496,226.506004333496,226.506004333496,226.506004333496,226.506004333496,226.506004333496,226.506004333496,226.506004333496,226.506004333496,226.506004333496,229.472297668457,229.472297668457,229.472297668457,229.472297668457,229.472297668457,229.472297668457,229.472297668457,229.472297668457,229.472297668457,229.472297668457,229.472297668457,229.472297668457,229.472297668457,229.472297668457,229.472297668457,229.472297668457,229.472297668457,229.472297668457,232.783813476562,232.783813476562,232.783813476562,232.783813476562,232.783813476562,232.783813476562,232.783813476562,232.783813476562,232.783813476562,232.783813476562,232.783813476562,235.81307220459,235.81307220459,235.81307220459,235.81307220459,235.81307220459,235.81307220459,235.81307220459,235.81307220459,235.81307220459,235.81307220459,235.81307220459,235.81307220459,240.300506591797,240.300506591797,240.300506591797,240.300506591797,240.300506591797,240.300506591797,240.300506591797,240.300506591797,243.650405883789,243.650405883789,243.650405883789,243.650405883789,243.650405883789,243.650405883789,243.650405883789,243.650405883789,243.650405883789,243.650405883789,243.650405883789,243.650405883789,243.650405883789,247.504524230957,247.504524230957,247.504524230957,247.504524230957,247.504524230957,247.504524230957,247.504524230957,247.504524230957,247.504524230957,247.504524230957,247.504524230957,247.504524230957,247.504524230957,247.504524230957,247.504524230957,247.504524230957,247.504524230957,220.246719360352,220.246719360352,220.246719360352,220.246719360352,220.246719360352,220.246719360352,220.246719360352,220.246719360352,220.246719360352,220.246719360352,220.246719360352,220.246719360352,223.566635131836,223.566635131836,223.566635131836,223.566635131836,223.566635131836,223.566635131836,223.566635131836,223.566635131836,223.566635131836,223.566635131836,223.566635131836,223.566635131836,223.566635131836,228.022895812988,228.022895812988,228.022895812988,228.022895812988,228.022895812988,228.022895812988,228.022895812988,228.022895812988,228.022895812988,228.022895812988,228.022895812988,228.022895812988,231.190841674805,231.190841674805,231.190841674805,231.190841674805,231.190841674805,231.190841674805,231.190841674805,231.190841674805,231.190841674805,231.190841674805,231.190841674805,231.190841674805,231.190841674805,231.190841674805,231.190841674805,231.190841674805,231.190841674805,233.405792236328,233.405792236328,233.405792236328,233.405792236328,233.405792236328,233.405792236328,233.405792236328,233.405792236328,233.405792236328,233.405792236328,233.405792236328,233.405792236328,233.405792236328,233.405792236328,233.586387634277,233.586387634277,233.586387634277,233.586387634277,233.586387634277,233.586387634277,233.586387634277,233.586387634277,233.586387634277,233.586387634277,233.586387634277,233.586387634277,233.586387634277,233.728576660156,233.728576660156,233.728576660156,233.728576660156,233.728576660156,233.728576660156,233.728576660156,233.728576660156,233.728576660156,233.728576660156,233.728576660156,233.728576660156,233.728576660156,233.728576660156,233.728576660156,233.728576660156,233.728576660156,233.903137207031,233.903137207031,233.903137207031,233.903137207031,233.903137207031,233.903137207031,233.903137207031,233.903137207031,233.903137207031,233.903137207031,233.903137207031,233.903137207031,233.903137207031,233.903137207031,233.903137207031,233.903137207031,233.903137207031,233.903137207031,233.903137207031,233.903137207031,220.644996643066,220.644996643066,220.644996643066,220.644996643066,220.644996643066,220.644996643066,220.644996643066,220.644996643066,220.807022094727,220.807022094727,220.807022094727,220.807022094727,220.807022094727,220.807022094727,220.807022094727,220.807022094727,220.807022094727,220.807022094727,220.807022094727,220.807022094727,220.807022094727,220.807022094727,220.807022094727,220.807022094727,220.950538635254,220.950538635254,220.950538635254,220.950538635254,220.950538635254,220.950538635254,220.950538635254,220.950538635254,220.950538635254,220.950538635254,221.112243652344,221.112243652344,221.112243652344,221.112243652344,221.112243652344,221.112243652344,221.112243652344,221.112243652344,221.112243652344,221.278175354004,221.278175354004,221.278175354004,221.278175354004,221.278175354004,221.278175354004,221.278175354004,221.278175354004,221.278175354004,221.278175354004,221.278175354004,221.278175354004,221.278175354004,221.278175354004,221.278175354004,221.278175354004,221.44100189209,221.44100189209,221.44100189209,221.44100189209,221.44100189209,221.44100189209,221.44100189209,221.44100189209,221.44100189209,221.44100189209,221.44100189209,221.44100189209,221.619956970215,221.619956970215,221.619956970215,221.619956970215,221.619956970215,221.619956970215,221.619956970215,221.619956970215,221.619956970215,221.619956970215,221.619956970215,221.619956970215,221.619956970215,221.619956970215,221.619956970215,221.619956970215,220.606803894043,220.606803894043,220.606803894043,220.606803894043,220.606803894043,220.606803894043,220.606803894043,220.606803894043,220.802612304688,220.802612304688,220.802612304688,220.802612304688,220.802612304688,220.802612304688,220.802612304688,220.802612304688,220.967132568359,220.967132568359,220.967132568359,220.967132568359,220.967132568359,220.967132568359,220.967132568359,220.967132568359,220.967132568359,220.967132568359,220.967132568359,220.967132568359,220.967132568359,220.967132568359,220.967132568359,220.967132568359,221.100784301758,221.100784301758,221.100784301758,221.100784301758,221.100784301758,221.100784301758,221.100784301758,221.100784301758,221.100784301758,221.100784301758,221.100784301758,221.100784301758,221.100784301758,221.100784301758,221.100784301758,221.100784301758,221.224632263184,221.224632263184,221.224632263184,221.224632263184,221.224632263184,221.224632263184,221.224632263184,221.375503540039,221.375503540039,221.375503540039,221.375503540039,221.375503540039,221.375503540039,221.375503540039,221.558296203613,221.558296203613,221.558296203613,221.558296203613,221.558296203613,221.558296203613,221.735816955566,221.735816955566,221.735816955566,221.735816955566,221.735816955566,221.735816955566,221.735816955566,221.735816955566,221.735816955566,221.735816955566,221.735816955566,221.735816955566,220.740188598633,220.740188598633,220.740188598633,220.740188598633,220.740188598633,220.740188598633,220.740188598633,220.918174743652,220.918174743652,220.918174743652,220.918174743652,220.918174743652,220.918174743652,220.918174743652,220.918174743652,220.918174743652,220.918174743652,220.918174743652,220.918174743652,220.918174743652,220.918174743652,220.918174743652,221.186065673828,221.186065673828,221.186065673828,221.186065673828,221.186065673828,221.186065673828,221.347282409668,221.347282409668,221.347282409668,221.347282409668,221.347282409668,221.347282409668,221.347282409668,221.550010681152,221.550010681152,221.550010681152,221.550010681152,221.550010681152,221.550010681152,221.550010681152,221.550010681152,221.550010681152,221.550010681152,221.550010681152,221.550010681152,221.550010681152,221.550010681152,221.550010681152,221.745796203613,221.745796203613,221.745796203613,221.745796203613,221.745796203613,221.745796203613,221.745796203613,221.745796203613,221.745796203613,221.745796203613,221.745796203613,221.745796203613,220.739219665527,220.739219665527,220.739219665527,220.739219665527,220.739219665527,220.739219665527,220.739219665527,220.739219665527,220.739219665527,220.91374206543,220.91374206543,220.91374206543,220.91374206543,220.91374206543,220.91374206543,220.91374206543,220.91374206543,221.121116638184,221.121116638184,221.121116638184,221.121116638184,221.121116638184,221.121116638184,221.121116638184,221.121116638184,221.121116638184,221.348892211914,221.348892211914,221.348892211914,221.348892211914,221.348892211914,221.348892211914,221.348892211914,221.348892211914,221.348892211914,221.348892211914,221.348892211914,221.348892211914,221.348892211914,221.348892211914,221.348892211914],"meminc":[0,0,0,29.6168212890625,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,29.6168212890625,0,0,0,0,0,3.00444030761719,0,0,3.08897399902344,0,0,3.15238189697266,0,0,3.15708923339844,0,0,2.87813568115234,0,0,2.39787292480469,0,0,2.82021331787109,0,0,2.64302825927734,0,0,2.17056274414062,0,0,0,1.50801086425781,0,0,0,0,0,0,1.55194854736328,0,0,0,0,0,0,1.36418914794922,0,0,0,0,0,0,0,0,0,0,0,0,1.73378753662109,0,0,0,0,1.56337738037109,0,0,0,0,0,0,0,0,1.250732421875,0,0,0,0,0,0,0,0,1.52994537353516,0,0,0,0,0,0,0,0,0,1.33541107177734,0,0,0,0,0,0,0,0,0,0,-65.6302947998047,0,0,0,0,0,0,0,0,0,0,0,0,-10.0447692871094,0,0,0,0,0,0,0,0,0,0,0,0,0,2.83311462402344,0,0,0,0,0,0,0,0,0,0,1.54300689697266,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1.77137756347656,0,0,0,0,0,0,0,0,0,0,0,0,1.43560028076172,0,0,0,0,0,0,0,0,0,0,1.60337829589844,0,0,0,0,0,0,0,0,0,0,0,0,2.01542663574219,0,0,0,0,0,0,0,0,0,0,0,-9.21906280517578,0,0,0,0,0,0,2.82032012939453,0,0,0,0,0,0,0,1.72003173828125,0,0,0,0,0,0,0,0,0,0,0,0,1.71128845214844,0,0,0,0,0,0,0,0,0,0,0,0,1.71636962890625,0,0,0,0,0,0,0,0,2.04523468017578,0,0,0,0,0,0,0,-9.11917877197266,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2.05742645263672,0,0,0,0,0,0,0,0,0,0,0,0,1.84236145019531,0,0,0,0,0,0,0,2.52165985107422,0,0,0,0,0,0,0,0,0,1.84245300292969,0,0,0,0,0,2.45955657958984,0,0,0,0,0,0,0,0,0,0,0,0,-9.01116943359375,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2.02285003662109,0,0,0,0,0,0,0,0,3.30503082275391,0,0,0,0,1.62730407714844,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1.91767883300781,0,0,0,0,0,0,0,0,0,1.68326568603516,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,-9.06253051757812,0,0,0,0,0,0,0,0,0,0,0,0,0,1.78925323486328,0,0,0,0,0,0,0,0,0,0,0,1.89535522460938,0,0,0,0,0,0,0,0,0,0,0,2.58958435058594,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1.82675933837891,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1.76885986328125,0,0,0,0,0,0,0,0,0,0,0,-8.80113220214844,0,0,0,0,0,0,0,2.02094268798828,0,0,0,0,0,0,0,0,0,0,0,2.01026153564453,0,0,0,0,0,0,0,0,0,0,0,1.85869598388672,0,0,0,0,0,0,1.73867797851562,0,0,0,0,0,0,0,0,0,0,0,0,1.53479766845703,0,0,0,0,0,0,0,0,0,0,0,-2.83731079101562,0,0,0,0,0,0,0,-4.35964202880859,0,0,0,0,0,0,0,0,0,0,0,0,3.380615234375,0,0,0,0,0,0,0,0,0,0,2.83374786376953,0,0,0,0,0,1.84510040283203,0,0,0,0,0,0,0,0,0,0,0,1.65384674072266,0,0,0,0,0,0,0,0,0,0,0,-9.08798980712891,0,0,0,0,0,0,0,0,1.33113098144531,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1.73230743408203,0,0,0,0,0,0,1.84662628173828,0,0,0,0,0,0,0,0.213325500488281,0,0,0,0,0,0,0,0.145515441894531,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,-5.63233947753906,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0133209228515625,0,0,0,0,0,0.162994384765625,0,0,0,0,0,0,0.157699584960938,0,0,0,0,0,0,0.153709411621094,0,0,0,0,0,0,0,0,0,0.182334899902344,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.162147521972656,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.2215576171875,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.135414123535156,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.128944396972656,0,0,0,0,0,-1.16700744628906,0,0,0,0,0,0.145782470703125,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.252372741699219,0,0,0,0,0,0,0,0,0,0,0,0,0.197647094726562,0,0,0,0,0,0,0,0.129158020019531,0,0,0,0,0,0,0,0,0,0,0.217231750488281,0,0,0,0,0,0,0,0.168632507324219,0,0,0,0,0,0,0,0,0,0,-1.19052886962891,0,0,0,0,0,0,0,0,0,0,0,0,0.227294921875,0,0,0,0,0,0,0,0,0.138999938964844,0,0,0,0,0,0,0,0,0,0,0,0,0.173553466796875,0,0,0,0,0,0,0,0,0,0.142372131347656,0,0,0,0,0,0,0,0,0.145370483398438,0,0,0,0,0,0,0,0,0,0.138450622558594,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.169990539550781,0,0,0,0,0,0,0,0,0,0,0,0,-0.543426513671875,0,0,0,0,0,0,0,0,0,0,2.82506561279297,0,0,0,0,0,0,0,0,0,2.68848419189453,0,0,0,0,0,0,0,0,0,0,0,0,1.55528259277344,0,0,0,0,0,0,0,0,0,0,0,0,2.02149200439453,0,0,0,0,0,0,0,0,0,0,0,0,1.93409729003906,0,0,0,0,0,0,0,0,0,0,0,0,1.97630310058594,0,0,0,0,0,0,0,0,0,0,0,0,1.68912506103516,0,0,0,0,0,0,0,0,0,0,0,0,1.92701721191406,0,0,0,0,0,0,0,0,2.02168273925781,0,0,0,0,0,0,0,0,0,0,0,0,2.14803314208984,0,0,0,0,0,0,0,0,0,0,2.04003143310547,0,0,0,0,1.56764984130859,0,0,0,0,0,0,0,0,0,0,0,0,-19.0265121459961,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2.92008972167969,0,0,0,0,0,2.18515014648438,0,0,0,0,0,0,0,0,0,0,0,0,2.09996032714844,0,0,0,0,0,0,0,0,0,0,2.63193511962891,0,0,0,0,0,1.91734313964844,0,0,0,0,0,0,0,0,0,0,0,0,2.36463928222656,0,0,0,0,0,2.62454986572266,0,0,0,0,0,0,0,0,2.00135040283203,0,0,0,0,0,0,2.11651611328125,0,0,0,0,0,0,0,0,0,0,-19.9818115234375,0,0,0,0,0,0,0,0,0,0,0,1.96416473388672,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2.63026428222656,0,0,0,0,0,0,0,2.99228668212891,0,0,0,0,0,0,0,0,0,0,0,0,0,2.94133758544922,0,0,0,0,0,0,0,0,0,0,0,2.46022033691406,0,0,0,0,0,0,0,0,0,2.26889038085938,0,0,0,0,0,0,0,0,0,0,0,0,1.85695648193359,0,0,0,0,0,0,0,0,0,0,2.04671478271484,0,0,0,0,0,0,1.57499694824219,0,0,0,0,0,0,0,0,0,0,1.70460510253906,0,0,0,0,0,0,0,-19.6716537475586,0,0,0,0,0,0,0,0,0,0,0,2.64918518066406,0,0,0,0,0,0,0,0,0,0,0,0,0,2.69699859619141,0,0,0,0,0,2.43739318847656,0,0,0,0,0,0,0,0,0,0,0,0,2.54534149169922,0,0,0,0,2.86655426025391,0,0,0,0,0,0,0,0,0,0,0,2.08485412597656,0,0,0,0,0,0,0,0,0,0,0,0,2.59996032714844,0,0,0,0,0,0,2.66290283203125,0,0,0,0,0,0,0,0,0,0,-18.2605743408203,0,0,0,0,0,0,0,0,0,0,3.28545379638672,0,0,0,0,0,0,0,0,0,0,0,0,2.64261627197266,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,3.20868682861328,0,0,0,0,0,0,0,0,0,0,2.34979248046875,0,0,0,0,0,0,0,3.95621490478516,0,0,0,0,0,0,0,0,0,0,0,0,1.92921447753906,0,0,0,0,0,0,0,0,0,0,0,0,0,2.63248443603516,0,0,0,0,0,2.35032653808594,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,-18.2044067382812,0,0,0,0,0,0,0,3.94867706298828,0,0,0,0,0,0,1.02410888671875,0,0,0,0,0,0,0,0,0,0,0.117958068847656,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.184364318847656,0,0,0,0,0,0.159507751464844,0,0,0,0,0,0,0,0,0,0,0,0,0,0.164176940917969,0,0,0,0,0,0,0.180465698242188,0,0,0,0,0,0,-7.15019226074219,0,0,0,0,0,0,0,0,0.171798706054688,0,0,0,0,0,0,0,0,0,0,0,0,0.168754577636719,0,0,0,0,0,0,0,0,0,0,0.129295349121094,0,0,0,0,0,0,0,0,0.162727355957031,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.210708618164062,0,0,0,0,0,0,0,0.15045166015625,0,0,0,0,0,0,0,0,0,0.159645080566406,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,-1.11939239501953,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.190788269042969,0,0,0,0,0,0,0,0,0,0,0,0,0,0.160606384277344,0,0,0,0,0,0,0,0,0,0,0,0,0,0.157684326171875,0,0,0,0,0,0,0,0,0,0,0,0,0,0.1646728515625,0,0,0,0,0,0,0.181022644042969,0,0,0,0,0,0,0.185775756835938,0,0,0,0,0,0,0,-1.13179016113281,0,0,0,0,0,0.189056396484375,0,0,0,0,0,0,0,0,0,0,0,0,0,0.17144775390625,0,0,0,0,0,0,0,0,0,0,0,0,0,0.287399291992188,0,0,0,0,0,0,0,0,0,0,0,0,0.188011169433594,0,0,0,0,0,0,0,0,0,0,0,0,2.13925170898438,0,0,0,0,0,2.41096496582031,0,0,0,0,0,0,0,0,0,-2.58793640136719,0,0,0,0,0,0,0,0,0,3.12728881835938,0,0,0,0,0,0,0,0,0,0,0,0,0,2.8902587890625,0,0,0,0,0,0,0,0,0,0,3.30644989013672,0,0,0,0,0,0,0,0,3.29141998291016,0,0,0,0,0,0,0,0,0,0,4.15283203125,0,0,0,0,0,0,0,2.08032989501953,0,0,0,0,2.81593322753906,0,0,0,0,0,2.41060638427734,0,0,0,0,0,0,-23.0703125,0,0,0,0,0,2.90189361572266,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,3.11743927001953,0,0,0,0,0,0,0,0,0,0,0,0,0,0,5.60472106933594,0,0,0,0,0,0,0,0,0,3.67066192626953,0,0,0,0,0,2.05801391601562,0,0,0,0,0,0,0,0,0,0,0,0,2.39190673828125,0,0,0,0,0,0,0,0,0,0,0,0,2.42257690429688,0,0,0,0,0,0,0,0,0,0,0,0,2.26893615722656,0,0,0,0,0,0,0,0,0,0,0,0,0,2.36598968505859,0,0,0,0,0,0,0,0,0,0,0,-22.8681793212891,0,0,0,0,0,0,0,0,0,0,0,0,2.50970458984375,0,0,0,0,0,0,0,0,0,0,0,0,2.83293151855469,0,0,0,0,0,0,0,0,0,0,3.38536834716797,0,0,0,0,0,0,0,0,0,0,0,3.95194244384766,0,0,0,0,0,0,3.68003845214844,0,0,0,0,0,0,0,0,0,0,0,0,2.40235900878906,0,0,0,0,0,0,0,0,0,0,2.80499267578125,0,0,0,0,0,0,0,0,0,0,0,0,4.0517578125,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,-21.4536590576172,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2.98985290527344,0,0,0,0,0,0,0,0,0,0,3.23527526855469,0,0,0,0,0,0,0,0,0,0,3.0450439453125,0,0,0,0,0,0,0,0,0,0,0,2.97325134277344,0,0,0,0,0,0,0,0,0,0,0,0,2.96134948730469,0,0,0,0,2.04755401611328,0,0,0,0,0,0,0,3.53730010986328,0,0,0,0,2.80838012695312,0,0,0,0,0,0,-18.9798736572266,0,0,0,0,0,0,0,0,3.34199523925781,0,0,0,0,0,0,0,0,0,0,0,3.12551879882812,0,0,0,0,0,0,0,0,4.01198577880859,0,0,0,0,0,0,0,0,4.31294250488281,0,0,0,0,0,0,0,0,0,0,0,3.18099975585938,0,0,0,0,0,0,0,0,0,0,0,0,0,2.59651184082031,0,0,0,0,0,0,0,0,0,0,0,-21.7057495117188,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,3.73628997802734,0,0,0,0,0,2.81029510498047,0,0,0,0,0,0,0,0,0,0,2.45246124267578,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,3.11757659912109,0,0,0,0,0,0,0,0,2.67889404296875,0,0,0,0,0,0,0,0,0,0,1.80682373046875,0,0,0,0,0,0,0,0,0,0.170524597167969,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.188743591308594,0,0,0,0,0,0,0,0,0,0,-15.3343505859375,0,0,0,0,0,0,0,0.272537231445312,0,0,0,0,0,0,0,0,0,0,0.140640258789062,0,0,0,0,0,0,0.180717468261719,0,0,0,0,0,0,0,0,0.18597412109375,0,0,0,0,0,0,0,0.181694030761719,0,0,0,0,0,0,0,0,0,0,0,0,0.1201171875,0,0,0,0,0,0,0,0,0,0,0,-0.960762023925781,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.129058837890625,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.202957153320312,0,0,0,0,0,0,0,0,0,0.174781799316406,0,0,0,0,0,0,0,0,0,0.108665466308594,0,0,0,0,0,0,0,0,0.1961669921875,0,0,0,0,0,0,0,0,0,0,0,0.179862976074219,0,0,0,0,0,0,0,0,0,0,-1.04146575927734,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.182632446289062,0,0,0,0,0,0.152565002441406,0,0,0,0,0,0,0,0,0,0,0,0,0.286788940429688,0,0,0,0,0,0,0,0,0,0,0.161529541015625,0,0,0,0,0,0,0,0,0,0,0,0.141578674316406,0,0,0,0,0,0,0,0,0,0.14788818359375,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,-12.7408218383789,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,-3.31459045410156,0,0,0,0,0,0,0,0,0,0.148994445800781,0,0,0,0,0,0,0,0.224998474121094,0,0,0,0,0,0,0.155357360839844,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2.89320373535156,0,0,0,0,0,0,0,0,0,0,3.36380004882812,0,0,0,0,0,0,0,0,0,0,0,4.17131805419922,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,3.25552368164062,0,0,0,0,0,0,0,0,0,2.84258270263672,0,0,0,0,0,0,0,0,0,-13.9725952148438,0,0,0,0,0,0,0,0,0,0,0,0,3.06414031982422,0,0,0,0,0,0,0,0,3.35755157470703,0,0,0,0,0,0,0,0,0,0,0,0,0,0,3.786865234375,0,0,0,0,0,0,0,0,0,0,0,3.68869781494141,0,0,0,0,0,0,0,0,0,0,0,2.46896362304688,0,0,0,0,0,0,0,0,0,0,2.58755493164062,0,0,0,0,0,0,0,0,0,0,2.10372924804688,0,0,0,0,0,0,0,3.29946899414062,0,0,0,0,2.89502716064453,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,3.69380950927734,0,0,0,0,0,0,-25.7530059814453,0,0,0,0,0,0,0,0,0,0,0,0,4.2611083984375,0,0,0,0,0,0,0,0,0,0,0,3.09154510498047,0,0,0,2.80908203125,0,0,0,0,0,0,3.67345428466797,0,0,0,0,0,0,0,0,0,0,0,0,2.98726654052734,0,0,0,0,0,0,0,0,0,0,0,3.26187896728516,0,0,0,0,0,0,2.98703765869141,0,0,0,0,0,0,0,0,0,0,0,0,2.79402923583984,0,0,0,0,0,0,0,0,0,0,0,0,-24.9902267456055,0,0,0,0,0,0,0,0,0,0,0,2.88421630859375,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2.89104461669922,0,0,0,0,0,0,0,0,0,0,0,4.24619293212891,0,0,0,0,0,0,0,0,0,0,0,2.69515991210938,0,0,0,0,0,0,0,0,0,0,0,3.06926727294922,0,0,0,0,0,0,0,0,0,0,0,0,3.62978363037109,0,0,0,0,0,0,0,0,0,0,3.66278839111328,0,0,0,0,0,0,0,0,0,0,0,3.90676116943359,0,0,0,0,3.36312866210938,0,0,0,0,0,0,0,0,0,0,0,0,-27.011116027832,0,0,0,0,0,0,2.34627532958984,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2.89590454101562,0,0,0,0,0,0,0,0,0,3.66355895996094,0,0,0,0,0,0,0,0,0,0,0,3.01023864746094,0,0,0,0,0,0,0,0,2.84080505371094,0,0,0,0,0,0,0,0,0,0,0,0,2.31929016113281,0,0,0,2.89488983154297,0,0,0,0,0,0,0,0,0,0,0,0,3.15839385986328,0,0,0,0,0,0,0,0,0,0,0,0,0,4.33113861083984,0,0,0,0,0,0,3.14057159423828,0,0,0,0,0,0,0,0,0,0,-25.9648895263672,0,0,0,0,0,0,0,3.13791656494141,0,0,0,0,0,0,0,0,0,0,0,2.54198455810547,0,0,0,0,0,0,0,0,0,4.22816467285156,0,0,0,0,0,0,5.24485778808594,0,0,0,0,0,0,0,0,0,0,0,0,3.63359069824219,0,0,0,0,0,0,0,0,0,0,0,0,0,2.999267578125,0,0,0,0,0,0,0,0,0,0,0,0,2.95619964599609,0,0,0,0,0,0,0,0,0,0,0,0,0,3.18904113769531,0,0,0,-25.0172882080078,0,0,0,0,0,2.36518859863281,0,0,0,0,0,0,0,0,0,0,0,2.78547668457031,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1.10609436035156,0,0,0,0,0,0,0,0,0,0,0,0,0,0.174720764160156,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.167701721191406,0,0,0,0,0,0,0,0.165359497070312,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.233558654785156,0,0,0,0,0,0,0,-7.58287048339844,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.166969299316406,0,0,0,0,0,0,0.188949584960938,0,0,0,0,0,0,0,0,0,0,0.164100646972656,0,0,0,0,0,0,0,0,0.239158630371094,0,0,0,0,0,0,0,0,0,0,0,0,0.231735229492188,0,0,0,0,0,0,0,0.160697937011719,0,0,0,0,0,0,0,0,0,0,0,-1.12123107910156,0,0,0,0,0,0,0,0,0,0,0,0,0,0.151771545410156,0,0,0,0,0,0,0,0,0,0,0,0.161735534667969,0,0,0,0,0,0,0,0,0,0,0,0.171829223632812,0,0,0,0,0,0.13458251953125,0,0,0,0.166542053222656,0,0,0,0,0,0,0,0.159614562988281,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.184333801269531,0,0,0,0,0,-1.11212158203125,0,0,0,0,0,0,0,0,0.140357971191406,0,0,0,0,0,0,0,0,0,0,0,0.158279418945312,0,0,0,0,0,0,0,0,0,0,0.188125610351562,0,0,0,0,0,0.158714294433594,0,0,0,0,0,0,0,0.169410705566406,0,0,0,0,0,0,0,0.135017395019531,0,0,0,0,0,0,0,0,0,0.138984680175781,0,0,0,0,0,0,0,0,0,0,0,0,0,-1.11544799804688,0,0,0,0,0,0,0,0,0,1.73372650146484,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,3.20938873291016,0,0,0,0,0,3.33203887939453,0,0,0,0,3.65134429931641,0,0,0,0,0,0,0,0,0,0,3.37938690185547,0,0,0,0,0,0,0,0,3.25368499755859,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2.88557434082031,0,0,0,0,0,0,0,0,0,0,0,3.77779388427734,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,3.19186401367188,0,0,0,0,2.80752563476562,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,-26.0277481079102,0,0,0,0,0,0,0,3.43117523193359,0,0,0,0,0,0,0,0,0,0,0,0,0,0,3.54740142822266,0,0,0,0,0,0,0,0,3.37758636474609,0,0,0,0,0,0,0,0,0,3.33527374267578,0,0,0,0,0,3.20762634277344,0,0,0,0,0,0,0,0,0,0,0,2.83913421630859,0,0,0,0,0,0,0,0,0,0,0,0,3.03742980957031,0,0,0,0,0,0,0,0,0,0,0,2.91249084472656,0,0,0,0,0,0,2.22870635986328,0,0,0,0,0,0,0,0,0,0,0,0,3.52956390380859,0,0,0,0,0,0,0,0,0,0,-29.4588470458984,0,0,0,0,0,0,0,3.16246032714844,0,0,0,0,0,0,0,0,0,0,2.8973388671875,0,0,0,3.55917358398438,0,0,0,0,0,0,0,0,0,0,0,3.66356658935547,0,0,0,0,0,0,0,0,0,0,0,3.97005462646484,0,0,0,0,0,0,0,0,0,0,2.43653106689453,0,0,0,0,0,0,0,0,0,0,2.77565765380859,0,0,0,0,0,0,0,0,0,0,0,0,3.59796905517578,0,0,0,0,0,0,0,0,0,0,0,0,3.31970977783203,0,0,0,0,0,0,0,0,0,0,0,2.84613800048828,0,0,0,0,0,0,0,0,0,0,0,0,0,-28.8018646240234,0,0,0,0,0,0,0,0,0,0,0,2.96658325195312,0,0,0,0,0,0,0,0,0,0,0,0,0,3.28638458251953,0,0,0,0,0,0,0,0,0,0,0,0,3.11025238037109,0,0,0,0,0,0,0,0,0,0,0,2.54145812988281,0,0,0,0,0,0,0,0,0,0,0,3.53612518310547,0,0,0,0,0,3.81864929199219,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,3.43272399902344,0,0,0,0,0,0,0,0,0,0,0,4.09300994873047,0,0,0,0,0,0,0,0,0,0,0,0,3.48345184326172,0,0,0,0,0,0,0,0,0,0,-27.2806091308594,0,0,0,0,0,0,0,0,0,0,0,0,3.6192626953125,0,0,0,0,0,0,0,0,0,0,0,5.02413940429688,0,0,0,0,0,0,2.92643737792969,0,0,0,0,0,0,0,0,0,0,3.49196624755859,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2.98740386962891,0,0,0,0,0,0,0,0,0,0,0,2.93413543701172,0,0,0,0,0,0,0,0,0,0,2.464111328125,0,0,0,0,0,0,0,0,0,0,3.20712280273438,0,0,0,0,3.88067626953125,0,0,0,0,0,0,0,0,0,0,0,0,0,2.68324279785156,0,0,0,0,0,0,0,0,-27.1125106811523,0,0,0,0,0,3.58148956298828,0,0,0,0,0,0,0,0,0,0,4.36193084716797,0,0,0,0,0,0,0,0,0,0,2.96629333496094,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,3.31151580810547,0,0,0,0,0,0,0,0,0,0,3.02925872802734,0,0,0,0,0,0,0,0,0,0,0,4.48743438720703,0,0,0,0,0,0,0,3.34989929199219,0,0,0,0,0,0,0,0,0,0,0,0,3.85411834716797,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,-27.2578048706055,0,0,0,0,0,0,0,0,0,0,0,3.31991577148438,0,0,0,0,0,0,0,0,0,0,0,0,4.45626068115234,0,0,0,0,0,0,0,0,0,0,0,3.16794586181641,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2.21495056152344,0,0,0,0,0,0,0,0,0,0,0,0,0,0.180595397949219,0,0,0,0,0,0,0,0,0,0,0,0,0.142189025878906,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.174560546875,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,-13.2581405639648,0,0,0,0,0,0,0,0.162025451660156,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.143516540527344,0,0,0,0,0,0,0,0,0,0.161705017089844,0,0,0,0,0,0,0,0,0.165931701660156,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.162826538085938,0,0,0,0,0,0,0,0,0,0,0,0.178955078125,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,-1.01315307617188,0,0,0,0,0,0,0,0.195808410644531,0,0,0,0,0,0,0,0.164520263671875,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.133651733398438,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.123847961425781,0,0,0,0,0,0,0.150871276855469,0,0,0,0,0,0,0.182792663574219,0,0,0,0,0,0.177520751953125,0,0,0,0,0,0,0,0,0,0,0,-0.995628356933594,0,0,0,0,0,0,0.177986145019531,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.267890930175781,0,0,0,0,0,0.161216735839844,0,0,0,0,0,0,0.202728271484375,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.195785522460938,0,0,0,0,0,0,0,0,0,0,0,-1.00657653808594,0,0,0,0,0,0,0,0,0.174522399902344,0,0,0,0,0,0,0,0.207374572753906,0,0,0,0,0,0,0,0,0.227775573730469,0,0,0,0,0,0,0,0,0,0,0,0,0,0],"filename":[null,null,"<expr>",null,null,"<expr>",null,null,"<expr>",null,null,"<expr>",null,null,"<expr>",null,null,"<expr>",null,null,"<expr>",null,null,"<expr>",null,null,"<expr>",null,null,"<expr>",null,null,"<expr>",null,null,"<expr>",null,null,"<expr>",null,null,"<expr>",null,null,"<expr>",null,null,"<expr>",null,null,"<expr>",null,null,null,"<expr>",null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,"<expr>",null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,"<expr>",null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,"<expr>",null,null,null,null,null,"<expr>",null,null,null,null,null,null,"<expr>",null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,"<expr>",null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,"<expr>",null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,"<expr>",null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,"<expr>",null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,"<expr>",null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,"<expr>",null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,"<expr>",null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>"]},"interval":10,"files":[{"filename":"<expr>","content":"library(profvis)\n\nprofvis({\n  cv_error <- vector(\"numeric\", 5)\n  terms <- 1:5\n  \n  for(i in terms){\n    loocv_models <- map(loocv_data$train, ~ lm(mpg ~ poly(horsepower, i), data = .))\n    loocv_mse <- map2_dbl(loocv_models, loocv_data$test, mse)\n    cv_error[[i]] <- mean(loocv_mse)\n  }\n})","normpath":"<expr>"}],"prof_output":"/var/folders/vw/l7k7vwhn3qqd990ww0dd101c0000gn/T//RtmpR9pbTf/file56a12c91ccd3.prof","highlight":{"output":["^output\\$"],"gc":["^<GC>$"],"stacktrace":["^\\.\\.stacktraceo(n|ff)\\.\\.$"]},"split":"h"}},"evals":[],"jsHooks":[]}</script>
</div>
<div id="fold-cv" class="section level3">
<h3>10-fold CV</h3>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(profvis)

<span class="kw">profvis</span>({
  cv_error_fold10 &lt;-<span class="st"> </span><span class="kw">vector</span>(<span class="st">&quot;numeric&quot;</span>, <span class="dv">5</span>)
  terms &lt;-<span class="st"> </span><span class="dv">1</span><span class="op">:</span><span class="dv">5</span>
  
  <span class="cf">for</span>(i <span class="cf">in</span> terms){
    cv10_models &lt;-<span class="st"> </span><span class="kw">map</span>(cv10_data<span class="op">$</span>train, <span class="op">~</span><span class="st"> </span><span class="kw">lm</span>(mpg <span class="op">~</span><span class="st"> </span><span class="kw">poly</span>(horsepower, i), <span class="dt">data =</span> .))
    cv10_mse &lt;-<span class="st"> </span><span class="kw">map2_dbl</span>(cv10_models, cv10_data<span class="op">$</span>test, mse)
    cv_error_fold10[[i]] &lt;-<span class="st"> </span><span class="kw">mean</span>(cv10_mse)
  }
})</code></pre></div>
<div id="htmlwidget-44b4ed5952b23f9e92f7" style="width:100%;height:600px;" class="profvis html-widget"></div>
<script type="application/json" data-for="htmlwidget-44b4ed5952b23f9e92f7">{"x":{"message":{"prof":{"time":[1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,2,2,2,2,2,2,2,3,3,3,3,3,3,3,3,3,3,3,3,3,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,5,5,5,5,5,5,5,5,5,5,5,5,6,6,6,6,6,6,6,6,6,6,6,6,6,6,7,7,7,7,7,7,7,8,8,8,8,8,8,8,8,8,8,8,8,8,9,9,9,9,9,9,9,9,9,9,9,9,9,10,10,10,10,10,10,10,10,11,11,11,11,11,11,11,11,11,12,12,12,12,12,12,12,12,12,12,12,12,12],"depth":[15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,7,6,5,4,3,2,1,13,12,11,10,9,8,7,6,5,4,3,2,1,16,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,12,11,10,9,8,7,6,5,4,3,2,1,14,13,12,11,10,9,8,7,6,5,4,3,2,1,7,6,5,4,3,2,1,13,12,11,10,9,8,7,6,5,4,3,2,1,13,12,11,10,9,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,13,12,11,10,9,8,7,6,5,4,3,2,1],"label":["recycle_columns","list_to_tibble","as_tibble.data.frame","[.tbl_df","[","as.data.frame.resample","as.data.frame","model.frame.default","stats::model.frame","eval","eval","lm",".f",".Call","map",".getNamespaceInfo","getExportedValue","::","modelr:::residuals",".f",".Call","map2_dbl","c","structure","poly","eval","eval","model.frame.default","stats::model.frame","eval","eval","lm",".f",".Call","map","match","%in%","deparse","paste","FUN","lapply","sapply","match","model.matrix.default","model.matrix","predict.lm","stats::predict","modelr:::residuals",".f",".Call","map2_dbl","outer","poly","eval","eval","model.frame.default","stats::model.frame","eval","eval","lm",".f",".Call","map","[.data.frame","[","na.exclude.data.frame","stats::na.exclude","<Anonymous>",".External2","model.frame.default","stats::model.frame","eval","eval","lm",".f",".Call","map","delete.response","predict.lm","stats::predict","modelr:::residuals",".f",".Call","map2_dbl",".Fortran","qr.qy","poly","eval","eval","model.frame.default","stats::model.frame","eval","eval","lm",".f",".Call","map","lapply","[.tbl_df","[","as.data.frame.resample","as.data.frame","model.frame.default","stats::model.frame","eval","eval","lm",".f",".Call","map","terms.default","terms","predict.lm","stats::predict","modelr:::residuals",".f",".Call","map2_dbl",".External2","model.frame.default","stats::model.frame","eval","eval","lm",".f",".Call","map","<GC>","match","%in%","[[.data.frame","[[","$.data.frame","$","model.weights","as.vector","lm",".f",".Call","map"],"filenum":[null,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,1],"linenum":[null,null,null,null,null,null,null,null,null,null,null,null,null,null,8,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,8,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,8,null,null,null,null,null,null,null,null,null,null,null,null,null,8,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,8,null,null,null,null,null,null,null,null,null,null,null,null,8,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,8,null,null,null,null,null,null,null,null,null,null,null,null,8],"memalloc":[120.618476867676,120.618476867676,120.618476867676,120.618476867676,120.618476867676,120.618476867676,120.618476867676,120.618476867676,120.618476867676,120.618476867676,120.618476867676,120.618476867676,120.618476867676,120.618476867676,120.618476867676,121.69019317627,121.69019317627,121.69019317627,121.69019317627,121.69019317627,121.69019317627,121.69019317627,123.473831176758,123.473831176758,123.473831176758,123.473831176758,123.473831176758,123.473831176758,123.473831176758,123.473831176758,123.473831176758,123.473831176758,123.473831176758,123.473831176758,123.473831176758,124.940536499023,124.940536499023,124.940536499023,124.940536499023,124.940536499023,124.940536499023,124.940536499023,124.940536499023,124.940536499023,124.940536499023,124.940536499023,124.940536499023,124.940536499023,124.940536499023,124.940536499023,124.940536499023,126.272155761719,126.272155761719,126.272155761719,126.272155761719,126.272155761719,126.272155761719,126.272155761719,126.272155761719,126.272155761719,126.272155761719,126.272155761719,126.272155761719,128.314933776855,128.314933776855,128.314933776855,128.314933776855,128.314933776855,128.314933776855,128.314933776855,128.314933776855,128.314933776855,128.314933776855,128.314933776855,128.314933776855,128.314933776855,128.314933776855,129.036209106445,129.036209106445,129.036209106445,129.036209106445,129.036209106445,129.036209106445,129.036209106445,131.017562866211,131.017562866211,131.017562866211,131.017562866211,131.017562866211,131.017562866211,131.017562866211,131.017562866211,131.017562866211,131.017562866211,131.017562866211,131.017562866211,131.017562866211,132.972763061523,132.972763061523,132.972763061523,132.972763061523,132.972763061523,132.972763061523,132.972763061523,132.972763061523,132.972763061523,132.972763061523,132.972763061523,132.972763061523,132.972763061523,133.718391418457,133.718391418457,133.718391418457,133.718391418457,133.718391418457,133.718391418457,133.718391418457,133.718391418457,136.60311126709,136.60311126709,136.60311126709,136.60311126709,136.60311126709,136.60311126709,136.60311126709,136.60311126709,136.60311126709,138.679496765137,138.679496765137,138.679496765137,138.679496765137,138.679496765137,138.679496765137,138.679496765137,138.679496765137,138.679496765137,138.679496765137,138.679496765137,138.679496765137,138.679496765137],"meminc":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1.07171630859375,0,0,0,0,0,0,1.78363800048828,0,0,0,0,0,0,0,0,0,0,0,0,1.46670532226562,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1.33161926269531,0,0,0,0,0,0,0,0,0,0,0,2.04277801513672,0,0,0,0,0,0,0,0,0,0,0,0,0,0.721275329589844,0,0,0,0,0,0,1.98135375976562,0,0,0,0,0,0,0,0,0,0,0,0,1.9552001953125,0,0,0,0,0,0,0,0,0,0,0,0,0.745628356933594,0,0,0,0,0,0,0,2.88471984863281,0,0,0,0,0,0,0,0,2.07638549804688,0,0,0,0,0,0,0,0,0,0,0,0],"filename":[null,null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,"<expr>"]},"interval":10,"files":[{"filename":"<expr>","content":"library(profvis)\n\nprofvis({\n  cv_error_fold10 <- vector(\"numeric\", 5)\n  terms <- 1:5\n  \n  for(i in terms){\n    cv10_models <- map(cv10_data$train, ~ lm(mpg ~ poly(horsepower, i), data = .))\n    cv10_mse <- map2_dbl(cv10_models, cv10_data$test, mse)\n    cv_error_fold10[[i]] <- mean(cv10_mse)\n  }\n})","normpath":"<expr>"}],"prof_output":"/var/folders/vw/l7k7vwhn3qqd990ww0dd101c0000gn/T//Rtmpuita0T/file562f351da299.prof","highlight":{"output":["^output\\$"],"gc":["^<GC>$"],"stacktrace":["^\\.\\.stacktraceo(n|ff)\\.\\.$"]},"split":"h"}},"evals":[],"jsHooks":[]}</script>
<p>On my machine, 10-fold CV was about 40 times faster than LOOCV. Again, estimating <span class="math inline">\(k=10\)</span> models is going to be much easier than estimating <span class="math inline">\(k=392\)</span> models.</p>
</div>
</div>
<div id="k-fold-cv-in-logistic-regression" class="section level2">
<h2>k-fold CV in logistic regression</h2>
<p>You’ve gotten the idea by now, but let’s do it one more time on our interactive Titanic model.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">titanic_kfold &lt;-<span class="st"> </span><span class="kw">crossv_kfold</span>(titanic, <span class="dt">k =</span> <span class="dv">10</span>)
titanic_models &lt;-<span class="st"> </span><span class="kw">map</span>(titanic_kfold<span class="op">$</span>train, <span class="op">~</span><span class="st"> </span><span class="kw">glm</span>(Survived <span class="op">~</span><span class="st"> </span>Age <span class="op">*</span><span class="st"> </span>Sex, <span class="dt">data =</span> .,
                                               <span class="dt">family =</span> binomial))
titanic_mse &lt;-<span class="st"> </span><span class="kw">map2_dbl</span>(titanic_models, titanic_kfold<span class="op">$</span>test, mse.glm)
<span class="kw">mean</span>(titanic_mse, <span class="dt">na.rm =</span> <span class="ot">TRUE</span>)</code></pre></div>
<pre><code>## [1] 0.1707907</code></pre>
<p>Not a large difference from the LOOCV approach, but it take much less time to compute.</p>
</div>
<div id="exercise-k-fold-cv" class="section level2">
<h2>Exercise: k-fold CV</h2>
<ol style="list-style-type: decimal">
<li><p>Estimate the 10-fold CV MSE of a linear regression of the relationship between admission rate and cost in the <a href="stat002_linear_models.html#exercise:_linear_regression_with_scorecard"><code>scorecard</code> dataset</a>.</p>
<details> <summary>Click for the solution</summary>
<p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">scorecard_cv10 &lt;-<span class="st"> </span><span class="kw">crossv_kfold</span>(scorecard, <span class="dt">k =</span> <span class="dv">10</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">model =</span> <span class="kw">map</span>(train, <span class="op">~</span><span class="st"> </span><span class="kw">lm</span>(cost <span class="op">~</span><span class="st"> </span>admrate, <span class="dt">data =</span> .)),
         <span class="dt">mse =</span> <span class="kw">map2_dbl</span>(model, test, mse))
<span class="kw">mean</span>(scorecard_cv10<span class="op">$</span>mse, <span class="dt">na.rm =</span> <span class="ot">TRUE</span>)</code></pre></div>
<pre><code>## [1] 147755418</code></pre>
</p>
<p></details></p></li>
<li><p>Estimate the 10-fold CV MSE of a <a href="stat003_logistic_regression.html#exercise:_logistic_regression_with_mental_health">logistic regression model of voter turnout</a> using only <code>mhealth</code> as the predictor. Compare this to the LOOCV MSE of a logistic regression model using all available predictors. Which is the better model?</p>
<details> <summary>Click for the solution</summary>
<p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># basic model</span>
mh_cv10_lite &lt;-<span class="st"> </span><span class="kw">crossv_kfold</span>(mental_health, <span class="dt">k =</span> <span class="dv">10</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">model =</span> <span class="kw">map</span>(train, <span class="op">~</span><span class="st"> </span><span class="kw">glm</span>(vote96 <span class="op">~</span><span class="st"> </span>mhealth, <span class="dt">data =</span> .,
                                  <span class="dt">family =</span> binomial)),
         <span class="dt">mse =</span> <span class="kw">map2_dbl</span>(model, test, mse.glm))
<span class="kw">mean</span>(mh_cv10_lite<span class="op">$</span>mse, <span class="dt">na.rm =</span> <span class="ot">TRUE</span>)</code></pre></div>
<pre><code>## [1] 0.2094337</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># full model</span>
mh_cv10_full &lt;-<span class="st"> </span><span class="kw">crossv_kfold</span>(mental_health, <span class="dt">k =</span> <span class="dv">10</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">model =</span> <span class="kw">map</span>(train, <span class="op">~</span><span class="st"> </span><span class="kw">glm</span>(vote96 <span class="op">~</span><span class="st"> </span>., <span class="dt">data =</span> .,
                                  <span class="dt">family =</span> binomial)),
         <span class="dt">mse =</span> <span class="kw">map2_dbl</span>(model, test, mse.glm))
<span class="kw">mean</span>(mh_cv10_full<span class="op">$</span>mse, <span class="dt">na.rm =</span> <span class="ot">TRUE</span>)</code></pre></div>
<pre><code>## [1] 0.182505</code></pre>
</p>
<p></details></p></li>
</ol>
</div>
</div>
<div id="the-bootstrap" class="section level1">
<h1>The bootstrap</h1>
<p>The <strong>bootstrap</strong> is a different resampling-based method for quantifying uncertainty associated with a given estimator or statistical method. It is extremely flexible and can be applied to virtually any statistical method.</p>
<div id="sampling-with-replacement" class="section level2">
<h2>Sampling with replacement</h2>
<p>Sampling with replacement allows us to potentially draw the same observation multiple times, and ignore other observations entirely.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">rerun</span>(<span class="dv">10</span>, <span class="kw">sample.int</span>(<span class="dv">10</span>, <span class="dt">replace =</span> <span class="ot">TRUE</span>)) <span class="op">%&gt;%</span>
<span class="st">  </span>unlist <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">matrix</span>(<span class="dt">ncol =</span> <span class="dv">10</span>, <span class="dt">byrow =</span> <span class="ot">TRUE</span>)</code></pre></div>
<pre><code>##       [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]
##  [1,]    6    2    6   10    3    8    2    7    3     2
##  [2,]    4    6    5    4    3    7    3    1    8     1
##  [3,]    2    5    9    8    8    8    3    9    9     2
##  [4,]    5    5    4   10    4    6    6   10    3     2
##  [5,]    1    6    2    9    6    7   10   10    8     3
##  [6,]    9    9    7    3    9    3   10    9    2     1
##  [7,]    1    7    5    9    2    6    1    1    7     7
##  [8,]    1    3   10    2    6   10    8    7    8     4
##  [9,]    2    5    5    2    2   10    7    1    8     6
## [10,]    5    8    1    9    9    8    3    6   10     7</code></pre>
<p>Here I’ve drawn 10 random samples with replacement from the vector <span class="math inline">\(1, 2, 3, 4, 5, 6, 7, 8, 9, 10\)</span>. Each row contains a different sample. Notice how some rows contain multiples of the same values and exclude others entirely. Compare this to <strong>sampling without replacement</strong>:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">rerun</span>(<span class="dv">10</span>, <span class="kw">sample.int</span>(<span class="dv">10</span>, <span class="dt">replace =</span> <span class="ot">FALSE</span>)) <span class="op">%&gt;%</span>
<span class="st">  </span>unlist <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">matrix</span>(<span class="dt">ncol =</span> <span class="dv">10</span>, <span class="dt">byrow =</span> <span class="ot">TRUE</span>)</code></pre></div>
<pre><code>##       [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]
##  [1,]    7    2    3    4    9    5    8    1   10     6
##  [2,]    8    7    6    4    9    2   10    1    3     5
##  [3,]    2    5   10    4    8    7    6    3    9     1
##  [4,]    8    3    7    9   10    6    1    2    5     4
##  [5,]    5    4   10    1    8    9    6    2    7     3
##  [6,]    1   10    8    6    9    7    5    3    4     2
##  [7,]    3    1   10    2    7    6    9    5    8     4
##  [8,]    9    3    6    4    2    8    5    1   10     7
##  [9,]    5    1    3    2    6   10    9    7    8     4
## [10,]    8    7    5    2    6    1    9    3   10     4</code></pre>
<p>Sampling without replacement doesn’t allow you to draw an observation more than once and would produce the exact same sample every time, just in a different order.</p>
</div>
<div id="why-use-the-bootstrap" class="section level2">
<h2>Why use the bootstrap?</h2>
<p>Statistical learning methods are frequently used to draw inferences about a population. Since you cannot directly measure the entire population<a href="#fn5" class="footnoteRef" id="fnref5"><sup>5</sup></a>, you take a sample and ask a question of it instead. But how do you know your sample answer is close to the population answer? There are two approaches you can take:</p>
<ol style="list-style-type: decimal">
<li>Make <strong>assumptions</strong> about the shape of the population.</li>
<li>Use the <strong>information in the sample</strong> to learn about it.</li>
</ol>
<p>Suppose you decide to make assumptions, e.g. that the sample is distributed normally or <a href="https://en.wikipedia.org/wiki/Bernoulli_distribution">Bernoulli</a> or some other probability distribution. You could learn about how much the answer to your question varies based on the specific sample drawn by repeatedly generating samples of the same size and asking them the same question. If you have a computaionally convenient assumption (such as the <a href="https://en.wikipedia.org/wiki/Central_limit_theorem">Central Limit Theorem</a>), you may even be able to bypass the resampling step and use a known formula to estimate your confidence in the original answer.</p>
<p>Provided you are happy to make the assumptions, this seems like a good idea. If you are not willing to make the assumption, you could instead take the sample you have and sample from it. You can do this because the sample you have <strong>is also a population</strong>, just a very small and discrete one. It is identical to the histogram of your data. Sampling with replacement merely allows you to treat the sample like it’s a population and sample from it in a way that reflects its shape.</p>
<p>This is a reasonable thing to do for a couple reasons. First, it’s the only information you have about the population. Second, randomly chosen samples should look quite similar to the population from which they came, so as long as you drew a random sample it is likely that your’s is also similar.</p>
<div id="estimating-the-accuracy-of-a-statistic-of-interest" class="section level3">
<h3>Estimating the accuracy of a statistic of interest</h3>
<p>Suppose you want to know how often Americans eat ice cream in a given month.</p>
<div class="figure">
<img src="https://upload.wikimedia.org/wikipedia/commons/thumb/1/1c/Rockyroadicecream.jpg/800px-Rockyroadicecream.jpg" />

</div>
<p>We decide to estimate this by tracking a sample of 1000 Americans and counting how many times they eat ice cream over the course of a month.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># simulate the sample</span>
<span class="kw">set.seed</span>(<span class="dv">1234</span>)
lambda &lt;-<span class="st"> </span><span class="dv">5</span>
n_obs &lt;-<span class="st"> </span><span class="dv">1000</span>
ice &lt;-<span class="st"> </span><span class="kw">data_frame</span>(<span class="dt">sim =</span> <span class="kw">rpois</span>(n_obs, <span class="dt">lambda =</span> lambda))

<span class="kw">ggplot</span>(ice, <span class="kw">aes</span>(sim)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_histogram</span>(<span class="dt">binwidth =</span> <span class="dv">1</span>)</code></pre></div>
<p><img src="stat005_resampling_files/figure-html/ice-sim-1.png" width="672" /></p>
<p>The mean of this sample is 5.062, which we will treat as the population mean <span class="math inline">\(\mu\)</span>. Remember that in the real world, we do not know <span class="math inline">\(\mu\)</span> because we have not observed all members of the population. Instead, we use the sample to estimate <span class="math inline">\(\hat{\mu}\)</span> on the assumption that the sample mean approximates the true mean.</p>
<p>The <a href="https://en.wikipedia.org/wiki/Poisson_distribution">Poisson distribution</a> is the most likely population distribution as it describes the count of event over time. The probability mass function of the Poisson distribution is</p>
<p><span class="math display">\[P(k) = \frac{\lambda^{k} e^{-k}}{k!}\]</span></p>
<p>where <span class="math inline">\(\lambda\)</span> is the event rate (average number of events per interval), <span class="math inline">\(e\)</span> is Euler’s number, <span class="math inline">\(k\)</span> is an integer with range <span class="math inline">\([0, \infty]\)</span>, and <span class="math inline">\(k!\)</span> is the factorial of <span class="math inline">\(k\)</span>. The mean <span class="math inline">\(\mu\)</span> and variance <span class="math inline">\(\sigma\)</span> of a Poisson distribution is defined as <span class="math inline">\(\lambda\)</span>.</p>
<p>Because we are estimating <span class="math inline">\(\mu\)</span> from a sample, we should also estimate the <strong>standard error</strong> of the sample mean. This is necessary because any random sample drawn from a population will not exactly reproduce the population. We need to account for sampling error by estimating how much our sample mean <span class="math inline">\(\hat{\mu}\)</span> might differ from the true mean <span class="math inline">\(\mu\)</span>.</p>
<p>The distribution of the mean of a set of samples is approximately <a href="https://en.wikipedia.org/wiki/Normal_distribution">normally distributed</a>.<a href="#fn6" class="footnoteRef" id="fnref6"><sup>6</sup></a> Therefore the standard error of the sample mean from a Poisson distribution is</p>
<p><span class="math display">\[\sqrt{\frac{\lambda}{n}}\]</span></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">lambda_samp &lt;-<span class="st"> </span><span class="kw">mean</span>(ice<span class="op">$</span>sim)

(sem &lt;-<span class="st"> </span><span class="kw">sqrt</span>(lambda_samp <span class="op">/</span><span class="st"> </span>n_obs))</code></pre></div>
<pre><code>## [1] 0.07114773</code></pre>
<p>The standard error of the sample mean is <span class="math inline">\(0.071\)</span>. This is a good estimate <strong>as long as the data generating process actually followed a Poisson distribution</strong>. The Poisson distribution requires <a href="https://en.wikipedia.org/wiki/Poisson_distribution#Assumptions:_When_is_the_Poisson_distribution_an_appropriate_model.3F">several assumptions</a>. If any of these assumptions are violated, then the formula for estimating the standard error of the sample mean <span class="math inline">\(\lambda\)</span> will not be accurate.</p>
<p>In that situation, we can use the bootstrap to estimate the standard error without making any distributional assumptions. In this approach, we draw <span class="math inline">\(B\)</span> samples with replacement from the original sample. To estimate the population mean <span class="math inline">\(\mu\)</span> we calculate the mean of the bootstrapped sample means <span class="math inline">\(\hat{\mu}_1, \hat{\mu}_2, \dots, \hat{\mu}_B\)</span>. To estimate the standard error of the sampling mean <span class="math inline">\(\hat{\mu}\)</span> we use the formula</p>
<p><span class="math display">\[SE_{B}(\hat{\mu}) = \sqrt{\frac{1}{B-1} \sum_{r = 1}^{B} \left( \hat{\mu}_r - \frac{1}{B} \sum_{r&#39; = 1}^{B} \hat{\mu}_{r&#39;} \right)^2}\]</span></p>
<p>What this boils down to is calculating the <strong>standard deviation</strong> of all the bootstrapped sample means. That gives us our standard error.</p>
<p>Let’s bootstrap our standard error of the mean for our simulated ice cream data. We’ll use <span class="math inline">\(B = 1000\)</span> to produce 1000 bootstrapped estimates of the mean, then calculate the standard deviation of them:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">ice_boot &lt;-<span class="st"> </span>ice <span class="op">%&gt;%</span>
<span class="st">  </span>modelr<span class="op">::</span><span class="kw">bootstrap</span>(<span class="dv">1000</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">mean =</span> <span class="kw">map_dbl</span>(strap, <span class="op">~</span><span class="st"> </span><span class="kw">mean</span>(<span class="kw">as_tibble</span>(.)<span class="op">$</span>sim, <span class="dt">na.rm =</span> <span class="ot">TRUE</span>)))
boot_sem &lt;-<span class="st"> </span><span class="kw">sd</span>(ice_boot<span class="op">$</span>mean)</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>(ice_boot, <span class="kw">aes</span>(mean)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_histogram</span>(<span class="dt">binwidth =</span> .<span class="dv">01</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_vline</span>(<span class="kw">aes</span>(<span class="dt">xintercept =</span> lambda, <span class="dt">color =</span> <span class="st">&quot;Population mean&quot;</span>), <span class="dt">size =</span> <span class="dv">1</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_vline</span>(<span class="kw">aes</span>(<span class="dt">xintercept =</span> lambda_samp, <span class="dt">color =</span> <span class="st">&quot;Sample mean&quot;</span>), <span class="dt">size =</span> <span class="dv">1</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_vline</span>(<span class="kw">aes</span>(<span class="dt">xintercept =</span> <span class="kw">mean</span>(ice_boot<span class="op">$</span>mean),
                 <span class="dt">color =</span> <span class="st">&quot;Bootstrapped mean&quot;</span>), <span class="dt">size =</span> <span class="dv">1</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_vline</span>(<span class="kw">aes</span>(<span class="dt">xintercept =</span> <span class="kw">mean</span>(ice_boot<span class="op">$</span>mean) <span class="op">+</span><span class="st"> </span><span class="fl">1.96</span> <span class="op">*</span><span class="st"> </span>boot_sem,
                 <span class="dt">color =</span> <span class="st">&quot;Bootstrapped mean&quot;</span>), <span class="dt">linetype =</span> <span class="dv">2</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_vline</span>(<span class="kw">aes</span>(<span class="dt">xintercept =</span> <span class="kw">mean</span>(ice_boot<span class="op">$</span>mean) <span class="op">-</span><span class="st"> </span><span class="fl">1.96</span> <span class="op">*</span><span class="st"> </span>boot_sem,
                 <span class="dt">color =</span> <span class="st">&quot;Bootstrapped mean&quot;</span>), <span class="dt">linetype =</span> <span class="dv">2</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_vline</span>(<span class="kw">aes</span>(<span class="dt">xintercept =</span> lambda_samp <span class="op">+</span><span class="st"> </span><span class="fl">1.96</span> <span class="op">*</span><span class="st"> </span>sem, <span class="dt">color =</span> <span class="st">&quot;Sample mean&quot;</span>),
             <span class="dt">linetype =</span> <span class="dv">2</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_vline</span>(<span class="kw">aes</span>(<span class="dt">xintercept =</span> lambda_samp <span class="op">-</span><span class="st"> </span><span class="fl">1.96</span> <span class="op">*</span><span class="st"> </span>sem, <span class="dt">color =</span> <span class="st">&quot;Sample mean&quot;</span>),
             <span class="dt">linetype =</span> <span class="dv">2</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">scale_color_manual</span>(<span class="dt">name =</span> <span class="ot">NULL</span>, <span class="dt">breaks =</span> <span class="kw">c</span>(<span class="st">&quot;Population mean&quot;</span>, <span class="st">&quot;Sample mean&quot;</span>,
                                             <span class="st">&quot;Bootstrapped mean&quot;</span>),
                     <span class="dt">values =</span> <span class="kw">c</span>(<span class="st">&quot;blue&quot;</span>, <span class="st">&quot;green&quot;</span>, <span class="st">&quot;orange&quot;</span>)) <span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">x =</span> <span class="st">&quot;Bootstrapped sample mean&quot;</span>,
       <span class="dt">y =</span> <span class="st">&quot;Count&quot;</span>)</code></pre></div>
<p><img src="stat005_resampling_files/figure-html/ice-boot-plot-1.png" width="672" /></p>
<p>The bootstrap estimate of the standard error of the sample mean is 0.074. Compared to the original estimate of 0.071, this is slightly closer to the defined population mean, but not by much. Why bother using the bootstrap? Because the bootstrap estimator will be more accurate <strong>when the distributional assumptions are not met</strong>.</p>
<p>Let’s simulate the results once again, but draw the observations from a combination of the Poisson distribution and <a href="https://en.wikipedia.org/wiki/Uniform_distribution_(continuous)">uniform distribution</a>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># simulate the sample</span>
<span class="kw">set.seed</span>(<span class="dv">113</span>)
ice2 &lt;-<span class="st"> </span><span class="kw">data_frame</span>(<span class="dt">sim =</span> <span class="kw">c</span>(<span class="kw">rpois</span>(n_obs <span class="op">/</span><span class="st"> </span><span class="dv">2</span>, <span class="dt">lambda =</span> lambda),
                           <span class="kw">round</span>(<span class="kw">runif</span>(n_obs <span class="op">/</span><span class="st"> </span><span class="dv">2</span>, <span class="dt">min =</span> <span class="dv">0</span>, <span class="dt">max =</span> <span class="dv">10</span>))))

<span class="co"># plot the sample distribution</span>
<span class="kw">ggplot</span>(ice2, <span class="kw">aes</span>(sim)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_histogram</span>(<span class="dt">binwidth =</span> <span class="dv">1</span>)</code></pre></div>
<p><img src="stat005_resampling_files/figure-html/ice-sim2-1.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># calculate sample mean and standard error</span>
lambda2_samp &lt;-<span class="st"> </span><span class="kw">mean</span>(ice2<span class="op">$</span>sim)
sem2 &lt;-<span class="st"> </span><span class="kw">sqrt</span>(lambda2_samp <span class="op">/</span><span class="st"> </span>n_obs)

<span class="co"># calculate the bootstrap</span>
ice2_boot &lt;-<span class="st"> </span>ice2 <span class="op">%&gt;%</span>
<span class="st">  </span>modelr<span class="op">::</span><span class="kw">bootstrap</span>(<span class="dv">1000</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">mean =</span> <span class="kw">map_dbl</span>(strap, <span class="op">~</span><span class="st"> </span><span class="kw">mean</span>(<span class="kw">as_tibble</span>(.)<span class="op">$</span>sim, <span class="dt">na.rm =</span> <span class="ot">TRUE</span>)))
boot2_sem &lt;-<span class="st"> </span><span class="kw">sd</span>(ice2_boot<span class="op">$</span>mean)

<span class="co"># plot the bootstrapped distribution</span>
<span class="kw">ggplot</span>(ice2_boot, <span class="kw">aes</span>(mean)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_histogram</span>(<span class="dt">binwidth =</span> .<span class="dv">01</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_vline</span>(<span class="kw">aes</span>(<span class="dt">xintercept =</span> lambda, <span class="dt">color =</span> <span class="st">&quot;Population mean&quot;</span>), <span class="dt">size =</span> <span class="dv">1</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_vline</span>(<span class="kw">aes</span>(<span class="dt">xintercept =</span> lambda2_samp, <span class="dt">color =</span> <span class="st">&quot;Sample mean&quot;</span>), <span class="dt">size =</span> <span class="dv">1</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_vline</span>(<span class="kw">aes</span>(<span class="dt">xintercept =</span> <span class="kw">mean</span>(ice2_boot<span class="op">$</span>mean),
                 <span class="dt">color =</span> <span class="st">&quot;Bootstrapped mean&quot;</span>), <span class="dt">size =</span> <span class="dv">1</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_vline</span>(<span class="kw">aes</span>(<span class="dt">xintercept =</span> <span class="kw">mean</span>(ice2_boot<span class="op">$</span>mean) <span class="op">+</span><span class="st"> </span><span class="fl">1.96</span> <span class="op">*</span><span class="st"> </span>boot2_sem,
                 <span class="dt">color =</span> <span class="st">&quot;Bootstrapped mean&quot;</span>), <span class="dt">linetype =</span> <span class="dv">2</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_vline</span>(<span class="kw">aes</span>(<span class="dt">xintercept =</span> <span class="kw">mean</span>(ice2_boot<span class="op">$</span>mean) <span class="op">-</span><span class="st"> </span><span class="fl">1.96</span> <span class="op">*</span><span class="st"> </span>boot2_sem,
                 <span class="dt">color =</span> <span class="st">&quot;Bootstrapped mean&quot;</span>), <span class="dt">linetype =</span> <span class="dv">2</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_vline</span>(<span class="kw">aes</span>(<span class="dt">xintercept =</span> lambda2_samp <span class="op">+</span><span class="st"> </span><span class="fl">1.96</span> <span class="op">*</span><span class="st"> </span>sem2, <span class="dt">color =</span> <span class="st">&quot;Sample mean&quot;</span>),
             <span class="dt">linetype =</span> <span class="dv">2</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_vline</span>(<span class="kw">aes</span>(<span class="dt">xintercept =</span> lambda2_samp <span class="op">-</span><span class="st"> </span><span class="fl">1.96</span> <span class="op">*</span><span class="st"> </span>sem2, <span class="dt">color =</span> <span class="st">&quot;Sample mean&quot;</span>),
             <span class="dt">linetype =</span> <span class="dv">2</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">scale_color_manual</span>(<span class="dt">name =</span> <span class="ot">NULL</span>, <span class="dt">breaks =</span> <span class="kw">c</span>(<span class="st">&quot;Population mean&quot;</span>, <span class="st">&quot;Sample mean&quot;</span>,
                                             <span class="st">&quot;Bootstrapped mean&quot;</span>),
                     <span class="dt">values =</span> <span class="kw">c</span>(<span class="st">&quot;blue&quot;</span>, <span class="st">&quot;green&quot;</span>, <span class="st">&quot;orange&quot;</span>)) <span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">x =</span> <span class="st">&quot;Bootstrapped sample mean&quot;</span>,
       <span class="dt">y =</span> <span class="st">&quot;Count&quot;</span>)</code></pre></div>
<p><img src="stat005_resampling_files/figure-html/ice-sim2-2.png" width="672" /></p>
<p>The population mean <span class="math inline">\(\mu\)</span> is still defined as 5, but now look what happens to the standard errors of the estimates. The estimated means are identical under the formula-based or bootstrapped approaches (5.147), however the standard error for the sample-based approach is <span class="math inline">\(0.072\)</span>, compared to 0.084. Because the bootstrap approach generates its estimate of the standard error directly from the data, the bootstrapped 95% confidence interval includes the population mean. However the 95% confidence interval under the formula-based method does not include the population mean. In this case we are better off using the bootstrapped standard error rather than using the formula for the Poisson distribution.</p>
</div>
<div id="estimating-the-accuracy-of-a-linear-regression-model" class="section level3">
<h3>Estimating the accuracy of a linear regression model</h3>
<p>In a linear regression model, the standard errors are statistical estimates of the average amount that the estimated parameters <span class="math inline">\(\hat{\beta}\)</span> differ from the true population parameters <span class="math inline">\(\beta\)</span>. The formula for estimating standard errors for a linear regression model is:</p>
<p><span class="math display">\[\widehat{s.e.}(\hat{\beta}_j) = \sqrt{\sigma^{2} (X^{T}X)^{-1}_{jj}}\]</span></p>
<p>More simply this is the square root of the diagonal of the <a href="https://en.wikipedia.org/wiki/Ordinary_least_squares#Finite_sample_properties">variance-covariance matrix</a>. For the formula to hold, we make certain assumptions, including that our estimate of <span class="math inline">\(\sigma^2\)</span> is accurate and that any variability in the model after we account for <span class="math inline">\(X\)</span> is the result of the errors <span class="math inline">\(\epsilon\)</span>. If these assumptions are wrong, then our estimates of the standard errors will also be wrong.</p>
<p>Let’s revisit our horsepower and highway mileage linear model.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># plot the data and model</span>
<span class="kw">ggplot</span>(Auto, <span class="kw">aes</span>(horsepower, mpg)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_point</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_smooth</span>(<span class="dt">method =</span> <span class="st">&quot;lm&quot;</span>)</code></pre></div>
<p><img src="stat005_resampling_files/figure-html/auto-boot-1.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># traditional parameter estimates and standard errors</span>
auto_lm &lt;-<span class="st"> </span><span class="kw">lm</span>(mpg <span class="op">~</span><span class="st"> </span>horsepower, <span class="dt">data =</span> Auto)
<span class="kw">tidy</span>(auto_lm)</code></pre></div>
<pre><code>##          term   estimate   std.error statistic       p.value
## 1 (Intercept) 39.9358610 0.717498656  55.65984 1.220362e-187
## 2  horsepower -0.1578447 0.006445501 -24.48914  7.031989e-81</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># bootstrapped estimates of the parameter estimates and standard errors</span>
auto_boot &lt;-<span class="st"> </span>Auto <span class="op">%&gt;%</span>
<span class="st">  </span>modelr<span class="op">::</span><span class="kw">bootstrap</span>(<span class="dv">1000</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">model =</span> <span class="kw">map</span>(strap, <span class="op">~</span><span class="st"> </span><span class="kw">lm</span>(mpg <span class="op">~</span><span class="st"> </span>horsepower, <span class="dt">data =</span> .)),
         <span class="dt">coef =</span> <span class="kw">map</span>(model, tidy))

auto_boot <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">unnest</span>(coef) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">group_by</span>(term) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">summarize</span>(<span class="dt">est.boot =</span> <span class="kw">mean</span>(estimate),
            <span class="dt">se.boot =</span> <span class="kw">sd</span>(estimate, <span class="dt">na.rm =</span> <span class="ot">TRUE</span>))</code></pre></div>
<pre><code>## # A tibble: 2 × 3
##          term   est.boot     se.boot
##         &lt;chr&gt;      &lt;dbl&gt;       &lt;dbl&gt;
## 1 (Intercept) 39.9759145 0.850939151
## 2  horsepower -0.1583452 0.007316326</code></pre>
<p>The bootstrapped estimates of parameters are virtually identical, however the standard errors on the bootstrap estimates are slightly larger. This is because they do not rely on any distributional assumptions, whereas the traditional estimates do. Recall from the <a href="#regression">demonstration above</a> that the relationship between horsepower and mpg is non-linear, so the residuals from a linear model will be inflated, and the residuals are used to estimate <span class="math inline">\(\sigma^2\)</span>. The bootstrap method is not biased by these assumptions and gives us a more robust estimate.</p>
<p>If we compare the traditional and bootstrap estimates for the polynomial regression model, we find more similarity in our results:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># traditional parameter estimates and standard errors</span>
auto2_lm &lt;-<span class="st"> </span><span class="kw">lm</span>(mpg <span class="op">~</span><span class="st"> </span>horsepower <span class="op">+</span><span class="st"> </span><span class="kw">I</span>(horsepower<span class="op">^</span><span class="dv">2</span>), <span class="dt">data =</span> Auto)
<span class="kw">tidy</span>(auto2_lm)</code></pre></div>
<pre><code>##              term     estimate    std.error statistic       p.value
## 1     (Intercept) 56.900099702 1.8004268063  31.60367 1.740911e-109
## 2      horsepower -0.466189630 0.0311246171 -14.97816  2.289429e-40
## 3 I(horsepower^2)  0.001230536 0.0001220759  10.08009  2.196340e-21</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># bootstrapped estimates of the parameter estimates and standard errors</span>
auto2_boot &lt;-<span class="st"> </span>Auto <span class="op">%&gt;%</span>
<span class="st">  </span>modelr<span class="op">::</span><span class="kw">bootstrap</span>(<span class="dv">1000</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">model =</span> <span class="kw">map</span>(strap, <span class="op">~</span><span class="st"> </span><span class="kw">lm</span>(mpg <span class="op">~</span><span class="st"> </span>horsepower <span class="op">+</span><span class="st"> </span><span class="kw">I</span>(horsepower<span class="op">^</span><span class="dv">2</span>), <span class="dt">data =</span> .)),
         <span class="dt">coef =</span> <span class="kw">map</span>(model, tidy))

auto2_boot <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">unnest</span>(coef) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">group_by</span>(term) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">summarize</span>(<span class="dt">est.boot =</span> <span class="kw">mean</span>(estimate),
            <span class="dt">se.boot =</span> <span class="kw">sd</span>(estimate, <span class="dt">na.rm =</span> <span class="ot">TRUE</span>))</code></pre></div>
<pre><code>## # A tibble: 3 × 3
##              term     est.boot      se.boot
##             &lt;chr&gt;        &lt;dbl&gt;        &lt;dbl&gt;
## 1     (Intercept) 56.915517472 2.1459109695
## 2      horsepower -0.466390996 0.0343177764
## 3 I(horsepower^2)  0.001231462 0.0001237015</code></pre>
</div>
</div>
<div id="exercise-bootstrap-standard-errors-of-regression-parameters" class="section level2">
<h2>Exercise: bootstrap standard errors of regression parameters</h2>
<ol style="list-style-type: decimal">
<li><p>Estimate 1000 bootstrapped standard errors of a linear regression of the relationship between admission rate and cost in the <a href="stat002_linear_models.html#exercise:_linear_regression_with_scorecard"><code>scorecard</code> dataset</a> and compare them to the original estimates (i.e. the original model with no cross-validation or separate training/test sets).</p>
<details> <summary>Click for the solution</summary>
<p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># original model</span>
scorecard_lm &lt;-<span class="st"> </span><span class="kw">lm</span>(cost <span class="op">~</span><span class="st"> </span>admrate, <span class="dt">data =</span> scorecard)
<span class="kw">tidy</span>(scorecard_lm)</code></pre></div>
<pre><code>##          term   estimate  std.error statistic       p.value
## 1 (Intercept) 43607.1844 1001.38991  43.54666 1.036472e-284
## 2     admrate  -181.6923   14.38405 -12.63151  3.980318e-35</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># bootstrapped model</span>
scorecard_boot &lt;-<span class="st"> </span>scorecard <span class="op">%&gt;%</span>
<span class="st">  </span>modelr<span class="op">::</span><span class="kw">bootstrap</span>(<span class="dv">1000</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">model =</span> <span class="kw">map</span>(strap, <span class="op">~</span><span class="st"> </span><span class="kw">lm</span>(cost <span class="op">~</span><span class="st"> </span>admrate, <span class="dt">data =</span> .)),
         <span class="dt">coef =</span> <span class="kw">map</span>(model, tidy))

scorecard_boot <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">unnest</span>(coef) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">group_by</span>(term) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">summarize</span>(<span class="dt">est.boot =</span> <span class="kw">mean</span>(estimate),
            <span class="dt">se.boot =</span> <span class="kw">sd</span>(estimate, <span class="dt">na.rm =</span> <span class="ot">TRUE</span>))</code></pre></div>
<pre><code>## # A tibble: 2 × 3
##          term   est.boot    se.boot
##         &lt;chr&gt;      &lt;dbl&gt;      &lt;dbl&gt;
## 1 (Intercept) 43593.7338 1234.55370
## 2     admrate  -181.6049   16.57404</code></pre>
</p>
<p></details></p></li>
<li><p>Estimate 1000 bootstrapped standard errors of a <a href="stat003_logistic_regression.html#exercise:_logistic_regression_with_mental_health">logistic regression model of voter turnout</a> using all available predictors and compare them to the original estimates (i.e. the original model with no cross-validation or separate training/test sets).</p>
<details> <summary>Click for the solution</summary>
<p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># basic model</span>
mh_glm &lt;-<span class="st"> </span><span class="kw">glm</span>(vote96 <span class="op">~</span><span class="st"> </span>mhealth, <span class="dt">data =</span> mental_health, <span class="dt">family =</span> binomial)
<span class="kw">tidy</span>(mh_glm)</code></pre></div>
<pre><code>##          term   estimate  std.error statistic      p.value
## 1 (Intercept)  1.2225348 0.08901013 13.734782 6.284476e-43
## 2     mhealth -0.1766612 0.02217096 -7.968135 1.610862e-15</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># bootstrapped model</span>
mh_boot &lt;-<span class="st"> </span>mental_health <span class="op">%&gt;%</span>
<span class="st">  </span>modelr<span class="op">::</span><span class="kw">bootstrap</span>(<span class="dv">1000</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">model =</span> <span class="kw">map</span>(strap, <span class="op">~</span><span class="st"> </span><span class="kw">glm</span>(vote96 <span class="op">~</span><span class="st"> </span>mhealth, <span class="dt">data =</span> .,
                                  <span class="dt">family =</span> binomial)),
         <span class="dt">coef =</span> <span class="kw">map</span>(model, tidy))

mh_boot <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">unnest</span>(coef) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">group_by</span>(term) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">summarize</span>(<span class="dt">est.boot =</span> <span class="kw">mean</span>(estimate),
            <span class="dt">se.boot =</span> <span class="kw">sd</span>(estimate, <span class="dt">na.rm =</span> <span class="ot">TRUE</span>))</code></pre></div>
<pre><code>## # A tibble: 2 × 3
##          term   est.boot    se.boot
##         &lt;chr&gt;      &lt;dbl&gt;      &lt;dbl&gt;
## 1 (Intercept)  1.2254148 0.09107391
## 2     mhealth -0.1773183 0.02257032</code></pre>
</p>
<p></details></p></li>
</ol>
</div>
</div>
<div id="acknowledgements" class="section level1 toc-ignore">
<h1>Acknowledgements</h1>
<ul>
<li>Bootstrap standard error of the mean example derived from <a href="http://t-redactyl.io/blog/2015/09/a-gentle-introduction-to-bootstrapping.html">A gentle introduction to bootstrapping</a>.</li>
<li>“Why use the bootstrap?” reproduced from <a href="http://stats.stackexchange.com/a/26093">Explaining to laypeople why bootstrapping works - Stack Overflow</a>, licensed under the <a href="https://creativecommons.org/licenses/by-sa/3.0/">CC BY-SA 3.0 Creative Commons License</a>.</li>
</ul>
</div>
<div id="session-info" class="section level1 toc-ignore">
<h1>Session Info</h1>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">devtools<span class="op">::</span><span class="kw">session_info</span>()</code></pre></div>
<pre><code>##  setting  value                       
##  version  R version 3.4.1 (2017-06-30)
##  system   x86_64, darwin15.6.0        
##  ui       X11                         
##  language (EN)                        
##  collate  en_US.UTF-8                 
##  tz       America/Chicago             
##  date     2017-08-01                  
## 
##  package      * version    date       source                              
##  assertthat     0.2.0      2017-04-11 CRAN (R 3.4.0)                      
##  backports      1.1.0      2017-05-22 CRAN (R 3.4.0)                      
##  base         * 3.4.1      2017-07-07 local                               
##  base64enc      0.1-3      2015-07-28 CRAN (R 3.4.0)                      
##  bindr          0.1        2016-11-13 CRAN (R 3.4.0)                      
##  bindrcpp     * 0.2        2017-06-17 CRAN (R 3.4.0)                      
##  bit            1.1-12     2014-04-09 CRAN (R 3.4.0)                      
##  bit64          0.9-7      2017-05-08 CRAN (R 3.4.0)                      
##  blob           1.1.0      2017-06-17 CRAN (R 3.4.0)                      
##  boxes          0.0.0.9000 2017-07-19 Github (r-pkgs/boxes@03098dc)       
##  broom        * 0.4.2      2017-02-13 CRAN (R 3.4.0)                      
##  car          * 2.1-5      2017-07-04 CRAN (R 3.4.1)                      
##  caret        * 6.0-76     2017-04-18 CRAN (R 3.4.0)                      
##  cellranger     1.1.0      2016-07-27 CRAN (R 3.4.0)                      
##  class          7.3-14     2015-08-30 CRAN (R 3.4.1)                      
##  clisymbols     1.2.0      2017-05-21 cran (@1.2.0)                       
##  codetools      0.2-15     2016-10-05 CRAN (R 3.4.1)                      
##  coefplot     * 1.2.4      2016-01-10 CRAN (R 3.4.0)                      
##  colorspace     1.3-2      2016-12-14 CRAN (R 3.4.0)                      
##  compiler       3.4.1      2017-07-07 local                               
##  config         0.2        2016-08-02 CRAN (R 3.4.0)                      
##  crayon         1.3.2.9000 2017-07-19 Github (gaborcsardi/crayon@750190f) 
##  data.table     1.10.4     2017-02-01 CRAN (R 3.4.0)                      
##  datasets     * 3.4.1      2017-07-07 local                               
##  DBI            0.7        2017-06-18 CRAN (R 3.4.0)                      
##  dbplyr         1.1.0      2017-06-27 CRAN (R 3.4.1)                      
##  devtools       1.13.2     2017-06-02 CRAN (R 3.4.0)                      
##  digest         0.6.12     2017-01-27 CRAN (R 3.4.0)                      
##  dplyr        * 0.7.2      2017-07-20 CRAN (R 3.4.1)                      
##  e1071        * 1.6-8      2017-02-02 CRAN (R 3.4.0)                      
##  evaluate       0.10.1     2017-06-24 CRAN (R 3.4.1)                      
##  FNN          * 1.1        2013-07-31 CRAN (R 3.4.0)                      
##  forcats      * 0.2.0      2017-01-23 CRAN (R 3.4.0)                      
##  foreach      * 1.4.3      2015-10-13 CRAN (R 3.4.0)                      
##  foreign        0.8-69     2017-06-22 CRAN (R 3.4.1)                      
##  gam          * 1.14-4     2017-04-25 CRAN (R 3.4.0)                      
##  gapminder    * 0.2.0      2015-12-31 CRAN (R 3.4.0)                      
##  gbm          * 2.1.3      2017-03-21 CRAN (R 3.4.0)                      
##  gganimate    * 0.1.0.9000 2017-05-26 Github (dgrtwo/gganimate@bf82002)   
##  ggdendro     * 0.1-20     2016-04-27 CRAN (R 3.4.0)                      
##  ggplot2      * 2.2.1      2016-12-30 CRAN (R 3.4.0)                      
##  glue           1.1.1      2017-06-21 CRAN (R 3.4.1)                      
##  graphics     * 3.4.1      2017-07-07 local                               
##  grDevices    * 3.4.1      2017-07-07 local                               
##  grid         * 3.4.1      2017-07-07 local                               
##  gridExtra    * 2.2.1      2016-02-29 CRAN (R 3.4.0)                      
##  gtable         0.2.0      2016-02-26 CRAN (R 3.4.0)                      
##  haven        * 1.1.0      2017-07-09 CRAN (R 3.4.1)                      
##  highr          0.6        2016-05-09 CRAN (R 3.4.0)                      
##  hms            0.3        2016-11-22 CRAN (R 3.4.0)                      
##  htmltools      0.3.6      2017-04-28 CRAN (R 3.4.0)                      
##  htmlwidgets    0.9        2017-07-10 CRAN (R 3.4.1)                      
##  httpuv         1.3.5      2017-07-04 CRAN (R 3.4.1)                      
##  httr           1.2.1      2016-07-03 CRAN (R 3.4.0)                      
##  igraph         1.1.2      2017-07-21 CRAN (R 3.4.1)                      
##  ISLR         * 1.0        2013-06-11 CRAN (R 3.4.0)                      
##  iterators      1.0.8      2015-10-13 CRAN (R 3.4.0)                      
##  janeaustenr    0.1.5      2017-06-10 CRAN (R 3.4.0)                      
##  jsonlite       1.5        2017-06-01 CRAN (R 3.4.0)                      
##  kknn         * 1.3.1      2016-03-26 CRAN (R 3.4.0)                      
##  knitr        * 1.16       2017-05-18 CRAN (R 3.4.0)                      
##  labeling       0.3        2014-08-23 CRAN (R 3.4.0)                      
##  lattice      * 0.20-35    2017-03-25 CRAN (R 3.4.1)                      
##  lazyeval       0.2.0      2016-06-12 CRAN (R 3.4.0)                      
##  lme4         * 1.1-13     2017-04-19 CRAN (R 3.4.0)                      
##  lmtest       * 0.9-35     2017-02-11 CRAN (R 3.4.0)                      
##  lubridate      1.6.0      2016-09-13 CRAN (R 3.4.0)                      
##  magrittr       1.5        2014-11-22 CRAN (R 3.4.0)                      
##  MASS           7.3-47     2017-02-26 CRAN (R 3.4.1)                      
##  Matrix       * 1.2-10     2017-05-03 CRAN (R 3.4.1)                      
##  MatrixModels   0.4-1      2015-08-22 CRAN (R 3.4.0)                      
##  memoise        1.1.0      2017-04-21 CRAN (R 3.4.0)                      
##  methods      * 3.4.1      2017-07-07 local                               
##  mgcv           1.8-18     2017-07-28 CRAN (R 3.4.1)                      
##  mime           0.5        2016-07-07 CRAN (R 3.4.0)                      
##  minqa          1.2.4      2014-10-09 CRAN (R 3.4.0)                      
##  mnormt         1.5-5      2016-10-15 CRAN (R 3.4.0)                      
##  ModelMetrics   1.1.0      2016-08-26 CRAN (R 3.4.0)                      
##  modelr       * 0.1.1      2017-07-24 CRAN (R 3.4.1)                      
##  modeltools     0.2-21     2013-09-02 CRAN (R 3.4.0)                      
##  munsell        0.4.3      2016-02-13 CRAN (R 3.4.0)                      
##  nlme           3.1-131    2017-02-06 CRAN (R 3.4.1)                      
##  nloptr         1.0.4      2014-08-04 CRAN (R 3.4.0)                      
##  NLP          * 0.1-10     2017-02-21 CRAN (R 3.4.0)                      
##  nnet         * 7.3-12     2016-02-02 CRAN (R 3.4.1)                      
##  nycflights13   0.2.2      2017-01-27 CRAN (R 3.4.0)                      
##  parallel     * 3.4.1      2017-07-07 local                               
##  pbkrtest       0.4-7      2017-03-15 CRAN (R 3.4.0)                      
##  pkgconfig      2.0.1      2017-03-21 CRAN (R 3.4.0)                      
##  plotly       * 4.7.1      2017-07-29 CRAN (R 3.4.1)                      
##  plyr           1.8.4      2016-06-08 CRAN (R 3.4.0)                      
##  pROC         * 1.10.0     2017-06-10 CRAN (R 3.4.0)                      
##  psych          1.7.5      2017-05-03 CRAN (R 3.4.1)                      
##  purrr        * 0.2.2.2    2017-05-11 CRAN (R 3.4.0)                      
##  quantreg       5.33       2017-04-18 CRAN (R 3.4.0)                      
##  R6             2.2.2      2017-06-17 CRAN (R 3.4.0)                      
##  randomForest * 4.6-12     2015-10-07 CRAN (R 3.4.0)                      
##  rappdirs       0.3.1      2016-03-28 CRAN (R 3.4.0)                      
##  rcfss        * 0.1.5      2017-07-31 local                               
##  RColorBrewer * 1.1-2      2014-12-07 CRAN (R 3.4.0)                      
##  Rcpp           0.12.12    2017-07-15 CRAN (R 3.4.1)                      
##  readr        * 1.1.1      2017-05-16 CRAN (R 3.4.0)                      
##  readxl         1.0.0      2017-04-18 CRAN (R 3.4.0)                      
##  reshape2       1.4.2      2016-10-22 CRAN (R 3.4.0)                      
##  rlang          0.1.1      2017-05-18 CRAN (R 3.4.0)                      
##  rmarkdown      1.6        2017-06-15 CRAN (R 3.4.0)                      
##  rprojroot      1.2        2017-01-16 CRAN (R 3.4.0)                      
##  RSQLite      * 2.0        2017-06-19 CRAN (R 3.4.1)                      
##  rstudioapi     0.6        2016-06-27 CRAN (R 3.4.0)                      
##  rvest          0.3.2      2016-06-17 CRAN (R 3.4.0)                      
##  scales         0.4.1      2016-11-09 CRAN (R 3.4.0)                      
##  shiny        * 1.0.3      2017-04-26 CRAN (R 3.4.0)                      
##  slam           0.1-40     2016-12-01 CRAN (R 3.4.0)                      
##  SnowballC      0.5.1      2014-08-09 CRAN (R 3.4.0)                      
##  sparklyr     * 0.6.0      2017-07-29 CRAN (R 3.4.1)                      
##  SparseM        1.77       2017-04-23 CRAN (R 3.4.0)                      
##  splines      * 3.4.1      2017-07-07 local                               
##  stats        * 3.4.1      2017-07-07 local                               
##  stats4         3.4.1      2017-07-07 local                               
##  stringi        1.1.5      2017-04-07 CRAN (R 3.4.0)                      
##  stringr      * 1.2.0      2017-02-18 CRAN (R 3.4.0)                      
##  survival     * 2.41-3     2017-04-04 CRAN (R 3.4.1)                      
##  tibble       * 1.3.3      2017-05-28 CRAN (R 3.4.0)                      
##  tidyr        * 0.6.3      2017-05-15 CRAN (R 3.4.0)                      
##  tidytext     * 0.1.3      2017-06-19 CRAN (R 3.4.1)                      
##  tidyverse    * 1.1.1.9000 2017-07-19 Github (tidyverse/tidyverse@a028619)
##  titanic      * 0.1.0      2015-08-31 CRAN (R 3.4.0)                      
##  tm           * 0.7-1      2017-03-02 CRAN (R 3.4.0)                      
##  tokenizers     0.1.4      2016-08-29 CRAN (R 3.4.0)                      
##  tools          3.4.1      2017-07-07 local                               
##  topicmodels  * 0.2-6      2017-04-18 CRAN (R 3.4.0)                      
##  tree         * 1.0-37     2016-01-21 CRAN (R 3.4.0)                      
##  tweenr       * 0.1.5      2016-10-10 CRAN (R 3.4.0)                      
##  useful         1.2.3      2017-06-07 CRAN (R 3.4.0)                      
##  utils        * 3.4.1      2017-07-07 local                               
##  viridisLite    0.2.0      2017-03-24 CRAN (R 3.4.0)                      
##  withr          2.0.0      2017-07-28 CRAN (R 3.4.1)                      
##  xml2           1.1.1      2017-01-24 CRAN (R 3.4.0)                      
##  xtable         1.8-2      2016-02-05 CRAN (R 3.4.0)                      
##  yaml           2.1.14     2016-11-12 CRAN (R 3.4.0)                      
##  zoo          * 1.8-0      2017-04-12 CRAN (R 3.4.0)</code></pre>
</div>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>The actual value you use is irrelevant. Just be sure to set it in the script, otherwise R will randomly pick one each time you start a new session.<a href="#fnref1">↩</a></p></li>
<li id="fn2"><p>The default <code>family</code> for <code>glm()</code> is <code>gaussian()</code>, or the <strong>Gaussian</strong> distribution. You probably know it by its other name, the <a href="https://en.wikipedia.org/wiki/Normal_distribution"><strong>Normal</strong> distribution</a>.<a href="#fnref2">↩</a></p></li>
<li id="fn3"><p>This function can also be loaded via the <a href="https://github.com/uc-cfss/rcfss"><code>rcfss</code></a> library. Be sure to update your package to the latest version to make sure the function is available.<a href="#fnref3">↩</a></p></li>
<li id="fn4"><p>This function can also be loaded via the <a href="https://github.com/uc-cfss/rcfss"><code>rcfss</code></a> library. Be sure to update your package to the latest version to make sure the function is available.<a href="#fnref4">↩</a></p></li>
<li id="fn5"><p>Exception - <a href="http://www.census.gov/2010census/">the Census</a>.<a href="#fnref5">↩</a></p></li>
<li id="fn6"><p>As defined by the <a href="https://en.wikipedia.org/wiki/Central_limit_theorem">Central Limit Theorem</a>.<a href="#fnref6">↩</a></p></li>
</ol>
</div>

<p>This work is licensed under the  <a href="http://creativecommons.org/licenses/by-nc/4.0/">CC BY-NC 4.0 Creative Commons License</a>.</p>


</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
