<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />


<meta name="author" content="MACS 30100 - Perspectives on Computational Modeling" />


<title>Statistical learning: generalized linear models</title>

<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/readable.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="site_libs/jqueryui-1.11.4/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<script src="site_libs/navigation-1.1/codefolding.js"></script>
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-45631879-2', 'auto');
  ga('send', 'pageview');

</script>


<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
div.sourceCode {
  overflow-x: visible;
}
</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>


<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>


</head>

<body>

<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
.tabbed-pane {
  padding-top: 12px;
}
button.code-folding-btn:focus {
  outline: none;
}
</style>


<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 66px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 71px;
  margin-top: -71px;
}

.section h2 {
  padding-top: 71px;
  margin-top: -71px;
}
.section h3 {
  padding-top: 71px;
  margin-top: -71px;
}
.section h4 {
  padding-top: 71px;
  margin-top: -71px;
}
.section h5 {
  padding-top: 71px;
  margin-top: -71px;
}
.section h6 {
  padding-top: 71px;
  margin-top: -71px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>


<div class="container-fluid main-container">

<!-- tabsets -->
<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});
</script>

<!-- code folding -->
<style type="text/css">
.code-folding-btn { margin-bottom: 4px; }
</style>
<script>
$(document).ready(function () {
  window.initializeCodeFolding("hide" === "show");
});
</script>




<script>
$(document).ready(function ()  {

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_').toLowerCase();
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}


.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
  padding-left: 25px;
  text-indent: 0;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>

<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row-fluid">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">Computing for the Social Sciences</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">Home</a>
</li>
<li>
  <a href="faq.html">FAQ</a>
</li>
<li>
  <a href="syllabus.html">Syllabus</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div class="fluid-row" id="header">

<div class="btn-group pull-right">
<button type="button" class="btn btn-default btn-xs dropdown-toggle" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false"><span>Code</span> <span class="caret"></span></button>
<ul class="dropdown-menu" style="min-width: 50px;">
<li><a id="rmd-show-all-code" href="#">Show All Code</a></li>
<li><a id="rmd-hide-all-code" href="#">Hide All Code</a></li>
</ul>
</div>



<h1 class="title toc-ignore">Statistical learning: generalized linear models</h1>
<h4 class="author"><em>MACS 30100 - Perspectives on Computational Modeling</em></h4>

</div>


<div id="objectives" class="section level1">
<h1>Objectives</h1>
<ul>
<li>Define the generalized linear model (GLM)</li>
<li>Identify the three elements of a GLM
<ul>
<li>Probability distribution</li>
<li>Linear predictor</li>
<li>Link function</li>
</ul></li>
<li>Explain ordinary least squares regression as a GLM</li>
<li>Explain logistic regression as a GLM</li>
<li>Introduce ordinal and multinomial logistic regression for discrete variables with more than two classes</li>
<li>Introduce poisson regression for count data</li>
</ul>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(tidyverse)
<span class="kw">library</span>(modelr)
<span class="kw">library</span>(broom)
<span class="kw">library</span>(haven)
<span class="kw">library</span>(nnet)

<span class="kw">set.seed</span>(<span class="dv">1234</span>)

<span class="kw">theme_set</span>(<span class="kw">theme_minimal</span>())</code></pre></div>
</div>
<div id="generalized-linear-models" class="section level1">
<h1>Generalized linear models</h1>
<p><strong>Generalized linear models</strong> are a flexible class of models that allow us to estimate linear regression for response variables that have error distribution models other than the normal distribution. GLMs are typically estimated via maximum likelihood estimation, though can also be estimated via generalized method of moments as well as Bayesian procedures.</p>
</div>
<div id="elements-of-a-glm" class="section level1">
<h1>Elements of a GLM</h1>
<p>A GLM consists of three components</p>
<ol style="list-style-type: decimal">
<li>A <strong>random component</strong> specifying the conditional distribution of the response variable, <span class="math inline">\(Y_i\)</span>, given the values of the predictor variables in the model. Typically these distributions are a member of the <a href="https://en.wikipedia.org/wiki/Exponential_family"><strong>exponential family</strong></a>, a set of related probability distributions.</li>
<li><p>A <strong>linear predictor</strong> that is a linear function of regressors:</p>
<p><span class="math display">\[\eta_i = \alpha + \beta_{1}X_{i1} + \beta_{2}X_{i2} + \dots + \beta_{k}X_{ik}\]</span></p>
The regressors are prespecified functions of the explanatory variables. This is exactly like the form you’ve seen for <a href="persp003_linear_regression.html">linear</a> and <a href="persp004_logistic_regression.html">logistic</a> regression, because in fact linear and logistic regression are types of GLMs.</li>
<li><p>A <strong>link function</strong> <span class="math inline">\(g(\cdot)\)</span> which transforms the expectation of the response variable, <span class="math inline">\(\mu_i \equiv E(Y_i)\)</span> to the linear predictor:</p>
<p><span class="math display">\[g(\mu_i) = \eta_i = \alpha + \beta_{1}X_{i1} + \beta_{2}X_{i2} + \dots + \beta_{k}X_{ik}\]</span></p>
<p>Because the link function must also be <strong>invertible</strong>, we can also write it as:</p>
<p><span class="math display">\[\mu_i = g^{-1}(\eta_i) = g^{-1}(\alpha + \beta_{1}X_{i1} + \beta_{2}X_{i2} + \dots + \beta_{k}X_{ik})\]</span></p>
<p>The inverted link function is also known as the <strong>mean function</strong>. The purpose of the link function is to relate the linear predictor to the mean of the distribution function.</p></li>
</ol>
<p>For any given probability distribution, there are common link functions called <strong>canonical link functions</strong> that are typically used in conjunction with the probability distribution in GLMs.</p>
</div>
<div id="glms-and-ordinary-least-squares-regression" class="section level1">
<h1>GLMs and ordinary least squares regression</h1>
<p>Previously we have discussed ordinary least squares regression in the context of <strong>minimizing the sum of the squared errors</strong>. This approach has a closed-form solution in the form of linear algebra:</p>
<p><span class="math display">\[\beta = (X^{&#39;}X)^{-1}X^{&#39;}Y\]</span></p>
<p>However we can also treat OLS as a special case of a generalized linear model.</p>
<p>We presume the response variable <span class="math inline">\(Y\)</span> is drawn from a Gaussian (normal) distribution with mean <span class="math inline">\(\mu\)</span> and variance <span class="math inline">\(\sigma^2\)</span>:</p>
<p><span class="math display">\[Pr(Y_i = y_i) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp \left[\frac{(Y_i - \mu)^2}{2\sigma^2}\right]\]</span></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">data_frame</span>(<span class="dt">x =</span> <span class="kw">rnorm</span>(<span class="dv">1000</span>, <span class="dv">0</span>, <span class="dv">1</span>)) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(x)) <span class="op">+</span>
<span class="st">  </span><span class="co"># geom_histogram() +</span>
<span class="st">  </span><span class="kw">stat_function</span>(<span class="dt">fun =</span> dnorm, <span class="dt">args =</span> <span class="kw">list</span>(<span class="dt">mean =</span> <span class="dv">0</span>, <span class="dt">sd =</span> <span class="dv">1</span>)) <span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="st">&quot;Normal distribution&quot;</span>,
       <span class="dt">subtitle =</span> <span class="kw">expression</span>(<span class="kw">paste</span>(mu <span class="op">==</span><span class="st"> </span><span class="dv">0</span>, <span class="st">&quot; , &quot;</span>, sigma<span class="op">^</span>{<span class="dv">2</span>} <span class="op">==</span><span class="st"> </span><span class="dv">1</span>)))</code></pre></div>
<p><img src="persp005_glm_files/figure-html/plot-normal-1.png" width="672" /></p>
<p>This is the density, or probability density function (PDF) of the variable <span class="math inline">\(Y\)</span>.</p>
<ul>
<li>The probability that, for any one observation <span class="math inline">\(i\)</span>, <span class="math inline">\(Y\)</span> will take on the particular value <span class="math inline">\(y\)</span>.</li>
<li>This is a function of <span class="math inline">\(\mu\)</span>, the expected value of the distribution, and <span class="math inline">\(\sigma^2\)</span>, the variability of the distribution around the mean.</li>
</ul>
<p>We want to generate estimates of the parameters <span class="math inline">\(\hat{\mu}\)</span> and <span class="math inline">\(\hat{\sigma^2}\)</span> based on the data. How do we do this? Maximum likelihood estimation of course. We need to find the parameter values that maximize the <strong>log-likelihood function</strong>. For the normal distribution, the log-likelihood function is:</p>
<p><span class="math display">\[\ln L(\hat{\mu}, \hat{\sigma}^2 | Y) = \ln \prod_{i = 1}^{N}{\frac{1}{\sqrt{2\pi\sigma^2}} \exp \left[\frac{(Y_i - \mu)^2}{2\sigma^2}\right]}\]</span></p>
<p>Which can also be written as:</p>
<p><span class="math display">\[\ln L(\hat{\mu}, \hat{\sigma}^2 | Y) = \sum_{i=1}^{N}{\ln\left(\frac{1}{\sqrt{2\pi\sigma^2}} \exp \left[\frac{(Y_i - \mu)^2}{2\sigma^2}\right]\right)}\]</span> <span class="math display">\[\ln L(\hat{\mu}, \hat{\sigma}^2 | Y) = -\frac{N}{2} \ln(2\pi) - \left[ \sum_{i = 1}^{N} \ln{\sigma^2 - \frac{1}{2\sigma^2}} (Y_i - \mu)^2 \right]\]</span></p>
<p>Suppose we had a sample of assistant professor salaries:</p>
<table>
<caption>Salaries of assistant professors</caption>
<thead>
<tr class="header">
<th align="right">id</th>
<th align="right">salary</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">1</td>
<td align="right">60</td>
</tr>
<tr class="even">
<td align="right">2</td>
<td align="right">55</td>
</tr>
<tr class="odd">
<td align="right">3</td>
<td align="right">65</td>
</tr>
<tr class="even">
<td align="right">4</td>
<td align="right">50</td>
</tr>
<tr class="odd">
<td align="right">5</td>
<td align="right">70</td>
</tr>
</tbody>
</table>
<p>If we want to explain the distribution of possible assistant professor salaries given these data points, we could use maximum-likelihood estimation to find the <span class="math inline">\(\hat{\mu}\)</span> that maximizes the likelihood of the data. We are testing different values for <span class="math inline">\(\mu\)</span> to see what optimizes the function. If we have no regressors or predictors, <span class="math inline">\(\hat{\mu}\)</span> is a constant. The log-likelihood curve would look like this:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">likelihood.normal.mu =<span class="st"> </span><span class="cf">function</span>(mu, <span class="dt">sig2 =</span> <span class="dv">1</span>, x) {
  <span class="co"># mu      mean of normal distribution for given sig</span>
  <span class="co"># x       vector of data</span>
  n =<span class="st"> </span><span class="kw">length</span>(x)
  a1 =<span class="st"> </span>(<span class="dv">2</span><span class="op">*</span>pi<span class="op">*</span>sig2)<span class="op">^-</span>(n<span class="op">/</span><span class="dv">2</span>)
  a2 =<span class="st"> </span><span class="op">-</span><span class="dv">1</span><span class="op">/</span>(<span class="dv">2</span><span class="op">*</span>sig2)
  y =<span class="st"> </span>(x<span class="op">-</span>mu)<span class="op">^</span><span class="dv">2</span>
  ans =<span class="st"> </span>a1<span class="op">*</span><span class="kw">exp</span>(a2<span class="op">*</span><span class="kw">sum</span>(y))
  <span class="kw">return</span>(<span class="kw">log</span>(ans))
}

<span class="kw">data_frame</span>(<span class="dt">mu_hat =</span> <span class="kw">seq</span>(<span class="dv">57</span>, <span class="dv">63</span>, <span class="dt">by =</span> .<span class="dv">05</span>),
           <span class="dt">logLik =</span> <span class="kw">map_dbl</span>(mu_hat, likelihood.normal.mu, <span class="dt">x =</span> prof<span class="op">$</span>salary)) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(mu_hat, logLik)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_line</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">x =</span> <span class="kw">expression</span>(<span class="kw">hat</span>(mu)),
       <span class="dt">y =</span> <span class="st">&quot;Log-likelihood&quot;</span>)</code></pre></div>
<p><img src="persp005_glm_files/figure-html/unnamed-chunk-1-1.png" width="672" /></p>
<p>And the maximum is 60, which is the mean of the 5 sample observations.</p>
<p>But more typically we want <span class="math inline">\(\mu\)</span> to be a function of some other variable <span class="math inline">\(X\)</span>. We can write this as:</p>
<p><span class="math display">\[E(Y) \equiv \mu = \beta_0 + \beta_{1}X_{i}\]</span></p>
<p><span class="math display">\[\mathrm{Var}(Y) = \sigma^2\]</span></p>
<p>Now we just substitute this equation for the systematic mean part (<span class="math inline">\(\mu\)</span>) in the previous equations:</p>
<p><span class="math display">\[\ln L(\beta_0, \beta_1, \sigma^2 | Y) = \ln \prod_{i = 1}^{N}{\frac{1}{\sqrt{2\pi\sigma^2}} \exp \left[\frac{(Y_i - \beta_0 - \beta_{1}X_{i})^2}{2\sigma^2}\right]}\]</span> <span class="math display">\[\ln L(\beta_0, \beta_1, \sigma^2 | Y) = -\frac{N}{2} \ln(2\pi) - \left[ \sum_{i = 1}^{N} \ln{\sigma^2 - \frac{1}{2\sigma^2}} (Y_i - \beta_0 - \beta_{1}X_{i})^2 \right]\]</span></p>
<div id="connecting-mle-estimator-to-ols" class="section level2">
<h2>Connecting MLE estimator to OLS</h2>
<p>With respect to the parameters <span class="math inline">\(\{\beta_0, \beta_1, \sigma^2\}\)</span>, only the last term is important. The first one (<span class="math inline">\(-\frac{N}{2} \ln(2\pi)\)</span>) is invariant with respect to the parameters of interest, and so it can be dropped using the <em>Fisher-Neyman Factorization Lemma</em>. Thus the <strong>kernal</strong> of the log-likelihood is:</p>
<p><span class="math display">\[-\sum_{i = 1}^{N} \ln{\sigma^2 - \frac{1}{2\sigma^2}} (Y_i - \beta_0 - \beta_{1}X_{i})^2\]</span></p>
<p>Which is eeriely familiar to the sum-of-squared-errors term, merely scaled by the variance parameter <span class="math inline">\(\sigma^2\)</span>:</p>
<p><span class="math display">\[RSS = \sum_{i = 1}^{N} (Y_i - \beta_0 - \beta_{1}X_{i})^2\]</span></p>
<p>This proves the least-squares estimator of OLS <span class="math inline">\(\beta\)</span>s <strong>is the maximum likelihood estimator as well</strong>.</p>
</div>
<div id="completing-the-elements" class="section level2">
<h2>Completing the elements</h2>
<p>So far we have the random component (a normal distribution) and a linear predictor (<span class="math inline">\(\beta_0 + \beta_{1}X_{i}\)</span>). Where is the link function? We actually have it already too. Because the normal distribution already supports an infinite range of real numbers <span class="math inline">\((-, +\infty)\)</span>, the data naturally scaled to the linear predictor. What we use here is an <strong>identity link function</strong> that returns its argument unaltered:</p>
<p><span class="math display">\[\eta_i = g(\mu_i) = \mu_i\]</span> <span class="math display">\[\mu_i = g^{-1}(\eta_i) = \eta_i\]</span></p>
<p>So to recap, in linear regression:</p>
<ol style="list-style-type: decimal">
<li><p>The random component is the normal distribution:</p>
<p><span class="math display">\[Pr(Y_i = y_i) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp \left[\frac{(Y_i - \mu)^2}{2\sigma^2}\right]\]</span></p></li>
<li><p>The linear predictor is:</p>
<p><span class="math display">\[\eta_{i} = \beta_0 + \beta_{1}X_{i}\]</span></p></li>
<li><p>The link function is the identity function:</p>
<p><span class="math display">\[\eta_i = \mu_i\]</span></p></li>
</ol>
<p>Therefore with ordinary least squares linear regression, we have now demonstrated that it is merely one type of generalized linear model.</p>
</div>
</div>
<div id="glms-and-logistic-regression" class="section level1">
<h1>GLMs and logistic regression</h1>
<p>The normal distribution does not properly describe binary outcome variables which take on values of <span class="math inline">\([0,1]\)</span>, because the normal distribution allows for values along the entire real number line. This was the problem last class in trying to apply OLS to the Titanic dataset.</p>
<p>Instead, we assume our outcome variable <span class="math inline">\(Y\)</span> is drawn from the <strong>binomial distribution</strong> with probability <span class="math inline">\(\pi\)</span>:</p>
<p><span class="math display">\[Pr(Y_i = y_i | \pi_i) = \pi_i^{y_i} (1 - \pi_i)^{(1 - y_i)}\]</span></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">data_frame</span>(<span class="dt">x =</span> <span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">1</span>),
           <span class="dt">y =</span> <span class="kw">c</span>(.<span class="dv">5</span>, .<span class="dv">5</span>)) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(x, y)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_col</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="st">&quot;Binomial distribution&quot;</span>,
       <span class="dt">subtitle =</span> <span class="kw">expression</span>(pi <span class="op">==</span><span class="st"> </span>.<span class="dv">5</span>),
       <span class="dt">x =</span> <span class="st">&quot;X&quot;</span>,
       <span class="dt">y =</span> <span class="st">&quot;Pr(X)&quot;</span>)</code></pre></div>
<p><img src="persp005_glm_files/figure-html/binom-plot-1.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">data_frame</span>(<span class="dt">x =</span> <span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">1</span>),
           <span class="dt">y =</span> <span class="kw">c</span>(.<span class="dv">2</span>, .<span class="dv">8</span>)) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(x, y)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_col</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="st">&quot;Binomial distribution&quot;</span>,
       <span class="dt">subtitle =</span> <span class="kw">expression</span>(pi <span class="op">==</span><span class="st"> </span>.<span class="dv">8</span>),
       <span class="dt">x =</span> <span class="st">&quot;X&quot;</span>,
       <span class="dt">y =</span> <span class="st">&quot;Pr(X)&quot;</span>)</code></pre></div>
<p><img src="persp005_glm_files/figure-html/binom-plot-2.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">data_frame</span>(<span class="dt">x =</span> <span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">1</span>),
           <span class="dt">y =</span> <span class="kw">c</span>(.<span class="dv">8</span>, .<span class="dv">2</span>)) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(x, y)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_col</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="st">&quot;Binomial distribution&quot;</span>,
       <span class="dt">subtitle =</span> <span class="kw">expression</span>(pi <span class="op">==</span><span class="st"> </span>.<span class="dv">2</span>),
       <span class="dt">x =</span> <span class="st">&quot;X&quot;</span>,
       <span class="dt">y =</span> <span class="st">&quot;Pr(X)&quot;</span>)</code></pre></div>
<p><img src="persp005_glm_files/figure-html/binom-plot-3.png" width="672" /></p>
<p>This is a probability mass function (PMF):</p>
<ul>
<li>The probability that, for any one observation <span class="math inline">\(i\)</span>, <span class="math inline">\(Y\)</span> will take on the particular value <span class="math inline">\(y\)</span>.</li>
<li><span class="math inline">\(Y_i\)</span> takes on the expected value of 1 with probability <span class="math inline">\(\pi\)</span> and 0 with probability <span class="math inline">\(1 - \pi\)</span>., so <span class="math inline">\(\pi_i\)</span> is the conditional probability of sampling a 1 in this group.</li>
</ul>
<p>Of course, the probability of the outcome <span class="math inline">\(Y\)</span> may vary systematically given known predictors. To incorporate that into the model, we need to choose a linear predictor:</p>
<p><span class="math display">\[\pi_i = \eta_i\]</span></p>
<p>Now in the linear context we would write something like this:</p>
<p><span class="math display">\[g(\pi_i) \equiv \eta_i = \beta_0 + \beta_{1}X_i\]</span></p>
<p>However remember the problem with that approach using the Titanic data. We need to constrain the linear predictor to the probability range <span class="math inline">\([0,1]\)</span>. So in logistic regression, we cannot use the identity link function. Instead, we use the <strong>logit link function</strong> to constrain the linear predictor onto the <span class="math inline">\([0,1]\)</span> range:</p>
<p><span class="math display">\[g(\pi_i) = \frac{e^{\eta_i}}{1 + e^{\eta_i}}\]</span></p>
<p>So to recap, for logistic regression:</p>
<ul>
<li><p>The random component is the Bernoulli distribution</p>
<p><span class="math display">\[Pr(Y_i = y_i | \pi) = \pi_i^{y_i} (1 - \pi_i)^{(1 - y_i)}\]</span></p></li>
<li><p>The linear predictor is:</p>
<p><span class="math display">\[\eta_i = \beta_0 + \beta_{1}X_i\]</span></p></li>
<li><p>The link function is the logit function:</p>
<p><span class="math display">\[\eta_i = \ln \left( \frac{\pi}{1 - \pi} \right)\]</span></p></li>
</ul>
<p>So the likelihood for a given observation <span class="math inline">\(i\)</span> is:</p>
<p><span class="math display">\[L_i = \left( \frac{e^{\eta_i}}{1 + e^{\eta_i}} \right) ^ {Y_i} \left[ 1 - \left( \frac{e^{\eta_i}}{1 + e^{\eta_i}} \right) \right]^{1 - Y_i}\]</span></p>
<p><span class="math display">\[L_i = \left( \frac{e^{\beta_0 + \beta_{1}X_i}}{1 + e^{\beta_0 + \beta_{1}X_i}} \right) ^ {Y_i} \left[ 1 - \left( \frac{e^{\beta_0 + \beta_{1}X_i}}{1 + e^{\beta_0 + \beta_{1}X_i}} \right) \right]^{1 - Y_i}\]</span></p>
<p>The product of which yields the likelihood function:</p>
<p><span class="math display">\[L = \prod_{i = 1}^{N} \left( \frac{e^{\beta_0 + \beta_{1}X_i}}{1 + e^{\beta_0 + \beta_{1}X_i}} \right) ^ {Y_i} \left[ 1 - \left( \frac{e^{\beta_0 + \beta_{1}X_i}}{1 + e^{\beta_0 + \beta_{1}X_i}} \right) \right]^{1 - Y_i}\]</span></p>
<p>And therefore the log-likelihood function:</p>
<p><span class="math display">\[\ln L = \sum_{i = 1}^{N} Y_i \ln \left( \frac{e^{\beta_0 + \beta_{1}X_i}}{1 + e^{\beta_0 + \beta_{1}X_i}} \right) + (1 - Y_i) \ln \left[ 1 - \left( \frac{e^{\beta_0 + \beta_{1}X_i}}{1 + e^{\beta_0 + \beta_{1}X_i}} \right) \right]\]</span></p>
<p>Which we maximize with respect to the <span class="math inline">\(\beta\)</span>s to obtain our estimated parameters, just as we would linear regression.</p>
</div>
<div id="logistic-regression-for-more-than-two-response-classes" class="section level1">
<h1>Logistic regression for more than two response classes</h1>
<p>A downside to logistic regression as presented above is that it only works for response variables with two potential values. However it is common to encounter discrete variables with more than two possible values. Consider voting in a legislature. Roll-call votes are typically recorded as:</p>
<ol style="list-style-type: decimal">
<li>Yay</li>
<li>Nay</li>
</ol>
<p>However there are other potential outcomes for legislators. We could instead code their votes as:</p>
<ol style="list-style-type: decimal">
<li>Yay</li>
<li>Nay</li>
<li>Abstain</li>
<li>Not present</li>
</ol>
<p>If a large number of legislators abstain or are missing from a vote, we may not want to throw that information away by arbitrarily restricting the outcome to two possibilities.</p>
<div id="multinomial-logistic-regression" class="section level2">
<h2>Multinomial logistic regression</h2>
<p>We can generalize the dichotomous logit model to a <strong>polytomy</strong> (more than 2 outcomes) bby employing the <strong>multivariate logistic distribution</strong>. Here the response variable <span class="math inline">\(Y\)</span> can take on any of <span class="math inline">\(m\)</span> discrete values from <span class="math inline">\(1,2,\dots,m\)</span>. We presume the ordering of the values does not matter, so that <span class="math inline">\(\pi_{ij}\)</span> denotes the probability that the <span class="math inline">\(i\)</span>th observation falls into the <span class="math inline">\(j\)</span>th category of the response variable:</p>
<p><span class="math display">\[\pi_{ij} \equiv \text{Pr}(Y_i = j), \text{for} j = 1, \dots, m\]</span></p>
<p>For a model with <span class="math inline">\(k\)</span> regressors, <span class="math inline">\(X_1, \dots, X_k\)</span>, this dependence of <span class="math inline">\(\pi_{ij}\)</span> can be modeled using the multivariate logistic distribution:</p>
<p><span class="math display">\[\pi_{ij} = \frac{\exp[\gamma_{0j} + \gamma_{1j}X_{i1} + \dots + \gamma_{kj}X_{ik}]}{1 + \sum_{l = 1}^{m-1} \exp[\gamma_{0l} + \gamma_{1l}X_{i1} + \dots + \gamma_{kl}X_{ik}]}, \text{for } j = 1, \dots, m-1\]</span></p>
<p><span class="math display">\[\pi_{im} = 1 - \sum_{i = 1}^{m-1} \pi_{ij}\]</span></p>
<p>There is one set of parameters <span class="math inline">\(\gamma_{0j}, \gamma_{1j}, \dots, \gamma_{kj}\)</span> for each set of response values but the last one. The last category (category <span class="math inline">\(m\)</span>) serves as the baseline category. The response category probabilities for each observation must sum to 1:</p>
<p><span class="math display">\[\sum_{j = 1}^{m} = 1\]</span></p>
<p>With algebraic manipulation, we can rewrite the probability function as:</p>
<p><span class="math display">\[\ln \frac{\pi_{ij}}{\pi_{im}} = \gamma_{0j} + \gamma_{1j}X_{i1} + \dots + \gamma_{kj}X_{ik}, \text{for } j = 1, \dots, m\]</span></p>
<p>So the estimated parameters tell us the log-odds of membership in any category <span class="math inline">\(j\)</span> versus the baseline category. We could rewrite this equation to compare the log-odds of membership in any two categories. This also demonstrates that the dichotomous logit model is a special case of the multinominal logit model:</p>
<p><span class="math display">\[\ln \frac{\pi_{i1}}{\pi_{i2}} = \ln \frac{\pi_{i1}}{1 - \pi_{i1}} = \gamma_{01} + \gamma_{11}X_{i1} + \dots + \gamma_{k1}X_{ik}\]</span></p>
<div id="applying-multinomial-logisitic-regression" class="section level3">
<h3>Applying multinomial logisitic regression</h3>
<p>Let’s return to our Titanic dataset. Recall the dataset and codebook:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(titanic)
titanic &lt;-<span class="st"> </span>titanic_train <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">as_tibble</span>() <span class="op">%&gt;%</span>
<span class="st">  </span><span class="co"># remove missing values</span>
<span class="st">  </span><span class="kw">filter</span>(Embarked <span class="op">!=</span><span class="st"> &quot;&quot;</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">na.omit</span>()

titanic <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">head</span>() <span class="op">%&gt;%</span>
<span class="st">  </span>knitr<span class="op">::</span><span class="kw">kable</span>()</code></pre></div>
<table>
<thead>
<tr class="header">
<th align="right">PassengerId</th>
<th align="right">Survived</th>
<th align="right">Pclass</th>
<th align="left">Name</th>
<th align="left">Sex</th>
<th align="right">Age</th>
<th align="right">SibSp</th>
<th align="right">Parch</th>
<th align="left">Ticket</th>
<th align="right">Fare</th>
<th align="left">Cabin</th>
<th align="left">Embarked</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">1</td>
<td align="right">0</td>
<td align="right">3</td>
<td align="left">Braund, Mr. Owen Harris</td>
<td align="left">male</td>
<td align="right">22</td>
<td align="right">1</td>
<td align="right">0</td>
<td align="left">A/5 21171</td>
<td align="right">7.2500</td>
<td align="left"></td>
<td align="left">S</td>
</tr>
<tr class="even">
<td align="right">2</td>
<td align="right">1</td>
<td align="right">1</td>
<td align="left">Cumings, Mrs. John Bradley (Florence Briggs Thayer)</td>
<td align="left">female</td>
<td align="right">38</td>
<td align="right">1</td>
<td align="right">0</td>
<td align="left">PC 17599</td>
<td align="right">71.2833</td>
<td align="left">C85</td>
<td align="left">C</td>
</tr>
<tr class="odd">
<td align="right">3</td>
<td align="right">1</td>
<td align="right">3</td>
<td align="left">Heikkinen, Miss. Laina</td>
<td align="left">female</td>
<td align="right">26</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="left">STON/O2. 3101282</td>
<td align="right">7.9250</td>
<td align="left"></td>
<td align="left">S</td>
</tr>
<tr class="even">
<td align="right">4</td>
<td align="right">1</td>
<td align="right">1</td>
<td align="left">Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>
<td align="left">female</td>
<td align="right">35</td>
<td align="right">1</td>
<td align="right">0</td>
<td align="left">113803</td>
<td align="right">53.1000</td>
<td align="left">C123</td>
<td align="left">S</td>
</tr>
<tr class="odd">
<td align="right">5</td>
<td align="right">0</td>
<td align="right">3</td>
<td align="left">Allen, Mr. William Henry</td>
<td align="left">male</td>
<td align="right">35</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="left">373450</td>
<td align="right">8.0500</td>
<td align="left"></td>
<td align="left">S</td>
</tr>
<tr class="even">
<td align="right">7</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="left">McCarthy, Mr. Timothy J</td>
<td align="left">male</td>
<td align="right">54</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="left">17463</td>
<td align="right">51.8625</td>
<td align="left">E46</td>
<td align="left">S</td>
</tr>
</tbody>
</table>
<p>The codebook contains the following information on the variables:</p>
<pre><code>VARIABLE DESCRIPTIONS:
Survived        Survival
                (0 = No; 1 = Yes)
Pclass          Passenger Class
                (1 = 1st; 2 = 2nd; 3 = 3rd)
Name            Name
Sex             Sex
Age             Age
SibSp           Number of Siblings/Spouses Aboard
Parch           Number of Parents/Children Aboard
Ticket          Ticket Number
Fare            Passenger Fare
Cabin           Cabin
Embarked        Port of Embarkation
                (C = Cherbourg; Q = Queenstown; S = Southampton)

SPECIAL NOTES:
Pclass is a proxy for socio-economic status (SES)
 1st ~ Upper; 2nd ~ Middle; 3rd ~ Lower

Age is in Years; Fractional if Age less than One (1)
 If the Age is Estimated, it is in the form xx.5

With respect to the family relation variables (i.e. sibsp and parch)
some relations were ignored.  The following are the definitions used
for sibsp and parch.

Sibling:  Brother, Sister, Stepbrother, or Stepsister of Passenger Aboard Titanic
Spouse:   Husband or Wife of Passenger Aboard Titanic (Mistresses and Fiances Ignored)
Parent:   Mother or Father of Passenger Aboard Titanic
Child:    Son, Daughter, Stepson, or Stepdaughter of Passenger Aboard Titanic

Other family relatives excluded from this study include cousins,
nephews/nieces, aunts/uncles, and in-laws.  Some children travelled
only with a nanny, therefore parch=0 for them.  As well, some
travelled with very close friends or neighbors in a village, however,
the definitions do not support such relations.</code></pre>
<p>But this time instead of modeling survival we want to model port of embarkation as a function of passenger fare and age. First we set “Southampton” as our reference category, then we can use the <code>multinom()</code> function from the <code>nnet</code> library to do this.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">titanic_multi &lt;-<span class="st"> </span>titanic <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">Embarked =</span> <span class="kw">relevel</span>(<span class="kw">factor</span>(Embarked), <span class="dt">ref =</span> <span class="st">&quot;S&quot;</span>))
port_mod &lt;-<span class="st"> </span><span class="kw">multinom</span>(Embarked <span class="op">~</span><span class="st"> </span>Fare, <span class="dt">data =</span> titanic_multi)</code></pre></div>
<pre><code>## # weights:  9 (4 variable)
## initial  value 782.211950 
## iter  10 value 423.965155
## final  value 422.773331 
## converged</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">summary</span>(port_mod)</code></pre></div>
<pre><code>## Call:
## multinom(formula = Embarked ~ Fare, data = titanic_multi)
## 
## Coefficients:
##   (Intercept)        Fare
## C   -1.961393  0.01255073
## Q   -2.635604 -0.01593038
## 
## Std. Errors:
##   (Intercept)        Fare
## C   0.1327167 0.002020486
## Q   0.2783763 0.010823741
## 
## Residual Deviance: 845.5467 
## AIC: 853.5467</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># calculate t-statistics</span>
(port_mod_t &lt;-<span class="st"> </span><span class="kw">summary</span>(port_mod)<span class="op">$</span>coefficients <span class="op">/</span><span class="st"> </span><span class="kw">summary</span>(port_mod)<span class="op">$</span>standard.errors)</code></pre></div>
<pre><code>##   (Intercept)      Fare
## C  -14.778790  6.211738
## Q   -9.467776 -1.471800</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># calculate p-values</span>
(port_mod_p &lt;-<span class="st"> </span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span><span class="kw">pnorm</span>(<span class="kw">abs</span>(port_mod_t), <span class="dv">0</span>, <span class="dv">1</span>))<span class="op">*</span><span class="dv">2</span>)</code></pre></div>
<pre><code>##   (Intercept)         Fare
## C           0 5.240164e-10
## Q           0 1.410749e-01</code></pre>
<p>As expected, we’ve estimated separate parameters and standard errors for each of our categories, Cherbourg and Queenstown, compared against our reference category Southampton. Each of these blocks has one row of values corresponding to a separate category. Focusing on the block of coefficients, we can look at the first row comparing <code>Embarked = &quot;C&quot;</code> to our baseline <code>Embarked = &quot;Q&quot;</code> and the second row comparing <code>Embarked = &quot;Q&quot;</code> to our baseline <code>Embarked = &quot;Q&quot;</code>.</p>
<ul>
<li>A one-unit increase in <code>Fare</code> is associated with a 0.013 increase in the log-odds of embarking from Cherbourg as opposed to Southampton.</li>
<li>A one-unit increase in <code>Fare</code> is associated with a 0.013 decrease in the log-odds of embarking from Cherbourg as opposed to Southampton.</li>
</ul>
<p>We can convert the log-odds parameters to <strong>relative risks</strong>, also referred to as odds, by exponentiating the parameters:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">(port_mod_exp &lt;-<span class="st"> </span><span class="kw">exp</span>(<span class="kw">coef</span>(port_mod)))</code></pre></div>
<pre><code>##   (Intercept)      Fare
## C  0.14066240 1.0126298
## Q  0.07167565 0.9841958</code></pre>
<ul>
<li>The relative risk ratio for a one-unit increase in fare is 1.013 for embarking from Cherbourg vs. Southampton.</li>
<li>The relative risk ratio for a one-unit increase in fare is 0.984 for embarking from Queenstown vs. Southampton.</li>
</ul>
<p>We can also calculate predicted probabilities of each outcome given known values for fare.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">dat_grid &lt;-<span class="st"> </span>titanic <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">data_grid</span>(Fare)

dat_pred &lt;-<span class="st"> </span><span class="kw">bind_cols</span>(dat_grid,
                      <span class="kw">predict</span>(port_mod, dat_grid, <span class="dt">type =</span> <span class="st">&quot;probs&quot;</span>) <span class="op">%&gt;%</span>
<span class="st">                        </span><span class="kw">as_tibble</span>()) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">gather</span>(port, prob, S<span class="op">:</span>Q) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">port =</span> <span class="kw">factor</span>(port, <span class="dt">levels =</span> <span class="kw">c</span>(<span class="st">&quot;C&quot;</span>, <span class="st">&quot;S&quot;</span>, <span class="st">&quot;Q&quot;</span>),
                       <span class="dt">labels =</span> <span class="kw">c</span>(<span class="st">&quot;Cherbourg&quot;</span>, <span class="st">&quot;Southampton&quot;</span>, <span class="st">&quot;Queenstown&quot;</span>)))

<span class="kw">ggplot</span>(dat_pred, <span class="kw">aes</span>(Fare, prob, <span class="dt">color =</span> port)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_line</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">x =</span> <span class="st">&quot;Fare&quot;</span>,
       <span class="dt">y =</span> <span class="st">&quot;Predicted probability for port of embarkation&quot;</span>)</code></pre></div>
<p><img src="persp005_glm_files/figure-html/titanic-multi-pred-1.png" width="672" /></p>
<p>As fare increases, the probability of embarking from Southampton decreases dramatically and increases for Cherbourg. Overall the probability of departing from Queenstown is low. Not surprising given that overall that had the lowest frequency of departure.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">titanic <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(Embarked)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_bar</span>()</code></pre></div>
<p><img src="persp005_glm_files/figure-html/titanic-port-hist-1.png" width="672" /></p>
<p>We could also look at the cumulative probability distribution:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>(dat_pred, <span class="kw">aes</span>(Fare, prob, <span class="dt">fill =</span> port)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_area</span>(<span class="dt">position =</span> <span class="st">&quot;stack&quot;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">x =</span> <span class="st">&quot;Fare&quot;</span>,
       <span class="dt">y =</span> <span class="st">&quot;Predicted probability for port of embarkation&quot;</span>)</code></pre></div>
<p><img src="persp005_glm_files/figure-html/titanic-multi-cum-1.png" width="672" /></p>
<p>This demonstrates that for any given fare value, the probabilities of all the possible outcomes must add up to 1.</p>
</div>
</div>
<div id="ordinal-logistic-regression" class="section level2">
<h2>Ordinal logistic regression</h2>
<p>Consider the case where your response variable contains more than 2 possible values and they are inherently ordered. Survey research typically contains answers measured using a Likert scale (strongly agree, agree, neither agree nor disagree, disagree, strongly disagree). What method should you use? Linear regression is not appropriate because the variable is discrete. Depending on the number of levels, some researchers may attempt to treat it as a continuous variable but when you only have 3-5 values this doesn’t work very well. Logistic regression sounds like it would work but so far we have only used it in situations where there are two possible outcomes. Can we generalize it to a larger number of response values?</p>
<p>Yes we can. This is called <strong>ordinal logistic regression</strong>. It works in situations where you have more than two possible outcomes and they contain an inherent ordering. It is similar to multinominal logistic regression but preserves the ordering of the variables. It presumes that the response variable is in fact a <strong>latent continuous variable</strong> that is a linear function of the <span class="math inline">\(X\)</span>s plus a random error:</p>
<p><span class="math display">\[\zeta_i = \alpha + \beta_{1}X_{i1} + \dots + \beta_{k}X_{ik} + \epsilon_i\]</span></p>
<p>Rather than dividing the latent variable into two regions (as we would with a dichotomous variable), instead it is divided into <span class="math inline">\(m\)</span> regions split by <span class="math inline">\(m-1\)</span> <strong>thresholds</strong>. The thresholds are denoted by <span class="math inline">\(\alpha_k\)</span>, and we only observe the resulting response <span class="math inline">\(Y\)</span>:</p>
<p><span class="math display">\[Y_i = \begin{cases} 
      1 &amp; \text{if } \zeta_{i} \leq \alpha_{1} \\
      2 &amp; \text{if } \alpha_{1} \lt \zeta_{i} \leq \alpha_{2} \\
      \vdots &amp; \\
      m - 1 &amp; \text{if } \alpha_{m - 2} \lt \zeta_{i} \leq \alpha_{m - 1} \\
      m &amp; \text{if } \alpha_{m - 1} \lt \zeta_{i} \\
   \end{cases}\]</span></p>
<p>Skipping some of the math, the result is an ordered logistic regression model. The parameters estimate for every one-unit increase in <span class="math inline">\(X\)</span> the log-odds of <span class="math inline">\(Y_i\)</span> increasing to the next level. If we exponentiate the parameters, we get the <strong>odds ratio</strong>. The important thing is that the change in log-odds between any given levels is <strong>constant</strong> - that is, a one-unit change in <span class="math inline">\(X\)</span> equates to the same magnitude of change between outcomes <span class="math inline">\(0\)</span> and <span class="math inline">\(1\)</span>, <span class="math inline">\(1\)</span> and <span class="math inline">\(2\)</span>, etc. The only difference is the intercept for each value.</p>
<p>Let’s see how this works by predicting passenger class on the Titanic. We can use the <code>polr</code> function from the <code>MASS</code> library to estimate the ordered logistic regression model:<a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">titanic_ord &lt;-<span class="st"> </span>titanic <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">Pclass =</span> <span class="kw">factor</span>(Pclass))

m &lt;-<span class="st"> </span>MASS<span class="op">::</span><span class="kw">polr</span>(Pclass <span class="op">~</span><span class="st"> </span>Fare, <span class="dt">data =</span> titanic_ord, <span class="dt">Hess=</span><span class="ot">TRUE</span>)
<span class="kw">summary</span>(m)</code></pre></div>
<pre><code>## Call:
## MASS::polr(formula = Pclass ~ Fare, data = titanic_ord, Hess = TRUE)
## 
## Coefficients:
##         Value Std. Error t value
## Fare -0.09585   0.006819  -14.06
## 
## Intercepts:
##     Value    Std. Error t value 
## 1|2  -4.0640   0.2245   -18.1020
## 2|3  -2.0243   0.1527   -13.2566
## 
## Residual Deviance: 970.3814 
## AIC: 976.3814</code></pre>
<p>The model results give use estimated parameters and standard errors. However now we have two separate intercepts. These are the <strong>cut points</strong> used to determine the estimated thresholds <span class="math inline">\(\hat{\alpha_{k}}\)</span> for the underlying latent variable. They are typically of little direct concern in our analysis. Instead, we care about the coefficients.</p>
<ul>
<li>For every one-pound increase in fare, the log-odds of a passenger’s berthing class change by -0.096. That is, the log-odds of berthing in second class are 0.096 lower than berthing in first class. The same for berthing in third class compared to second class.</li>
</ul>
<p>We can convert the log-odds to an odds ratio by exponentiating the estimated parameter.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">exp</span>(<span class="kw">coef</span>(m))</code></pre></div>
<pre><code>##      Fare 
## 0.9086044</code></pre>
<p>That is, the odds of berthing in second class are 0.909 the size of berthing in first class.</p>
<p>We can predict log-odds, odds, and predicted probabilities of each outcome.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">dat_grid &lt;-<span class="st"> </span>titanic_ord <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">data_grid</span>(Fare)

dat_pred &lt;-<span class="st"> </span><span class="kw">bind_cols</span>(dat_grid,
                      <span class="kw">predict</span>(m, dat_grid, <span class="dt">type =</span> <span class="st">&quot;probs&quot;</span>) <span class="op">%&gt;%</span>
<span class="st">                        </span><span class="kw">as_tibble</span>()) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">gather</span>(outcome, prob, <span class="st">`</span><span class="dt">1</span><span class="st">`</span><span class="op">:</span><span class="st">`</span><span class="dt">3</span><span class="st">`</span>)

<span class="kw">ggplot</span>(dat_pred, <span class="kw">aes</span>(Fare, prob, <span class="dt">color =</span> outcome)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_line</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">x =</span> <span class="st">&quot;Fare amount&quot;</span>,
       <span class="dt">y =</span> <span class="st">&quot;Predicted probability of passenger class&quot;</span>,
       <span class="dt">color =</span> <span class="st">&quot;Passenger class&quot;</span>)</code></pre></div>
<p><img src="persp005_glm_files/figure-html/unnamed-chunk-2-1.png" width="672" /></p>
<p>Unsurprisingly, cheap fares have a high probability of 3rd class berthing, whereas expensive fairs are almost exclusively 1st class cabins.</p>
</div>
</div>
<div id="poisson-regression" class="section level1">
<h1>Poisson regression</h1>
<p>The <strong>Poisson distribution</strong> is a discrete probability function defined by:</p>
<p><span class="math display">\[P(Y_i = y_i | \mu) = \frac{\mu^{y_i} e^{-\mu}}{y_i!}\]</span></p>
<p>where <span class="math inline">\(\mu\)</span> is the event rate (average number of events per interval), <span class="math inline">\(e\)</span> is Euler’s number, <span class="math inline">\(y_i\)</span> is an integer with range <span class="math inline">\([0, \infty]\)</span>, and <span class="math inline">\(y_i!\)</span> is the factorial of <span class="math inline">\(y_i\)</span>. The mean <span class="math inline">\(\mu\)</span> and variance <span class="math inline">\(\sigma\)</span> of a Poisson distribution are the same parameterm and hence are both defined by <span class="math inline">\(\mu\)</span>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">data_frame</span>(<span class="dt">x =</span> <span class="kw">rpois</span>(<span class="dv">10000</span>, <span class="dv">1</span>)) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(x)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_bar</span>(<span class="kw">aes</span>(<span class="dt">y =</span> (..count..)<span class="op">/</span><span class="kw">sum</span>(..count..))) <span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="st">&quot;Poisson distribution&quot;</span>,
     <span class="dt">subtitle =</span> <span class="kw">expression</span>(mu <span class="op">==</span><span class="st"> </span><span class="dv">1</span>),
       <span class="dt">x =</span> <span class="st">&quot;k&quot;</span>,
       <span class="dt">y =</span> <span class="st">&quot;Pr(X = k)&quot;</span>)</code></pre></div>
<p><img src="persp005_glm_files/figure-html/plot-poisson-1.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">data_frame</span>(<span class="dt">x =</span> <span class="kw">rpois</span>(<span class="dv">10000</span>, <span class="dv">4</span>)) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(x)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_bar</span>(<span class="kw">aes</span>(<span class="dt">y =</span> (..count..)<span class="op">/</span><span class="kw">sum</span>(..count..))) <span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="st">&quot;Poisson distribution&quot;</span>,
       <span class="dt">subtitle =</span> <span class="kw">expression</span>(mu <span class="op">==</span><span class="st"> </span><span class="dv">4</span>),
       <span class="dt">x =</span> <span class="st">&quot;k&quot;</span>,
       <span class="dt">y =</span> <span class="st">&quot;Pr(X = k)&quot;</span>)</code></pre></div>
<p><img src="persp005_glm_files/figure-html/plot-poisson-2.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">data_frame</span>(<span class="dt">x =</span> <span class="kw">rpois</span>(<span class="dv">10000</span>, <span class="dv">8</span>)) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(x)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_bar</span>(<span class="kw">aes</span>(<span class="dt">y =</span> (..count..)<span class="op">/</span><span class="kw">sum</span>(..count..))) <span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="st">&quot;Poisson distribution&quot;</span>,
       <span class="dt">subtitle =</span> <span class="kw">expression</span>(mu <span class="op">==</span><span class="st"> </span><span class="dv">8</span>),
       <span class="dt">x =</span> <span class="st">&quot;k&quot;</span>,
       <span class="dt">y =</span> <span class="st">&quot;Pr(X = k)&quot;</span>)</code></pre></div>
<p><img src="persp005_glm_files/figure-html/plot-poisson-3.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">data_frame</span>(<span class="dt">x =</span> <span class="kw">rpois</span>(<span class="dv">100000</span>, <span class="dv">1000</span>)) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(x)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_histogram</span>(<span class="kw">aes</span>(<span class="dt">y =</span> (..count..)<span class="op">/</span><span class="kw">sum</span>(..count..)), <span class="dt">binwidth =</span> <span class="dv">10</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="st">&quot;Poisson distribution&quot;</span>,
       <span class="dt">subtitle =</span> <span class="kw">expression</span>(mu <span class="op">==</span><span class="st"> </span><span class="dv">1000</span>),
       <span class="dt">x =</span> <span class="st">&quot;k&quot;</span>,
       <span class="dt">y =</span> <span class="st">&quot;Pr(X = k)&quot;</span>)</code></pre></div>
<p><img src="persp005_glm_files/figure-html/plot-poisson-4.png" width="672" /></p>
<p>The Poisson distribution is ideal for explaining <strong>count variables</strong>, where the variable contains frequency counts of events occurring. This could include things such as:</p>
<ul>
<li>Number of persons killed by mule or horse kicks in the Prussian army per year (this was one of the first applications of the Poisson distribution)</li>
<li>Number of terrorist attacks in a country per year</li>
<li>Number of times an individual consumes ice cream per month</li>
</ul>
<p>Count variables can take on non-negative integer values <span class="math inline">\(\{ 0,1,2,3,\dots \}\)</span>. To estimate a poisson regression model, we need the following elements:</p>
<ul>
<li><p>The random component is the Poisson distribution:</p>
<p><span class="math display">\[P(Y_i = y_i | \mu) = \frac{\mu^{k} e^{-y_i}}{y_i!}\]</span></p></li>
<li><p>The linear predictor is</p>
<p><span class="math display">\[\eta_i = \beta_0 + \beta_{1}X_i\]</span></p></li>
<li><p>The canonical link function for the Poisson distribution is the <strong>log function</strong></p>
<p><span class="math display">\[\eta_i = \ln(\mu)\]</span></p></li>
</ul>
<p>So after substituting terms, we need to estimate the parameters that maximize the log-likelihood of:</p>
<p><span class="math display">\[P(Y_i = y_i | \beta_0, \beta_1) = \frac{\ln(\beta_0 + \beta_{1}X_i)^{k} e^{-y_i}}{y_i!}\]</span></p>
<p>Let’s test this method using some real-world data.</p>
<div id="internal-armed-conflict-in-africa" class="section level2">
<h2>Internal armed conflict in Africa</h2>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">africa &lt;-<span class="st"> </span><span class="kw">read_dta</span>(<span class="st">&quot;data/internal-conflict.dta&quot;</span>)
africa</code></pre></div>
<pre><code>## # A tibble: 650 × 12
##    ccode  year cabbr INTERNAL latitude longitude literacy refugees
##    &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;
## 1    404  1995              0 12.07582 -14.64071     44.0    15400
## 2    404  1996              0 12.07582 -14.64071     42.5    15400
## 3    404  1997   GNB        0 12.07582 -14.64071     41.0    16000
## 4    404  1998              0 12.07582 -14.64071     39.5     6600
## 5    404  1999   GNB        0 12.07582 -14.64071     38.0     7100
## 6    404  2000              0 12.07582 -14.64071     40.0     7600
## 7    404  2001   GNB        0 12.07582 -14.64071     42.0     7300
## 8    404  2002              0 12.07582 -14.64071     44.0     7600
## 9    404  2003              0 12.07582 -14.64071     46.0     7600
## 10   404  2004              0 12.07582 -14.64071     48.0     7536
## # ... with 640 more rows, and 4 more variables: lnRefs &lt;dbl&gt;, lnGDP &lt;dbl&gt;,
## #   lnTrade &lt;dbl&gt;, PolityLag &lt;dbl&gt;</code></pre>
<p><code>africa</code> contains annual measures for 50 African countries from 1995-2007. The response variable <code>INTERNAL</code> contains a count of internal armed conflicts (aka civil wars and insurgencies) for each country in each year. Additional covariates include:</p>
<ul>
<li>Measures of latitude and longitude</li>
<li>Adult literacy rate</li>
<li>Number of refugees living in the country</li>
<li>Lagged natural log of GDP and trade (in constant dollars)</li>
<li>Lagged polity score (ranging from -10 to +10, with 10 = democracy)</li>
</ul>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>(africa, <span class="kw">aes</span>(INTERNAL)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_histogram</span>(<span class="dt">binwidth =</span> <span class="dv">1</span>)</code></pre></div>
<p><img src="persp005_glm_files/figure-html/africa-hist-1.png" width="672" /></p>
<p>To estimate a Poisson regression model, we use the <code>glm()</code> function with <code>family = &quot;poisson&quot;</code>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">africa_mod &lt;-<span class="st"> </span><span class="kw">glm</span>(INTERNAL <span class="op">~</span><span class="st"> </span>literacy, <span class="dt">family =</span> <span class="st">&quot;poisson&quot;</span>, <span class="dt">data =</span> africa)
<span class="kw">summary</span>(africa_mod)</code></pre></div>
<pre><code>## 
## Call:
## glm(formula = INTERNAL ~ literacy, family = &quot;poisson&quot;, data = africa)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -1.1706  -0.6836  -0.5112  -0.4111   2.8078  
## 
## Coefficients:
##               Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept) -0.0002775  0.2706730  -0.001    0.999    
## literacy    -0.0290697  0.0050831  -5.719 1.07e-08 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for poisson family taken to be 1)
## 
##     Null deviance: 405.45  on 563  degrees of freedom
## Residual deviance: 372.10  on 562  degrees of freedom
##   (86 observations deleted due to missingness)
## AIC: 584.23
## 
## Number of Fisher Scoring iterations: 6</code></pre>
<div id="interpreting-estimated-parameters" class="section level3">
<h3>Interpreting estimated parameters</h3>
<p>In linear regression, the parameters tell us the estimated linear relationship between the predictor and the response variable. In logistic regression, the parameters tell us the estimated linear relationship between the predictor and the log-odds of the response variable. In Poisson regression, the parameters tell us the estimated linear relationship between the predictor and the log-count of the response variable. So the parameter for <code>literacy</code> suggests that for every one-unit increase in adult literacy, we expect the log of the number of internal armed conflicts to decrease by 0.029.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">africa <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">data_grid</span>(literacy) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">add_predictions</span>(africa_mod) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(literacy, pred)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_line</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">x =</span> <span class="st">&quot;Literacy&quot;</span>,
       <span class="dt">y =</span> <span class="st">&quot;Predicted log-count of internal armed conflicts&quot;</span>)</code></pre></div>
<p><img src="persp005_glm_files/figure-html/africa-mod-plot-1.png" width="672" /></p>
<p>If we want to discuss the results in terms of the predicted count of the response variable, we need to <strong>exponentiate</strong> the parameters and predicted values.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">exp</span>(<span class="kw">coef</span>(africa_mod))</code></pre></div>
<pre><code>## (Intercept)    literacy 
##   0.9997225   0.9713488</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">africa <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">data_grid</span>(literacy) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">add_predictions</span>(africa_mod) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">pred =</span> <span class="kw">exp</span>(pred)) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(literacy, pred)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_line</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">x =</span> <span class="st">&quot;Literacy&quot;</span>,
       <span class="dt">y =</span> <span class="st">&quot;Predicted count of internal armed conflicts&quot;</span>)</code></pre></div>
<p><img src="persp005_glm_files/figure-html/africa-exp-1.png" width="672" /></p>
<p>And so like with logistic regression, the relationship between predictors and the predicted log-count is linear, whereas the relationship between predictors and the predicted count is non-linear.</p>
</div>
<div id="multiple-predictors" class="section level3">
<h3>Multiple predictors</h3>
<p>Like with other GLMs, we can add multiple predictors to the model.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">africa_big &lt;-<span class="st"> </span><span class="kw">glm</span>(INTERNAL <span class="op">~</span><span class="st"> </span>literacy <span class="op">+</span><span class="st"> </span>PolityLag,
                  <span class="dt">data =</span> africa, <span class="dt">family =</span> <span class="st">&quot;poisson&quot;</span>)
<span class="kw">summary</span>(africa_big)</code></pre></div>
<pre><code>## 
## Call:
## glm(formula = INTERNAL ~ literacy + PolityLag, family = &quot;poisson&quot;, 
##     data = africa)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -1.3804  -0.6773  -0.5169  -0.3424   2.8026  
## 
## Coefficients:
##              Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept)  0.111814   0.286781   0.390    0.697    
## literacy    -0.030630   0.005394  -5.679 1.36e-08 ***
## PolityLag   -0.047329   0.019465  -2.432    0.015 *  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for poisson family taken to be 1)
## 
##     Null deviance: 380.81  on 520  degrees of freedom
## Residual deviance: 341.96  on 518  degrees of freedom
##   (129 observations deleted due to missingness)
## AIC: 544.08
## 
## Number of Fisher Scoring iterations: 6</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">africa <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">data_grid</span>(literacy, <span class="dt">PolityLag =</span> <span class="kw">seq</span>(<span class="op">-</span><span class="dv">10</span>, <span class="dv">10</span>, <span class="dv">4</span>)) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">add_predictions</span>(africa_big) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">pred =</span> <span class="kw">exp</span>(pred),
         <span class="dt">PolityLag =</span> <span class="kw">factor</span>(PolityLag)) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(literacy, pred, <span class="dt">color =</span> PolityLag)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_line</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">x =</span> <span class="st">&quot;Literacy&quot;</span>,
       <span class="dt">y =</span> <span class="st">&quot;Predicted count of internal armed conflicts&quot;</span>)</code></pre></div>
<p><img src="persp005_glm_files/figure-html/africa-mod-big-1.png" width="672" /></p>
</div>
</div>
<div id="over-or-underdispersion" class="section level2">
<h2>Over or underdispersion</h2>
<p>Recall that the Poisson distribution assumes that the mean and variance are identical. What happens if this assumption is violated? Sometimes this is the result of omitted variable bias, and we are just not accounting for all the variables that are included in the underlying data generating process. However if the variable truly has a conditional variance different from the conditional mean, then the Poisson distribution is not the most appropriate random component for the data.</p>
<p>There are a couple ways to test for overdispersion (variance larger than the mean) or underdispersion (variance smaller than the mean). One method is to estimate a <strong>quasi-poisson model</strong>. It still relies on the Poisson distribution, but introduces a dispersion parameter:</p>
<p><span class="math display">\[V(Y_i | \eta_i) = \phi \mu_i\]</span></p>
<p>If <span class="math inline">\(\phi &gt; 1\)</span>, then the conditional variance of <span class="math inline">\(Y\)</span> increases more rapidly than its mean (overdispersion). Estimating this dispersion parameter requires <strong>quasi-likelihood</strong> estimation which combines aspects of maximum-likelihood and method-of-moments and is beyond the scope of this class. However it yields virtually identical estimates of the parameters, and in the case of overdispersion will also yield larger (and more realistic) standard errors.</p>
<p>Let’s estimate the original African regression model using the quasi-poisson method:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">africa_quasimod &lt;-<span class="st"> </span><span class="kw">glm</span>(INTERNAL <span class="op">~</span><span class="st"> </span>literacy, <span class="dt">family =</span> <span class="st">&quot;quasipoisson&quot;</span>, <span class="dt">data =</span> africa)
<span class="kw">summary</span>(africa_quasimod)</code></pre></div>
<pre><code>## 
## Call:
## glm(formula = INTERNAL ~ literacy, family = &quot;quasipoisson&quot;, data = africa)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -1.1706  -0.6836  -0.5112  -0.4111   2.8078  
## 
## Coefficients:
##               Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) -0.0002775  0.2630325  -0.001    0.999    
## literacy    -0.0290697  0.0049396  -5.885 6.84e-09 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for quasipoisson family taken to be 0.9443412)
## 
##     Null deviance: 405.45  on 563  degrees of freedom
## Residual deviance: 372.10  on 562  degrees of freedom
##   (86 observations deleted due to missingness)
## AIC: NA
## 
## Number of Fisher Scoring iterations: 6</code></pre>
<p>Here the disperson parameter is actually quite close to 1, so underdispersion doesn’t appear to be a significant problem. What about for the full model?</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">africa_quasibig &lt;-<span class="st"> </span><span class="kw">glm</span>(INTERNAL <span class="op">~</span><span class="st"> </span>literacy <span class="op">+</span><span class="st"> </span>PolityLag,
                  <span class="dt">data =</span> africa, <span class="dt">family =</span> <span class="st">&quot;quasipoisson&quot;</span>)
<span class="kw">summary</span>(africa_quasibig)</code></pre></div>
<pre><code>## 
## Call:
## glm(formula = INTERNAL ~ literacy + PolityLag, family = &quot;quasipoisson&quot;, 
##     data = africa)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -1.3804  -0.6773  -0.5169  -0.3424   2.8026  
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  0.111814   0.278662   0.401   0.6884    
## literacy    -0.030630   0.005241  -5.844 9.01e-09 ***
## PolityLag   -0.047329   0.018914  -2.502   0.0126 *  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for quasipoisson family taken to be 0.944181)
## 
##     Null deviance: 380.81  on 520  degrees of freedom
## Residual deviance: 341.96  on 518  degrees of freedom
##   (129 observations deleted due to missingness)
## AIC: NA
## 
## Number of Fisher Scoring iterations: 6</code></pre>
<p>Still quite similar to 1.</p>
</div>
</div>
<div id="session-info" class="section level1 toc-ignore">
<h1>Session Info</h1>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">devtools<span class="op">::</span><span class="kw">session_info</span>()</code></pre></div>
<pre><code>##  setting  value                       
##  version  R version 3.4.1 (2017-06-30)
##  system   x86_64, darwin15.6.0        
##  ui       X11                         
##  language (EN)                        
##  collate  en_US.UTF-8                 
##  tz       America/Chicago             
##  date     2017-08-01                  
## 
##  package      * version    date       source                              
##  assertthat     0.2.0      2017-04-11 CRAN (R 3.4.0)                      
##  backports      1.1.0      2017-05-22 CRAN (R 3.4.0)                      
##  base         * 3.4.1      2017-07-07 local                               
##  base64enc      0.1-3      2015-07-28 CRAN (R 3.4.0)                      
##  bindr          0.1        2016-11-13 CRAN (R 3.4.0)                      
##  bindrcpp     * 0.2        2017-06-17 CRAN (R 3.4.0)                      
##  bit            1.1-12     2014-04-09 CRAN (R 3.4.0)                      
##  bit64          0.9-7      2017-05-08 CRAN (R 3.4.0)                      
##  blob           1.1.0      2017-06-17 CRAN (R 3.4.0)                      
##  boxes          0.0.0.9000 2017-07-19 Github (r-pkgs/boxes@03098dc)       
##  broom        * 0.4.2      2017-02-13 CRAN (R 3.4.0)                      
##  car            2.1-5      2017-07-04 CRAN (R 3.4.1)                      
##  caret        * 6.0-76     2017-04-18 CRAN (R 3.4.0)                      
##  cellranger     1.1.0      2016-07-27 CRAN (R 3.4.0)                      
##  class          7.3-14     2015-08-30 CRAN (R 3.4.1)                      
##  clisymbols     1.2.0      2017-05-21 cran (@1.2.0)                       
##  codetools      0.2-15     2016-10-05 CRAN (R 3.4.1)                      
##  colorspace     1.3-2      2016-12-14 CRAN (R 3.4.0)                      
##  compiler       3.4.1      2017-07-07 local                               
##  config         0.2        2016-08-02 CRAN (R 3.4.0)                      
##  crayon         1.3.2.9000 2017-07-19 Github (gaborcsardi/crayon@750190f) 
##  datasets     * 3.4.1      2017-07-07 local                               
##  DBI            0.7        2017-06-18 CRAN (R 3.4.0)                      
##  dbplyr         1.1.0      2017-06-27 CRAN (R 3.4.1)                      
##  devtools       1.13.2     2017-06-02 CRAN (R 3.4.0)                      
##  digest         0.6.12     2017-01-27 CRAN (R 3.4.0)                      
##  dplyr        * 0.7.2      2017-07-20 CRAN (R 3.4.1)                      
##  e1071          1.6-8      2017-02-02 CRAN (R 3.4.0)                      
##  evaluate       0.10.1     2017-06-24 CRAN (R 3.4.1)                      
##  forcats      * 0.2.0      2017-01-23 CRAN (R 3.4.0)                      
##  foreach        1.4.3      2015-10-13 CRAN (R 3.4.0)                      
##  foreign        0.8-69     2017-06-22 CRAN (R 3.4.1)                      
##  gapminder    * 0.2.0      2015-12-31 CRAN (R 3.4.0)                      
##  gganimate    * 0.1.0.9000 2017-05-26 Github (dgrtwo/gganimate@bf82002)   
##  ggplot2      * 2.2.1      2016-12-30 CRAN (R 3.4.0)                      
##  glue           1.1.1      2017-06-21 CRAN (R 3.4.1)                      
##  graphics     * 3.4.1      2017-07-07 local                               
##  grDevices    * 3.4.1      2017-07-07 local                               
##  grid           3.4.1      2017-07-07 local                               
##  gtable         0.2.0      2016-02-26 CRAN (R 3.4.0)                      
##  haven        * 1.1.0      2017-07-09 CRAN (R 3.4.1)                      
##  highr          0.6        2016-05-09 CRAN (R 3.4.0)                      
##  hms            0.3        2016-11-22 CRAN (R 3.4.0)                      
##  htmltools      0.3.6      2017-04-28 CRAN (R 3.4.0)                      
##  httpuv         1.3.5      2017-07-04 CRAN (R 3.4.1)                      
##  httr           1.2.1      2016-07-03 CRAN (R 3.4.0)                      
##  iterators      1.0.8      2015-10-13 CRAN (R 3.4.0)                      
##  jsonlite       1.5        2017-06-01 CRAN (R 3.4.0)                      
##  knitr        * 1.16       2017-05-18 CRAN (R 3.4.0)                      
##  labeling       0.3        2014-08-23 CRAN (R 3.4.0)                      
##  lattice      * 0.20-35    2017-03-25 CRAN (R 3.4.1)                      
##  lazyeval       0.2.0      2016-06-12 CRAN (R 3.4.0)                      
##  lme4           1.1-13     2017-04-19 CRAN (R 3.4.0)                      
##  lubridate      1.6.0      2016-09-13 CRAN (R 3.4.0)                      
##  magrittr       1.5        2014-11-22 CRAN (R 3.4.0)                      
##  MASS           7.3-47     2017-02-26 CRAN (R 3.4.1)                      
##  Matrix         1.2-10     2017-05-03 CRAN (R 3.4.1)                      
##  MatrixModels   0.4-1      2015-08-22 CRAN (R 3.4.0)                      
##  memoise        1.1.0      2017-04-21 CRAN (R 3.4.0)                      
##  methods        3.4.1      2017-07-07 local                               
##  mgcv           1.8-18     2017-07-28 CRAN (R 3.4.1)                      
##  mime           0.5        2016-07-07 CRAN (R 3.4.0)                      
##  minqa          1.2.4      2014-10-09 CRAN (R 3.4.0)                      
##  mnormt         1.5-5      2016-10-15 CRAN (R 3.4.0)                      
##  ModelMetrics   1.1.0      2016-08-26 CRAN (R 3.4.0)                      
##  modelr       * 0.1.1      2017-07-24 CRAN (R 3.4.1)                      
##  munsell        0.4.3      2016-02-13 CRAN (R 3.4.0)                      
##  nlme           3.1-131    2017-02-06 CRAN (R 3.4.1)                      
##  nloptr         1.0.4      2014-08-04 CRAN (R 3.4.0)                      
##  nnet         * 7.3-12     2016-02-02 CRAN (R 3.4.1)                      
##  nycflights13   0.2.2      2017-01-27 CRAN (R 3.4.0)                      
##  parallel       3.4.1      2017-07-07 local                               
##  pbkrtest       0.4-7      2017-03-15 CRAN (R 3.4.0)                      
##  pkgconfig      2.0.1      2017-03-21 CRAN (R 3.4.0)                      
##  plyr           1.8.4      2016-06-08 CRAN (R 3.4.0)                      
##  pROC         * 1.10.0     2017-06-10 CRAN (R 3.4.0)                      
##  psych          1.7.5      2017-05-03 CRAN (R 3.4.1)                      
##  purrr        * 0.2.2.2    2017-05-11 CRAN (R 3.4.0)                      
##  quantreg       5.33       2017-04-18 CRAN (R 3.4.0)                      
##  R6             2.2.2      2017-06-17 CRAN (R 3.4.0)                      
##  rappdirs       0.3.1      2016-03-28 CRAN (R 3.4.0)                      
##  rcfss        * 0.1.5      2017-07-31 local                               
##  Rcpp           0.12.12    2017-07-15 CRAN (R 3.4.1)                      
##  readr        * 1.1.1      2017-05-16 CRAN (R 3.4.0)                      
##  readxl         1.0.0      2017-04-18 CRAN (R 3.4.0)                      
##  reshape2       1.4.2      2016-10-22 CRAN (R 3.4.0)                      
##  rlang          0.1.1      2017-05-18 CRAN (R 3.4.0)                      
##  rmarkdown      1.6        2017-06-15 CRAN (R 3.4.0)                      
##  rprojroot      1.2        2017-01-16 CRAN (R 3.4.0)                      
##  RSQLite      * 2.0        2017-06-19 CRAN (R 3.4.1)                      
##  rstudioapi     0.6        2016-06-27 CRAN (R 3.4.0)                      
##  rvest          0.3.2      2016-06-17 CRAN (R 3.4.0)                      
##  scales         0.4.1      2016-11-09 CRAN (R 3.4.0)                      
##  shiny          1.0.3      2017-04-26 CRAN (R 3.4.0)                      
##  sparklyr     * 0.6.0      2017-07-29 CRAN (R 3.4.1)                      
##  SparseM        1.77       2017-04-23 CRAN (R 3.4.0)                      
##  splines        3.4.1      2017-07-07 local                               
##  stats        * 3.4.1      2017-07-07 local                               
##  stats4         3.4.1      2017-07-07 local                               
##  stringi        1.1.5      2017-04-07 CRAN (R 3.4.0)                      
##  stringr      * 1.2.0      2017-02-18 CRAN (R 3.4.0)                      
##  tibble       * 1.3.3      2017-05-28 CRAN (R 3.4.0)                      
##  tidyr        * 0.6.3      2017-05-15 CRAN (R 3.4.0)                      
##  tidyverse    * 1.1.1.9000 2017-07-19 Github (tidyverse/tidyverse@a028619)
##  titanic      * 0.1.0      2015-08-31 CRAN (R 3.4.0)                      
##  tools          3.4.1      2017-07-07 local                               
##  tweenr       * 0.1.5      2016-10-10 CRAN (R 3.4.0)                      
##  utils        * 3.4.1      2017-07-07 local                               
##  withr          2.0.0      2017-07-28 CRAN (R 3.4.1)                      
##  xml2           1.1.1      2017-01-24 CRAN (R 3.4.0)                      
##  xtable         1.8-2      2016-02-05 CRAN (R 3.4.0)                      
##  yaml           2.1.14     2016-11-12 CRAN (R 3.4.0)</code></pre>
</div>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>Note that the response variable must be coded as a factor, not a numeric, column.<a href="#fnref1">↩</a></p></li>
</ol>
</div>

<p>This work is licensed under the  <a href="http://creativecommons.org/licenses/by-nc/4.0/">CC BY-NC 4.0 Creative Commons License</a>.</p>


</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
