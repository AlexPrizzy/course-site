<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />




<title>Text analysis: supervised classification</title>

<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/readable.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="site_libs/jqueryui-1.11.4/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-45631879-2', 'auto');
  ga('send', 'pageview');

</script>


<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>


<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>


</head>

<body>

<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
.tabbed-pane {
  padding-top: 12px;
}
button.code-folding-btn:focus {
  outline: none;
}
</style>


<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 66px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 71px;
  margin-top: -71px;
}

.section h2 {
  padding-top: 71px;
  margin-top: -71px;
}
.section h3 {
  padding-top: 71px;
  margin-top: -71px;
}
.section h4 {
  padding-top: 71px;
  margin-top: -71px;
}
.section h5 {
  padding-top: 71px;
  margin-top: -71px;
}
.section h6 {
  padding-top: 71px;
  margin-top: -71px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>


<div class="container-fluid main-container">

<!-- tabsets -->
<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});
</script>

<!-- code folding -->




<script>
$(document).ready(function ()  {

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_').toLowerCase();
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}


.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
  padding-left: 25px;
  text-indent: 0;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>

<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row-fluid">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">Computing for the Social Sciences</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">Home</a>
</li>
<li>
  <a href="faq.html">FAQ</a>
</li>
<li>
  <a href="syllabus.html">Syllabus</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div class="fluid-row" id="header">



<h1 class="title toc-ignore">Text analysis: supervised classification</h1>

</div>


<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(tidyverse)
<span class="kw">library</span>(tidytext)
<span class="kw">library</span>(stringr)
<span class="kw">library</span>(caret)
<span class="kw">library</span>(tm)

<span class="kw">set.seed</span>(<span class="dv">1234</span>)
<span class="kw">theme_set</span>(<span class="kw">theme_minimal</span>())</code></pre></div>
<p>A common task in social science involves hand-labeling sets of documents for specific variables (e.g.Â manual coding). In previous years, this required hiring a set of research assistants and training them to read and evaluate text by hand. It was expensive, prone to error, required extensive data quality checks, and was infeasible if you had an extremely large corpus of text that required classification.</p>
<p>Alternatively, we can now use statistical learning models to classify text into specific sets of categories. This is known as <strong>supervised learning</strong>. The basic process is:</p>
<ol style="list-style-type: decimal">
<li>Hand-code a small set of documents (say <span class="math inline">\(1000\)</span>) for whatever variable(s) you care about</li>
<li>Train a statistical learning model on the hand-coded data, using the variable as the outcome of interest and the text features of the documents as the predictors</li>
<li>Evaluate the effectiveness of the statistical learning model via a <a href="stat004_decision_trees.html">resampling method</a></li>
<li>Once you have trained a model with sufficient predictive accuracy, apply the model to the remaining set of documents that have never been hand-coded (say <span class="math inline">\(1000000\)</span>)</li>
</ol>
<div id="sample-set-of-documents-uscongress" class="section level1">
<h1>Sample set of documents: <code>USCongress</code></h1>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># get USCongress data</span>
<span class="kw">data</span>(USCongress, <span class="dt">package =</span> <span class="st">&quot;RTextTools&quot;</span>)

(congress &lt;-<span class="st"> </span><span class="kw">as_tibble</span>(USCongress) <span class="op">%&gt;%</span>
<span class="st">    </span><span class="kw">mutate</span>(<span class="dt">text =</span> <span class="kw">as.character</span>(text)))</code></pre></div>
<pre><code>## # A tibble: 4,449 x 6
##       ID  cong billnum h_or_sen major
##    &lt;int&gt; &lt;int&gt;   &lt;int&gt;   &lt;fctr&gt; &lt;int&gt;
##  1     1   107    4499       HR    18
##  2     2   107    4500       HR    18
##  3     3   107    4501       HR    18
##  4     4   107    4502       HR    18
##  5     5   107    4503       HR     5
##  6     6   107    4504       HR    21
##  7     7   107    4505       HR    15
##  8     8   107    4506       HR    18
##  9     9   107    4507       HR    18
## 10    10   107    4508       HR    18
## # ... with 4,439 more rows, and 1 more variables: text &lt;chr&gt;</code></pre>
<p><code>USCongress</code> from the <a href="http://www.rtexttools.com/"><code>RTextTools</code> package</a> contains a sample of hand-labeled bills from the United States Congress. For each bill we have a text description of the billâs purpose (e.g. âTo amend the Immigration and Nationality Act in regard to Caribbean-born immigrants.â) as well as the billâs <a href="http://www.comparativeagendas.net/pages/master-codebook">major policy topic code corresponding to the subject of the bill</a>. There are 20 major policy topics according to this coding scheme (e.g.Â Macroeconomics, Civil Rights, Health). These topic codes have been labeled by hand. The current dataset only contains a sample of bills from the 107th Congress (2001-03). If we wanted to obtain policy topic codes for all bills introduced over a longer period, we would have to manually code tens of thousands if not millions of bill descriptions. Clearly a task outside of our capabilities.</p>
<p>Instead, we can build a statistical learning model which predicts the major topic code of a bill given its text description. These notes outline a potential <code>tidytext</code> workflow for such an approach.</p>
</div>
<div id="create-tidy-text-data-frame" class="section level1">
<h1>Create tidy text data frame</h1>
<p>First we convert <code>USCongress</code> into a tidy text data frame.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">(congress_tokens &lt;-<span class="st"> </span>congress <span class="op">%&gt;%</span>
<span class="st">   </span><span class="kw">unnest_tokens</span>(<span class="dt">output =</span> word, <span class="dt">input =</span> text) <span class="op">%&gt;%</span>
<span class="st">   </span><span class="co"># remove numbers</span>
<span class="st">   </span><span class="kw">filter</span>(<span class="op">!</span><span class="kw">str_detect</span>(word, <span class="st">&quot;^[0-9]*$&quot;</span>)) <span class="op">%&gt;%</span>
<span class="st">   </span><span class="co"># remove stop words</span>
<span class="st">   </span><span class="kw">anti_join</span>(stop_words) <span class="op">%&gt;%</span>
<span class="st">   </span><span class="co"># stem the words</span>
<span class="st">   </span><span class="kw">mutate</span>(<span class="dt">word =</span> SnowballC<span class="op">::</span><span class="kw">wordStem</span>(word)))</code></pre></div>
<pre><code>## Joining, by = &quot;word&quot;</code></pre>
<pre><code>## # A tibble: 58,820 x 6
##       ID  cong billnum h_or_sen major        word
##    &lt;int&gt; &lt;int&gt;   &lt;int&gt;   &lt;fctr&gt; &lt;int&gt;       &lt;chr&gt;
##  1     1   107    4499       HR    18     suspend
##  2     1   107    4499       HR    18 temporarili
##  3     1   107    4499       HR    18        duti
##  4     1   107    4499       HR    18        fast
##  5     1   107    4499       HR    18     magenta
##  6     1   107    4499       HR    18       stage
##  7     2   107    4500       HR    18     suspend
##  8     2   107    4500       HR    18 temporarili
##  9     2   107    4500       HR    18        duti
## 10     2   107    4500       HR    18        fast
## # ... with 58,810 more rows</code></pre>
<p>Notice there are a few key steps involved here:</p>
<ul>
<li><code>unnest_tokens(output = word, input = text)</code> - converts the data frame to a tidy text data frame and automatically converts all tokens to lowercase</li>
<li><code>filter(!str_detect(word, &quot;^[0-9]*$&quot;))</code> - removes all tokens which are strictly numbers. Numbers are generally not useful features in classifying documents (though sometimes they may be useful - you can compare results with and without numbers)</li>
<li><code>anti_join(stop_words)</code> - remove common stop words that are uninformative and will likely not be useful in predicting major topic codes</li>
<li><code>mutate(word = SnowballC::wordStem(word)))</code> - uses the <a href="https://tartarus.org/martin/PorterStemmer/">Porter stemming algorithm</a> to stem all the tokens to their root word</li>
</ul>
<p>Most of these steps are to reduce the number of text features in the set of documents. This is necessary because as you increase the number of observations (i.e.Â documents) and variables/features (i.e.Â tokens/words), the resulting statistical learning model will become more complex and harder to compute. Given a large enough corpus or set of variables, you may not be able to estimate many statistical learning models with your local computer - you would need to offload the work to a remote computing cluster.</p>
</div>
<div id="create-document-term-matrix" class="section level1">
<h1>Create document-term matrix</h1>
<p>tidy text data frames are one-row-per-token, but for statistical learning algorithms we need our data in a one-row-per-document format. That is, a document-term matrix. We can use <code>cast_dtm()</code> to create a document-term matrix.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">(congress_dtm &lt;-<span class="st"> </span>congress_tokens <span class="op">%&gt;%</span>
<span class="st">   </span><span class="co"># get count of each token in each document</span>
<span class="st">   </span><span class="kw">count</span>(ID, word) <span class="op">%&gt;%</span>
<span class="st">   </span><span class="co"># create a document-term matrix with all features and tf weighting</span>
<span class="st">   </span><span class="kw">cast_dtm</span>(<span class="dt">document =</span> ID, <span class="dt">term =</span> word, <span class="dt">value =</span> n))</code></pre></div>
<pre><code>## &lt;&lt;DocumentTermMatrix (documents: 4449, terms: 4902)&gt;&gt;
## Non-/sparse entries: 55033/21753965
## Sparsity           : 100%
## Maximal term length: 24
## Weighting          : term frequency (tf)</code></pre>
<div id="weighting" class="section level2">
<h2>Weighting</h2>
<p>The default approach is to use <a href="http://tidytextmining.com/tfidf.html"><strong>term frequency</strong> (tf) weighting</a>, or a simple count of how frequently a word occurs in a document. An alternative approach is <strong>term frequency inverse document frequency</strong> (tf-idf), which is the frequency of a term adjusted for how rarely it is used. To generate tf-idf and use this for the document-term matrix, we can change the weighting function in <code>cast_dtm()</code>:<a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">congress_tokens <span class="op">%&gt;%</span>
<span class="st">  </span><span class="co"># get count of each token in each document</span>
<span class="st">  </span><span class="kw">count</span>(ID, word) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="co"># create a document-term matrix with all features and tf-idf weighting</span>
<span class="st">  </span><span class="kw">cast_dtm</span>(<span class="dt">document =</span> ID, <span class="dt">term =</span> word, <span class="dt">value =</span> n,
           <span class="dt">weighting =</span> tm<span class="op">::</span>weightTfIdf)</code></pre></div>
<pre><code>## &lt;&lt;DocumentTermMatrix (documents: 4449, terms: 4902)&gt;&gt;
## Non-/sparse entries: 55033/21753965
## Sparsity           : 100%
## Maximal term length: 24
## Weighting          : term frequency - inverse document frequency (normalized) (tf-idf)</code></pre>
<p>For now, letâs just continue to use the term frequency approach. But it is a good idea to compare the results of tf vs.Â tf-idf to see if one method improves model performance over the other method.</p>
</div>
<div id="sparsity" class="section level2">
<h2>Sparsity</h2>
<p>Another approach to reducing model complexity is to remove sparse terms from the model. That is, remove tokens which do not appear across many documents. It is similar to using tf-idf weighting, but directly deletes sparse variables from the document-term matrix. This results in a statistical learning model with a much smaller set of variables.</p>
<p>The <code>tm</code> package contains the <code>removeSparseTerms()</code> function, which does this task. The first argument is a document-term matrix, and the second argument defines the maximal allowed sparsity in the range from 0 to 1. So for instance, <code>sparse = .99</code> would remove any tokens which are missing from more than <span class="math inline">\(99%\)</span> of the documents in the corpus. Notice the effect changing this value has on the number of variables (tokens) retained in the document-term matrix:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">removeSparseTerms</span>(congress_dtm, <span class="dt">sparse =</span> .<span class="dv">99</span>)</code></pre></div>
<pre><code>## &lt;&lt;DocumentTermMatrix (documents: 4449, terms: 209)&gt;&gt;
## Non-/sparse entries: 33794/896047
## Sparsity           : 96%
## Maximal term length: 11
## Weighting          : term frequency (tf)</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">removeSparseTerms</span>(congress_dtm, <span class="dt">sparse =</span> .<span class="dv">95</span>)</code></pre></div>
<pre><code>## &lt;&lt;DocumentTermMatrix (documents: 4449, terms: 28)&gt;&gt;
## Non-/sparse entries: 18447/106125
## Sparsity           : 85%
## Maximal term length: 11
## Weighting          : term frequency (tf)</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">removeSparseTerms</span>(congress_dtm, <span class="dt">sparse =</span> .<span class="dv">90</span>)</code></pre></div>
<pre><code>## &lt;&lt;DocumentTermMatrix (documents: 4449, terms: 16)&gt;&gt;
## Non-/sparse entries: 14917/56267
## Sparsity           : 79%
## Maximal term length: 9
## Weighting          : term frequency (tf)</code></pre>
<p>It will be tough to build an effective model with just 16 tokens. Normal values for <code>sparse</code> are generally around <span class="math inline">\(.99\)</span>. Letâs use that to create and store our final document-term matrix.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">congress_dtm &lt;-<span class="st"> </span><span class="kw">removeSparseTerms</span>(congress_dtm, <span class="dt">sparse =</span> .<span class="dv">99</span>)</code></pre></div>
</div>
</div>
<div id="exploratory-analysis" class="section level1">
<h1>Exploratory analysis</h1>
<p>Before building a fancy schmancy statistical model, we can first investigate if there are certain terms or tokens associated with each major topic category. We can do this purely with <code>tidytext</code> tools: we directly calculate the tf-idf for each term <strong>treating each major topic code as the document</strong>, rather than the individual bill. Then we can visualize the tokens with the highest tf-idf associated with each topic.<a href="#fn2" class="footnoteRef" id="fnref2"><sup>2</sup></a></p>
<p>To calculate tf-idf directly in the data frame, first we <code>count()</code> the frequency each token appears in bills from each major topic code, then use <code>bind_tf_idf()</code> to calculate the tf-idf for each token in each topic:<a href="#fn3" class="footnoteRef" id="fnref3"><sup>3</sup></a></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">(congress_tfidf &lt;-<span class="st"> </span>congress_tokens <span class="op">%&gt;%</span>
<span class="st">   </span><span class="kw">count</span>(major, word) <span class="op">%&gt;%</span>
<span class="st">   </span><span class="kw">bind_tf_idf</span>(<span class="dt">term_col =</span> word, <span class="dt">document_col =</span> major, <span class="dt">n_col =</span> n))</code></pre></div>
<pre><code>## # A tibble: 13,190 x 6
##    major      word     n          tf       idf       tf_idf
##    &lt;int&gt;     &lt;chr&gt; &lt;int&gt;       &lt;dbl&gt;     &lt;dbl&gt;        &lt;dbl&gt;
##  1     1    25,000     1 0.000483559 1.8971200 0.0009173694
##  2     1 3,500,000     1 0.000483559 2.9957323 0.0014486133
##  3     1      38.6     1 0.000483559 2.9957323 0.0014486133
##  4     1   abolish     1 0.000483559 2.3025851 0.0011134357
##  5     1    abroad     1 0.000483559 1.8971200 0.0009173694
##  6     1      abus     2 0.000967118 1.0498221 0.0010153019
##  7     1   acceler     1 0.000483559 1.0498221 0.0005076509
##  8     1   account     6 0.002901354 0.2231436 0.0006474184
##  9     1     accur     1 0.000483559 1.0498221 0.0005076509
## 10     1    acquir     2 0.000967118 1.3862944 0.0013407102
## # ... with 13,180 more rows</code></pre>
<p>Now all we need to do is plot the words with the highest tf-idf scores for each category. Since there are 20 major topics and a 20 panel facet graph is very dense, letâs just look at four of the categories:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># sort the data frame and convert word to a factor column</span>
plot_congress &lt;-<span class="st"> </span>congress_tfidf <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">arrange</span>(<span class="kw">desc</span>(tf_idf)) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">word =</span> <span class="kw">factor</span>(word, <span class="dt">levels =</span> <span class="kw">rev</span>(<span class="kw">unique</span>(word))))

<span class="co"># graph the top 10 tokens for 4 categories</span>
plot_congress <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">filter</span>(major <span class="op">%in%</span><span class="st"> </span><span class="kw">c</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">3</span>, <span class="dv">6</span>)) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">major =</span> <span class="kw">factor</span>(major, <span class="dt">levels =</span> <span class="kw">c</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">3</span>, <span class="dv">6</span>),
                        <span class="dt">labels =</span> <span class="kw">c</span>(<span class="st">&quot;Macroeconomics&quot;</span>, <span class="st">&quot;Civil Rights&quot;</span>,
                                   <span class="st">&quot;Health&quot;</span>, <span class="st">&quot;Education&quot;</span>))) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">group_by</span>(major) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">top_n</span>(<span class="dv">10</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">ungroup</span>() <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(word, tf_idf)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_col</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">x =</span> <span class="ot">NULL</span>, <span class="dt">y =</span> <span class="st">&quot;tf-idf&quot;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">facet_wrap</span>(<span class="op">~</span>major, <span class="dt">scales =</span> <span class="st">&quot;free&quot;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">coord_flip</span>()</code></pre></div>
<pre><code>## Selecting by tf_idf</code></pre>
<p><img src="text_classification_files/figure-html/plot-tf-idf-1.png" width="672" /></p>
<p>Do these make sense? I think they do (well, some of them). This suggests a statistical learning model may find these tokens useful in predicting major topic codes.</p>
</div>
<div id="estimate-model" class="section level1">
<h1>Estimate model</h1>
<p>Now to estimate the model, we return to the <code>caret</code> package. Letâs try a random forest model first. Here is the syntax for estimating the model:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">congress_rf &lt;-<span class="st"> </span><span class="kw">train</span>(<span class="dt">x =</span> <span class="kw">as.matrix</span>(congress_dtm),
                     <span class="dt">y =</span> <span class="kw">factor</span>(congress<span class="op">$</span>major),
                     <span class="dt">method =</span> <span class="st">&quot;rf&quot;</span>,
                     <span class="dt">ntree =</span> <span class="dv">200</span>,
                     <span class="dt">trControl =</span> <span class="kw">trainControl</span>(<span class="dt">method =</span> <span class="st">&quot;oob&quot;</span>))</code></pre></div>
<p>Note the differences from <a href="stat004_decision_trees.html#estimating_a_random_forest">how we specified them before with a standard data frame</a>.</p>
<ul>
<li><code>x = as.matrix(congress_dtm)</code> - instead of using a formula, we pass the independent and dependent variables separately into <code>train()</code>. <code>x</code> needs to be a simple matrix, data frame, or sparse matrix. These are specific types of objects in R. <code>congress_dtm</code> is a <code>DocumentTermMatrix</code>, so we use <code>as.matrix()</code> to convert it to a simple matrix.</li>
<li><code>y = factor(congress$major)</code> - we return to the original <code>congress</code> data frame to obtain the vector of outcome values for each document. Here, this is the major topic code associated with each bill. The important thing is that the order of documents in <code>x</code> remains the same as the order of documents in <code>y</code>, so that each document is associated with the correct outcome. Because <code>congress$major</code> is a numeric vector, we need to convert it to a factor vector so that we perform classification (and not regression).</li>
</ul>
<p>Otherwise everything else is the same as before. Notice how long it takes to build a random forest model with 10 trees, compared to a more typical random forest model with 200 trees:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">system.time</span>({
  congress_rf_<span class="dv">10</span> &lt;-<span class="st"> </span><span class="kw">train</span>(<span class="dt">x =</span> <span class="kw">as.matrix</span>(congress_dtm),
                          <span class="dt">y =</span> <span class="kw">factor</span>(congress<span class="op">$</span>major),
                          <span class="dt">method =</span> <span class="st">&quot;rf&quot;</span>,
                          <span class="dt">ntree =</span> <span class="dv">10</span>,
                          <span class="dt">trControl =</span> <span class="kw">trainControl</span>(<span class="dt">method =</span> <span class="st">&quot;oob&quot;</span>))
})</code></pre></div>
<pre><code>## Loading required package: randomForest</code></pre>
<pre><code>## randomForest 4.6-12</code></pre>
<pre><code>## Type rfNews() to see new features/changes/bug fixes.</code></pre>
<pre><code>## 
## Attaching package: &#39;randomForest&#39;</code></pre>
<pre><code>## The following object is masked from &#39;package:dplyr&#39;:
## 
##     combine</code></pre>
<pre><code>## The following object is masked from &#39;package:ggplot2&#39;:
## 
##     margin</code></pre>
<pre><code>##    user  system elapsed 
##  10.832   0.188  11.201</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">system.time</span>({
  congress_rf_<span class="dv">200</span> &lt;-<span class="st"> </span><span class="kw">train</span>(<span class="dt">x =</span> <span class="kw">as.matrix</span>(congress_dtm),
                           <span class="dt">y =</span> <span class="kw">factor</span>(congress<span class="op">$</span>major),
                           <span class="dt">method =</span> <span class="st">&quot;rf&quot;</span>,
                           <span class="dt">ntree =</span> <span class="dv">200</span>,
                           <span class="dt">trControl =</span> <span class="kw">trainControl</span>(<span class="dt">method =</span> <span class="st">&quot;oob&quot;</span>))
})</code></pre></div>
<pre><code>##    user  system elapsed 
## 188.377   1.873 198.240</code></pre>
<p>This is why it is important to remove sparse features and simplify the document-term matrix as much as possible - the more text features and observations in the document-term matrix, the longer it takes to train the model.</p>
<p>Otherwise, the result is no different from a model trained on categorical or continuous variables. We can generate the same diagnostics information:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">congress_rf_<span class="dv">200</span><span class="op">$</span>finalModel</code></pre></div>
<pre><code>## 
## Call:
##  randomForest(x = x, y = y, ntree = 200, mtry = param$mtry) 
##                Type of random forest: classification
##                      Number of trees: 200
## No. of variables tried at each split: 105
## 
##         OOB estimate of  error rate: 32.64%
## Confusion matrix:
##      1  2   3  4   5   6   7  8  10  12 13 14  15  16 17  18 19  20  21 99
## 1  106  0   2  0   4   3   2  4   6   9  0  3  10   2  2   3  0   6   1  0
## 2    1 19   5  0   5   4   5  1   2   7  3  2   5   6  4   1  1   9   4  0
## 3    3  1 536  3  10  10   4  1   5   8  6  3   8  10  1   0  0   4   4  0
## 4    2  0   9 92   4   1   4  1   1   2  0  1   1   0  1   5  2   1   6  0
## 5    7  2  12  2 152  12   5  1   9  11  2  1   6   7  6   7  4   7   8  1
## 6    7  1   7  0   8 165   1  0   2  11  1  1   3   2  2   4  2   1   4  0
## 7    3  2   6  5   4   3 105  6   9   7  1  1   7   3  4   2  4   5  24  0
## 8    7  1   1  1   3   1   3 99   2   3  1  2   2   1  1   1  1   2   6  0
## 10   5  1   0  2   4   1   6  2 100  12  0  0   6   3  2   4  3  16   4  0
## 12  10  2  17  5  10   8   8  0   6 149  3  3  13   3  4   8  6  28   5  3
## 13   4  0   5  0   6   0   3  2   1   2 62  2   1   0  1   1  1   2   1  0
## 14   1  0   1  2   5   2   2  1   4   1  1 45   4   2  3   2  0   2   2  0
## 15  14  7   8  6  15   2   7  4   6  16  1  2 149   3  7  11  6  11   3  1
## 16   1  5   2  0   6   4   4  2   5   6  2  1   5 137  0   8  6  18   7  0
## 17   4  0   2  1   6   3   4  1   3  10  1  1   3   1 42   1  1   2   3  1
## 18   0  0   1  2   1   2   5  3   2   1  0  0   2   2  0 371  7   2   1  0
## 19   2  0   3  2  10   5   4  0   4   8  0  0   2   5  0   9 52   4  11  0
## 20  10  1   7  1  15   4   5  2   7  27  1  3  12  13  4  11  1 236  19  1
## 21   7  2   5  2   4   4  24  1   8   7  1  4   4  13  3   9  4  15 355  0
## 99   0  0   0  0   1   0   0  0   1   0  0  0   0   1  1   0  0   0   1 25
##    class.error
## 1   0.34969325
## 2   0.77380952
## 3   0.13128039
## 4   0.30827068
## 5   0.41984733
## 6   0.25675676
## 7   0.47761194
## 8   0.28260870
## 10  0.41520468
## 12  0.48797251
## 13  0.34042553
## 14  0.43750000
## 15  0.46594982
## 16  0.37442922
## 17  0.53333333
## 18  0.07711443
## 19  0.57024793
## 20  0.37894737
## 21  0.24788136
## 99  0.16666667</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">varImpPlot</span>(congress_rf_<span class="dv">200</span><span class="op">$</span>finalModel)</code></pre></div>
<p><img src="text_classification_files/figure-html/rf-varimp-1.png" width="672" /></p>
<p>And if we had a test set of observations (or a set of congressional bills never previously hand-coded), we could use this model to predict their major topic codes.</p>
</div>
<div id="session-info" class="section level1 toc-ignore">
<h1>Session Info</h1>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">devtools<span class="op">::</span><span class="kw">session_info</span>()</code></pre></div>
<pre><code>## Session info -------------------------------------------------------------</code></pre>
<pre><code>##  setting  value                       
##  version  R version 3.4.1 (2017-06-30)
##  system   x86_64, darwin15.6.0        
##  ui       X11                         
##  language (EN)                        
##  collate  en_US.UTF-8                 
##  tz       America/Chicago             
##  date     2017-11-21</code></pre>
<pre><code>## Packages -----------------------------------------------------------------</code></pre>
<pre><code>##  package      * version    date       source                              
##  assertthat     0.2.0      2017-04-11 CRAN (R 3.4.0)                      
##  backports      1.1.0      2017-05-22 CRAN (R 3.4.0)                      
##  base         * 3.4.1      2017-07-07 local                               
##  bindr          0.1        2016-11-13 CRAN (R 3.4.0)                      
##  bindrcpp     * 0.2        2017-06-17 CRAN (R 3.4.0)                      
##  boxes          0.0.0.9000 2017-07-19 Github (r-pkgs/boxes@03098dc)       
##  broom          0.4.2      2017-08-09 local                               
##  car            2.1-5      2017-07-04 CRAN (R 3.4.1)                      
##  caret        * 6.0-76     2017-04-18 CRAN (R 3.4.0)                      
##  cellranger     1.1.0      2016-07-27 CRAN (R 3.4.0)                      
##  clisymbols     1.2.0      2017-05-21 cran (@1.2.0)                       
##  codetools      0.2-15     2016-10-05 CRAN (R 3.4.1)                      
##  colorspace     1.3-2      2016-12-14 CRAN (R 3.4.0)                      
##  compiler       3.4.1      2017-07-07 local                               
##  crayon         1.3.4      2017-10-03 Github (gaborcsardi/crayon@b5221ab) 
##  datasets     * 3.4.1      2017-07-07 local                               
##  devtools       1.13.3     2017-08-02 CRAN (R 3.4.1)                      
##  digest         0.6.12     2017-01-27 CRAN (R 3.4.0)                      
##  dplyr        * 0.7.4.9000 2017-10-03 Github (tidyverse/dplyr@1a0730a)    
##  evaluate       0.10.1     2017-06-24 CRAN (R 3.4.1)                      
##  forcats      * 0.2.0      2017-01-23 CRAN (R 3.4.0)                      
##  foreach        1.4.3      2015-10-13 CRAN (R 3.4.0)                      
##  foreign        0.8-69     2017-06-22 CRAN (R 3.4.1)                      
##  ggplot2      * 2.2.1      2016-12-30 CRAN (R 3.4.0)                      
##  glue           1.1.1      2017-06-21 CRAN (R 3.4.1)                      
##  graphics     * 3.4.1      2017-07-07 local                               
##  grDevices    * 3.4.1      2017-07-07 local                               
##  grid           3.4.1      2017-07-07 local                               
##  gtable         0.2.0      2016-02-26 CRAN (R 3.4.0)                      
##  haven          1.1.0      2017-07-09 CRAN (R 3.4.1)                      
##  hms            0.3        2016-11-22 CRAN (R 3.4.0)                      
##  htmltools      0.3.6      2017-04-28 CRAN (R 3.4.0)                      
##  httr           1.3.1      2017-08-20 CRAN (R 3.4.1)                      
##  iterators      1.0.8      2015-10-13 CRAN (R 3.4.0)                      
##  janeaustenr    0.1.5      2017-06-10 CRAN (R 3.4.0)                      
##  jsonlite       1.5        2017-06-01 CRAN (R 3.4.0)                      
##  knitr          1.17       2017-08-10 cran (@1.17)                        
##  lattice      * 0.20-35    2017-03-25 CRAN (R 3.4.1)                      
##  lazyeval       0.2.0      2016-06-12 CRAN (R 3.4.0)                      
##  lme4           1.1-13     2017-04-19 CRAN (R 3.4.0)                      
##  lubridate      1.6.0      2016-09-13 CRAN (R 3.4.0)                      
##  magrittr       1.5        2014-11-22 CRAN (R 3.4.0)                      
##  MASS           7.3-47     2017-02-26 CRAN (R 3.4.1)                      
##  Matrix         1.2-11     2017-08-16 CRAN (R 3.4.1)                      
##  MatrixModels   0.4-1      2015-08-22 CRAN (R 3.4.0)                      
##  memoise        1.1.0      2017-04-21 CRAN (R 3.4.0)                      
##  methods      * 3.4.1      2017-07-07 local                               
##  mgcv           1.8-18     2017-07-28 CRAN (R 3.4.1)                      
##  minqa          1.2.4      2014-10-09 CRAN (R 3.4.0)                      
##  mnormt         1.5-5      2016-10-15 CRAN (R 3.4.0)                      
##  ModelMetrics   1.1.0      2016-08-26 CRAN (R 3.4.0)                      
##  modelr         0.1.1      2017-08-10 local                               
##  munsell        0.4.3      2016-02-13 CRAN (R 3.4.0)                      
##  nlme           3.1-131    2017-02-06 CRAN (R 3.4.1)                      
##  nloptr         1.0.4      2014-08-04 CRAN (R 3.4.0)                      
##  NLP          * 0.1-11     2017-08-15 CRAN (R 3.4.1)                      
##  nnet           7.3-12     2016-02-02 CRAN (R 3.4.1)                      
##  parallel       3.4.1      2017-07-07 local                               
##  pbkrtest       0.4-7      2017-03-15 CRAN (R 3.4.0)                      
##  pkgconfig      2.0.1      2017-03-21 CRAN (R 3.4.0)                      
##  plyr           1.8.4      2016-06-08 CRAN (R 3.4.0)                      
##  psych          1.7.5      2017-05-03 CRAN (R 3.4.1)                      
##  purrr        * 0.2.3      2017-08-02 CRAN (R 3.4.1)                      
##  quantreg       5.33       2017-04-18 CRAN (R 3.4.0)                      
##  R6             2.2.2      2017-06-17 CRAN (R 3.4.0)                      
##  randomForest * 4.6-12     2015-10-07 CRAN (R 3.4.0)                      
##  Rcpp           0.12.13    2017-09-28 cran (@0.12.13)                     
##  readr        * 1.1.1      2017-05-16 CRAN (R 3.4.0)                      
##  readxl         1.0.0      2017-04-18 CRAN (R 3.4.0)                      
##  reshape2       1.4.2      2016-10-22 CRAN (R 3.4.0)                      
##  rlang          0.1.2      2017-08-09 CRAN (R 3.4.1)                      
##  rmarkdown      1.6        2017-06-15 CRAN (R 3.4.0)                      
##  rprojroot      1.2        2017-01-16 CRAN (R 3.4.0)                      
##  rstudioapi     0.6        2016-06-27 CRAN (R 3.4.0)                      
##  rvest          0.3.2      2016-06-17 CRAN (R 3.4.0)                      
##  scales         0.4.1      2016-11-09 CRAN (R 3.4.0)                      
##  slam           0.1-40     2016-12-01 CRAN (R 3.4.0)                      
##  SnowballC      0.5.1      2014-08-09 CRAN (R 3.4.0)                      
##  SparseM        1.77       2017-04-23 CRAN (R 3.4.0)                      
##  splines        3.4.1      2017-07-07 local                               
##  stats        * 3.4.1      2017-07-07 local                               
##  stats4         3.4.1      2017-07-07 local                               
##  stringi        1.1.5      2017-04-07 CRAN (R 3.4.0)                      
##  stringr      * 1.2.0      2017-02-18 CRAN (R 3.4.0)                      
##  tibble       * 1.3.4      2017-08-22 CRAN (R 3.4.1)                      
##  tidyr        * 0.7.0      2017-08-16 CRAN (R 3.4.1)                      
##  tidytext     * 0.1.3      2017-06-19 CRAN (R 3.4.1)                      
##  tidyverse    * 1.1.1.9000 2017-07-19 Github (tidyverse/tidyverse@a028619)
##  tm           * 0.7-1      2017-03-02 CRAN (R 3.4.0)                      
##  tokenizers     0.1.4      2016-08-29 CRAN (R 3.4.0)                      
##  tools          3.4.1      2017-07-07 local                               
##  utils        * 3.4.1      2017-07-07 local                               
##  withr          2.0.0      2017-07-28 CRAN (R 3.4.1)                      
##  xml2           1.1.1      2017-01-24 CRAN (R 3.4.0)                      
##  yaml           2.1.14     2016-11-12 CRAN (R 3.4.0)</code></pre>
</div>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>We use <code>weightTfIdf()</code> from the <code>tm</code> package to calculate the new weights. <a href="http://tm.r-forge.r-project.org/"><code>tm</code></a> is a robust package in R for text mining and has many useful features for text analysis (though is not part of the <code>tidyverse</code>, so it may take some familiarization).<a href="#fnref1">â©</a></p></li>
<li id="fn2"><p>See <a href="http://tidytextmining.com/tfidf.html">here</a> for a more in-depth explanation of this approach.<a href="#fnref2">â©</a></p></li>
<li id="fn3"><p>Notice our effort to remove numbers was not exactly perfect, but it probably removed a good portion of them.<a href="#fnref3">â©</a></p></li>
</ol>
</div>

<p>This work is licensed under the  <a href="http://creativecommons.org/licenses/by-nc/4.0/">CC BY-NC 4.0 Creative Commons License</a>.</p>


</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
