<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />


<meta name="author" content="MACS 30200 - Perspectives on Computational Research" />


<title>Multilevel data</title>

<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/readable.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="site_libs/jqueryui-1.11.4/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<script src="site_libs/navigation-1.1/codefolding.js"></script>
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-45631879-2', 'auto');
  ga('send', 'pageview');

</script>


<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
div.sourceCode {
  overflow-x: visible;
}
</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>


<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>


</head>

<body>

<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
.tabbed-pane {
  padding-top: 12px;
}
button.code-folding-btn:focus {
  outline: none;
}
</style>


<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 66px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 71px;
  margin-top: -71px;
}

.section h2 {
  padding-top: 71px;
  margin-top: -71px;
}
.section h3 {
  padding-top: 71px;
  margin-top: -71px;
}
.section h4 {
  padding-top: 71px;
  margin-top: -71px;
}
.section h5 {
  padding-top: 71px;
  margin-top: -71px;
}
.section h6 {
  padding-top: 71px;
  margin-top: -71px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>


<div class="container-fluid main-container">

<!-- tabsets -->
<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});
</script>

<!-- code folding -->
<style type="text/css">
.code-folding-btn { margin-bottom: 4px; }
</style>
<script>
$(document).ready(function () {
  window.initializeCodeFolding("hide" === "show");
});
</script>




<script>
$(document).ready(function ()  {

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_').toLowerCase();
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}


.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
  padding-left: 25px;
  text-indent: 0;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>

<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row-fluid">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">Computing for the Social Sciences</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">Home</a>
</li>
<li>
  <a href="faq.html">FAQ</a>
</li>
<li>
  <a href="syllabus.html">Syllabus</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div class="fluid-row" id="header">

<div class="btn-group pull-right">
<button type="button" class="btn btn-default btn-xs dropdown-toggle" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false"><span>Code</span> <span class="caret"></span></button>
<ul class="dropdown-menu" style="min-width: 50px;">
<li><a id="rmd-show-all-code" href="#">Show All Code</a></li>
<li><a id="rmd-hide-all-code" href="#">Hide All Code</a></li>
</ul>
</div>



<h1 class="title toc-ignore">Multilevel data</h1>
<h4 class="author"><em>MACS 30200 - Perspectives on Computational Research</em></h4>

</div>


<div id="objectives" class="section level1">
<h1>Objectives</h1>
<ul>
<li>Identify types of multilevel data structures</li>
<li>Explain why classical GLM regression models don’t work with multilevel data</li>
<li>Introduce random effects models</li>
<li>Estimate a random effects model using the partisan defection dataset</li>
<li>Identify limitations and drawbacks to MLM with traditional approximation methods</li>
</ul>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(tidyverse)
<span class="kw">library</span>(broom)
<span class="kw">library</span>(forcats)
<span class="kw">library</span>(modelr)
<span class="kw">library</span>(stringr)
<span class="kw">library</span>(car)
<span class="kw">library</span>(rcfss)
<span class="kw">library</span>(coefplot)
<span class="kw">library</span>(RColorBrewer)
<span class="kw">library</span>(lme4)

<span class="kw">options</span>(<span class="dt">digits =</span> <span class="dv">3</span>)
<span class="kw">set.seed</span>(<span class="dv">1234</span>)
<span class="kw">theme_set</span>(<span class="kw">theme_minimal</span>())</code></pre></div>
</div>
<div id="what-influences-partisan-defection-in-voters" class="section level1">
<h1>What influences partisan defection in voters?</h1>
<p>What motivates partisan individuals to cross party lines and vote for a presidential candidate from the opposition? In an era of heightened partisan polarization Democrats and Republicans are increasingly reluctant to cast ballots for their partisan opponents, making these individuals even stronger of outliers. While only a relatively small number of self-identified partisan individuals cast a defecting vote in the United States, the outcome of closely contested elections may be decided by these actions. Understanding individual motivations to cast aside partisanship improves our understanding of how deeply PID influences individual political behavior.</p>
<p>Based upon previous studies of partisan defection, I formed several hypotheses for how these factors influence vote selection in partisan individuals:</p>
<ul>
<li><strong>Partisan intensity</strong> - individuals who more strongly identify with a political party have a lower probability of defecting. If partisan identification is related to vote choice and individuals can possess different intensities of PID, stronger partisans should have more attachment to their party’s candidate and be less likely to defect compared to weak or leaning partisans. This effect should be present for individuals affiliated with either major party and can apply in any election. Since partisanship is strongly associated with vote choice and is theorized to be a core psychological predisposition, PID intensity could moderate the effects of other causes of partisan defection.</li>
<li><strong>Relative favorability</strong> - as the perceived favorability of an opposing party’s candidate increases relative to the own party’s candidate, the probability of a voter’s partisan defection increases. Under certain circumstances short-term influences have been shown to significantly impact an individual’s vote choice. Intangible traits such as likability or relative warmth might conceivably influence individual vote decisions. Cited as the “with whom would you like to have a beer?” test, political commentators and scholars have explored this relationship with mixed results. If the opposing party’s candidate seems more appealing, regardless of policy views, individuals may be persuaded to defect for the more charismatic candidate. This might potentially explain specific episodes of defection such as the Reagan Democrats in 1984 and Republicans for Obama in 2008 where one candidate was perceived as more charismatic than the other.</li>
<li><strong>Correct is defect</strong> - if an individual’s correct vote decision requires him to defect, then the probability of defection increases. Correct voting decision factors include policy preferences, evaluation’s of candidates’ traits, and social group identification. As such, correct voting should be, and in fact is, closely associated with actual voting decisions. If an individual’s correct vote decision requires the voter to defect, that individual should have a higher probability of actually defecting relative to voters whose correct vote decisions match their partisan preference. This influence should be relevant to all voters in any election.</li>
<li><strong>Incumbency</strong> - when the incumbent is a candidate for election, voters who identify with the opposition party have a higher probability of defecting. When an election occurs and the incumbent does not participate, incumbency advantage will have no effect (the candidate from the incumbent’s party is not expected to receive any advantage). When the incumbent does participate, the probability of defection should only increase for voters who identify with the opposition’s party. Incumbents are unlikely to consistently cause voters from their own party to defect, so under this hypothesis the probability of defection differs across parties. There should be no effect on voters affiliated with the incumbent’s party (e.g. Republicans and Reagan in 1984), but the effect should be positive for opposition party members (Democrats in ’84).</li>
</ul>
</div>
<div id="traditional-glm-issues-related-to-pooling" class="section level1">
<h1>Traditional GLM: issues related to pooling</h1>
<p><strong>Pooling</strong> is nothing more than combining data, either across units or time. In this instance, I am pooling <a href="http://www.electionstudies.org/">American National Election Studies (ANES)</a> observations over time. The key to pooling is exchangability: the notion that, conditional on the values of the covariates, any two observations within the data are considered to be the same (exchangable).</p>
<div id="why-pool" class="section level2">
<h2>Why Pool?</h2>
<p>Several reasons:</p>
<ul>
<li><strong>Pooling adds data</strong> - this is the number one reason for pooling data. If the assumption of poolability holds, we can get “better” (read: more precise) estimates of our <span class="math inline">\(\hat{\beta}\)</span>s.</li>
<li><strong>Generalizability</strong> - again, if a case can be made for model-conditional poolability, adding different cases means we can be more sure that our data generalized to broader sets of cases and/or longer time periods.</li>
<li><strong>Readability</strong> - if I estimate separate models for each election year, I have to report results for 10 different models.</li>
</ul>
<p>In this study, I want to understand how partisan defection has occurred over time, not in a single election. Instead of having 752 observations from 2004, I can utilize over 10,000 data points from 10 different elections. I’d rather estimate and report a single model than have to report results from 10 different models. Pooling the data will also produce more generalizable results and only require interpretation of a single model.</p>
</div>
<div id="issues-with-pooling" class="section level2">
<h2>Issues with pooling</h2>
<p>Implicit in this analysis, then, is the idea that the coefficients <span class="math inline">\(\beta\)</span> do not vary over subsets of the data defined along <span class="math inline">\(\mathbf{E} \in \{1972 ,\dotsc, 2008 \}\)</span>. In (say) the general, restrictive model:</p>
<p><span class="math display">\[
\begin{align}
 Pr(Y_{ei} = 1) &amp;= \text{logit}^{-1}[\alpha + \beta_{1}\text{PID Intensity}_{ei} + \beta_{2}{\text{Relative Favorability}_{ei}} \nonumber \\
 &amp;\: + \beta_{3}\text{Defect is Correct}_{ei}  + \beta_{4}\text{Incumbent Candidate}_{ei}] \nonumber \\
  \text{logit}^{-1}[x] &amp;= \frac{e^x}{1+e^x}
\end{align}
\]</span></p>
<p>the implicit assumption is one of <strong>exchangability</strong> – i.e., that all of the data come from the same <strong>regime</strong>, that is,</p>
<ul>
<li>that the process governing the relationship between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> is exactly the same for each <span class="math inline">\(e\)</span>,</li>
<li>that the process governing the relationship between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> is the same for all <span class="math inline">\(i\)</span>,</li>
<li>that the process governing the <span class="math inline">\(u\)</span>s is the same <span class="math inline">\(\forall \: e\)</span> and <span class="math inline">\(i\)</span> as well.</li>
</ul>
<p>If any of these assumptions are not true, the pooled estimator <span class="math inline">\(\hat{\beta}_P\)</span> will be biased towards the regime with:</p>
<ul>
<li>the larger <span class="math inline">\(N\)</span>,</li>
<li>the larger values of the coefficients, and/or</li>
<li>the smaller standard errors of <span class="math inline">\(\hat{\beta}\)</span></li>
</ul>
</div>
<div id="the-error-term" class="section level2">
<h2>The error term</h2>
<p>Note as well that, throughout all this discussion, we’ve been assuming that the error term <span class="math inline">\(u_{it}\)</span> is homoscedastic and uncorrelated, both within and across <span class="math inline">\(i\)</span> and <span class="math inline">\(t\)</span>. Formally, that means we need to have:</p>
<p><span class="math display">\[u_{ij} \sim i.i.d.N(0, \sigma^2), \: \forall\: i,j\]</span></p>
<p>This is exactly what we’ve done in the previous model specifications with <span class="math inline">\(\sigma^2_y\)</span>. If you stop and think about it, that’s a pretty tall order. In particular, it requires that:</p>
<p><span class="math display">\[
\begin{align}
Var(u_{ab}) &amp;= Var(u_{cb}) \: \forall \: a \neq b  \text{ (i.e., no cross-unit heteroscedasticity)},  \\
Var(u_{ab}) &amp;= Var(u_{ac}) \: \forall \: b \neq c \text{ (i.e., no temporal heteroscedasticity)},  \\
Cov(u_{ab},u_{cd}) &amp;= 0 \: \forall \: a \neq c,\: \forall \: b \neq d \text{ (i.e., no auto- or spatial correlation)}
\end{align}
\]</span></p>
<p>Remember: Residuals are (among other things) just an indicator of how good a job the model does of explaining <span class="math inline">\(Y\)</span> with <span class="math inline">\(\mathbf{X}\)</span>. In that light, these assumptions are violated if (for example):</p>
<ul>
<li>Cross-unit differences mean that the model does a better job of explaining some units than others,</li>
<li>Time effects (such as socialization, institutionalization, learning, or other such dynamics) cause the model to do a better or worse job of explaining <span class="math inline">\(Y\)</span> over time,</li>
<li>Omitted variables lead to residual correlation, either across units or (more commonly) over time.</li>
</ul>
<p>While in a linear model, at least, problems with the error term don’t bias coefficient estimates, they can screw up one’s inferences pretty badly. And in nonlinear models (i.e. logits and all other GLMs) they can also lead to biases in the point estimates as well.</p>
<p>Additionally, pooling can lead to biased standard errors. This is due to the assumption of GLMs that the errors for each observation are independent of one another. Formally, <span class="math inline">\(E(u_i*u_j)=0 \: \forall \: i,j \: \text{combinations}\)</span>. If this assumption is violated and errors in some observations are related to the errors in other observations (likely within election year), the model appears to have more power than it otherwise should. This will create artificially deflated standard errors and make point estimates appear more precise.</p>
</div>
<div id="simulation-of-bad-pooling" class="section level2">
<h2>Simulation of bad pooling</h2>
<p><span class="math display">\[
\begin{align}
y_{i} &amp;\sim N(\alpha_{j[i]} + \beta x_{i},\sigma^2_y), &amp;\text{for } i=1,\dotsc,n \nonumber \\
\alpha_j &amp;\sim N(\mu_{\alpha},\sigma^2_{\alpha}), &amp;\text{for } j=1,\dotsc,J
\end{align}
\]</span></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">obs &lt;-<span class="st"> </span><span class="dv">60</span>
group.obs &lt;-<span class="st"> </span><span class="dv">6</span>
groups &lt;-<span class="st"> </span><span class="kw">rep</span>(<span class="dv">1</span><span class="op">:</span>group.obs, <span class="dt">times =</span> obs <span class="op">/</span><span class="st"> </span>group.obs)

<span class="co">#varying intercept</span>
beta &lt;-<span class="st"> </span><span class="dv">2</span>
alpha &lt;-<span class="st"> </span><span class="kw">rnorm</span>(group.obs,<span class="dv">2</span>,<span class="dv">2</span>)

vary_int &lt;-<span class="st"> </span><span class="kw">data_frame</span>(<span class="dt">x =</span> <span class="kw">runif</span>(obs,<span class="dv">0</span>,<span class="dv">3</span>),
                       <span class="dt">y =</span> alpha <span class="op">+</span><span class="st"> </span>beta <span class="op">*</span><span class="st"> </span>x,
                       <span class="dt">groups =</span> <span class="kw">factor</span>(groups))

p &lt;-<span class="st"> </span><span class="kw">ggplot</span>(vary_int, <span class="kw">aes</span>(x, y)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_point</span>(<span class="kw">aes</span>(<span class="dt">color =</span> groups, <span class="dt">shape =</span> groups)) <span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="st">&quot;Simulated data with varying intercepts&quot;</span>,
       <span class="dt">x =</span> <span class="kw">expression</span>(X),
       <span class="dt">y =</span> <span class="kw">expression</span>(Y)) <span class="op">+</span>
<span class="st">  </span><span class="kw">theme</span>(<span class="dt">legend.position =</span> <span class="st">&quot;none&quot;</span>)
p</code></pre></div>
<p><img src="persp015_multilevel_data_files/figure-html/sim-pool-1.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">p <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_smooth</span>(<span class="dt">method =</span> <span class="st">&quot;lm&quot;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">subtitle =</span> <span class="st">&quot;OLS&quot;</span>)</code></pre></div>
<p><img src="persp015_multilevel_data_files/figure-html/sim-pool-2.png" width="672" /></p>
</div>
<div id="partisan-defection-and-pooling-issues" class="section level2">
<h2>Partisan defection and pooling issues</h2>
<p>Am I likely to violate any of these assumptions using this pooled dataset? Yes. The mean probability of partisan defection differs significantly across elections.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(haven)

<span class="co"># read in data</span>
anes &lt;-<span class="st"> </span><span class="kw">read_dta</span>(<span class="st">&quot;data/anes_pres.dta&quot;</span>)

<span class="co"># generate variables</span>
anes &lt;-<span class="st"> </span>anes <span class="op">%&gt;%</span>
<span class="st">  </span><span class="co">#generate binary party measures</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">rep =</span> <span class="kw">ifelse</span>(pid <span class="op">&lt;</span><span class="st"> </span><span class="dv">4</span>, <span class="dv">0</span>,
                      <span class="kw">ifelse</span>(pid <span class="op">&gt;</span><span class="st"> </span><span class="dv">4</span>, <span class="dv">1</span>, <span class="ot">NA</span>))) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="co">#generate measure of defection and whether or not respondent actually voted correctly</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">defect =</span> <span class="ot">NA</span>,
    <span class="dt">defect =</span> <span class="kw">replace</span>(defect, <span class="kw">which</span>((pres_vote_r <span class="op">==</span><span class="st"> </span><span class="dv">0</span>) <span class="op">&amp;</span>
<span class="st">                                     </span>rep <span class="op">==</span><span class="st"> </span><span class="dv">0</span>), <span class="dv">0</span>),
    <span class="dt">defect =</span> <span class="kw">replace</span>(defect, <span class="kw">which</span>((pres_vote_r <span class="op">==</span><span class="st"> </span><span class="dv">1</span>) <span class="op">&amp;</span>
<span class="st">                                     </span>rep <span class="op">==</span><span class="st"> </span><span class="dv">1</span>), <span class="dv">0</span>),
    <span class="dt">defect =</span> <span class="kw">replace</span>(defect, <span class="kw">which</span>((pres_vote_r <span class="op">==</span><span class="st"> </span><span class="dv">1</span> <span class="op">|</span><span class="st"> </span>pres_vote_r <span class="op">==</span><span class="st"> </span><span class="dv">6</span>) <span class="op">&amp;</span>
<span class="st">                                     </span>rep <span class="op">==</span><span class="st"> </span><span class="dv">0</span>), <span class="dv">1</span>),
    <span class="dt">defect =</span> <span class="kw">replace</span>(defect, <span class="kw">which</span>((pres_vote_r <span class="op">==</span><span class="st"> </span><span class="dv">0</span> <span class="op">|</span><span class="st"> </span>pres_vote_r <span class="op">==</span><span class="st"> </span><span class="dv">6</span>) <span class="op">&amp;</span>
<span class="st">                                     </span>rep <span class="op">==</span><span class="st"> </span><span class="dv">1</span>), <span class="dv">1</span>),
    <span class="dt">vote_cor_actual =</span> <span class="kw">ifelse</span>(pres_vote_r <span class="op">==</span><span class="st"> </span>vote_cor, <span class="dv">1</span>, <span class="dv">0</span>)) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="co"># does the correct vote require one to defect?</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">defect_cor =</span> <span class="kw">ifelse</span>(vote_cor <span class="op">!=</span><span class="st"> </span>rep, <span class="dv">1</span>, <span class="dv">0</span>)) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="co"># generate index of PID intensity</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">pid_abs =</span> <span class="kw">abs</span>(pid <span class="op">-</span><span class="st"> </span><span class="dv">4</span>) <span class="op">-</span><span class="st"> </span><span class="dv">1</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="co"># remove 1948</span>
<span class="st">  </span><span class="kw">filter</span>(year <span class="op">!=</span><span class="st"> </span><span class="dv">1948</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="co"># does feeling thermometer towards candidates explain defection?</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">feel_def =</span> <span class="kw">ifelse</span>(rep <span class="op">==</span><span class="st"> </span><span class="dv">1</span>, (dem_feel <span class="op">-</span><span class="st"> </span>rep_feel) <span class="op">/</span><span class="st"> </span><span class="dv">2</span>,
                           <span class="kw">ifelse</span>(rep <span class="op">==</span><span class="st"> </span><span class="dv">0</span>, (rep_feel <span class="op">-</span><span class="st"> </span>dem_feel) <span class="op">/</span><span class="st"> </span><span class="dv">2</span>, <span class="ot">NA</span>))) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="co">#does incumbency influence partisan defection?</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">inc =</span> <span class="kw">ifelse</span>(year <span class="op">%in%</span><span class="st"> </span><span class="kw">c</span>(<span class="dv">1956</span>, <span class="dv">1964</span>, <span class="dv">1972</span>, <span class="dv">1976</span>,
                                  <span class="dv">1980</span>, <span class="dv">1984</span>, <span class="dv">1992</span>, <span class="dv">1996</span>, <span class="dv">2004</span>), <span class="dv">1</span>, <span class="dv">0</span>),
         <span class="dt">inc_opp =</span> <span class="kw">ifelse</span>((rep <span class="op">==</span><span class="st"> </span><span class="dv">1</span> <span class="op">&amp;</span><span class="st"> </span>year <span class="op">%in%</span><span class="st"> </span><span class="kw">c</span>(<span class="dv">1964</span>, <span class="dv">1980</span>, <span class="dv">1996</span>)) <span class="op">|</span>
<span class="st">                            </span>(rep <span class="op">==</span><span class="st"> </span><span class="dv">0</span> <span class="op">&amp;</span><span class="st"> </span>year <span class="op">%in%</span><span class="st"> </span><span class="kw">c</span>(<span class="dv">1956</span>, <span class="dv">1972</span>, <span class="dv">1976</span>,
                                                    <span class="dv">1984</span>, <span class="dv">1992</span>, <span class="dv">2004</span>)), <span class="dv">1</span>, <span class="dv">0</span>))</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co">#generate a plot of the percentage of partisan defectors in each election</span>
##get weighted proportions for all, dems only, and reps only
anes <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">group_by</span>(year) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">summarize</span>(<span class="dt">defect =</span> <span class="kw">weighted.mean</span>(defect, std_wt, <span class="dt">na.rm =</span> <span class="ot">TRUE</span>)) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(year, defect)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_col</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">scale_y_continuous</span>(<span class="dt">labels =</span> scales<span class="op">::</span>percent) <span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="st">&quot;Percentage of partisan defectors&quot;</span>,
       <span class="dt">x =</span> <span class="st">&quot;Presidential election year&quot;</span>,
       <span class="dt">y =</span> <span class="st">&quot;Percentage of partisan defectors&quot;</span>)</code></pre></div>
<p><img src="persp015_multilevel_data_files/figure-html/prop-defect-1.png" width="672" /></p>
<p>Unless the composition of the electorate is changing significantly from year-to-year in a specific way, this assumption will not hold. Regardless, this assumption should be tested and explored, which is not possible under a standard GLM approach.</p>
</div>
</div>
<div id="multilevel-data-structures" class="section level1">
<h1>Multilevel data structures</h1>
<p>Consider the classical linear model:</p>
<p><span class="math display">\[y_{i} \sim N(\alpha + \beta x_{i}, \sigma^2_{y}), \: \text{for}\: i=1,\dotsc,n\]</span></p>
<p>Implicitly, this model assumes (along with classical OLS assumptions):</p>
<ul>
<li>That the constant term is constant across different <span class="math inline">\(i\)</span>s, and</li>
<li>That the effect of any given variable <span class="math inline">\(X\)</span> on <span class="math inline">\(Y\)</span> is constant across observations<a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a></li>
</ul>
<p>We can write a similar model in the panel context as follows:</p>
<p><span class="math display">\[y_{ij} \sim N(\alpha + \beta x_{ij}, \sigma^2_{y}), \: \text{for}\: i=1,\dotsc,n; j = 1, \dotsc,k\]</span></p>
<p>Note that this model assumes the same things as the earlier ones, especially about the effects of constants and covariates.</p>
<p>In <strong>any</strong> regression context, the two assumptions mentioned are critical; violating them leads to a form of specification bias. In the panel context, these two assumptions are often going to be problematic. This is because, since we’re observing multiple units over time or across groups, there’s often (in fact, usually) some reason to believe that there may be differences in either <span class="math inline">\(\alpha\)</span> or <span class="math inline">\(\beta\)</span> over either <span class="math inline">\(i\)</span> or <span class="math inline">\(j\)</span>. How could we correct for these possibilities?</p>
<div id="variable-intercepts" class="section level2">
<h2>Variable intercepts</h2>
<p>One possible violation of the above assumptions is that the intercepts vary. The most common way this occurs is for different units to have varying intercepts:</p>
<p><span class="math display">\[y_{ij} \sim N(\alpha_i + \beta x_{ij}, \sigma^2_{y}), \: for\: i=1,\dotsc,n\]</span></p>
<p>The slopes for each unit are the same, but the intercepts are different. It’s also possible that the intercepts vary over time, rather than over units:</p>
<p><span class="math display">\[y_{ij} \sim N(\alpha_t + \beta x_{ij}, \sigma^2_{y}), \: for\: i=1,\dotsc,n\]</span></p>
<p>If we have data that corresponds to the first formula, but estimate a model like the second formula, we can get biased coefficients.</p>
</div>
<div id="variable-slopes" class="section level2">
<h2>Variable slopes</h2>
<p>The other obvious possibility is that we have a constant intercept, but the effects of <span class="math inline">\(X\)</span> on <span class="math inline">\(Y\)</span> differs across either units or (less likely) time:</p>
<p><span class="math display">\[y_{it} \sim N(\alpha + \beta_{i} x_{ij}, \sigma^2_{y}), \: for\: i=1,\dotsc,n\]</span></p>
<p>We could also have variation in <span class="math inline">\(\beta\)</span> over time, or even over both units and time.</p>
<p>A model like above assumes that the regression lines all pass through the same point on the <span class="math inline">\(Y\)</span>-axis, but that their slopes differ. The idea of a common intercept, however, is a bit strange (at least to social scientists). Instead what is more likely…</p>
</div>
<div id="variable-slopes-and-intercepts" class="section level2">
<h2>Variable slopes and intercepts</h2>
<p>This is when things really start to get difficult. We might, for example, have variable slopes and intercepts for each unit <span class="math inline">\(i\)</span>:</p>
<p><span class="math display">\[y_{ij} \sim N(\alpha_{i} + \beta_{i} x_{ij}, \sigma^2_{y}), \: for\: i=1,\dotsc,n\]</span></p>
<p>Moreover, we could instead have different <span class="math inline">\(\alpha\)</span>s and <span class="math inline">\(\beta\)</span>s for every time point, rather than for every unit:</p>
<p><span class="math display">\[y_{ij} \sim N(\alpha_{t} + \beta_{j} x_{ij}, \sigma^2_{y}), \: for\: i=1,\dotsc,n\]</span></p>
<p>or for both different units and time points:</p>
<p><span class="math display">\[y_{ij} \sim N(\alpha_{it} + \beta_{it} x_{ij}, \sigma^2_{y}), \: for\: i=1,\dotsc,n\]</span></p>
<p>Using the incorrect model specification for the data can lead to odd, even nonsensical, results: underestimating some slopes, overestimating others, and in some cases even getting the sign wrong.</p>
<p>All of this leads to…</p>
</div>
</div>
<div id="multilevel-modeling" class="section level1">
<h1>Multilevel modeling</h1>
<p>What many social scientists refer to as <strong>random effects</strong>, <a href="http://www.stat.columbia.edu/~gelman/arm/">Gelman and Hill</a> simply call <strong>multilevel modeling</strong>. <strong>Fixed effects</strong> are usually defined as varying coefficients that are not themselves modeled. For example, a classical regression including <span class="math inline">\(J-1\)</span> unit indicators as regression predictors is sometimes called a <strong>fixed-effects model</strong>. As we will see, fixed effects are a subtype of random effects; I (and others) recommend always using random effects and focusing on the description of the model itself (for example, varying intercepts and constant slopes), with the understanding that batches of coefficients (for example, <span class="math inline">\(\alpha_1,\dotsc,\alpha_J\)</span>) will themselves be modeled.</p>
<div id="fixed-effects" class="section level2">
<h2>Fixed effects</h2>
<p><strong>Fixed effects</strong> has several potential meanings in a regression model. Most commonly, fixed effects are when an input variable with <span class="math inline">\(J\)</span> categories is incorporated into a regression model as <span class="math inline">\(J-1\)</span> dummy (dichotomous) variables, with one category held out to serve as the baseline. In essence, this adds a coefficient for each category. The goal of this is to parse out all between-category effects on <span class="math inline">\(Y\)</span>, allowing the remaining variables to explain only within-category effects.</p>
<p>This approach is fine (assuming <span class="math inline">\(J\)</span> is small relative to <span class="math inline">\(n\)</span>) as long as you have no category-level predictors or explanatory variables in the model. So in our partisan defection model, this is fine if we include fixed effects for election year and test partisan intensity, relative favorability, and correct is defect hypotheses:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">basic.fix &lt;-<span class="st"> </span><span class="kw">glm</span>(defect <span class="op">~</span><span class="st"> </span>pid_abs <span class="op">+</span><span class="st"> </span>feel_def <span class="op">+</span><span class="st"> </span>defect_cor <span class="op">+</span><span class="st"> </span><span class="kw">factor</span>(year),
             <span class="dt">data =</span> anes, <span class="dt">family =</span> <span class="kw">binomial</span>(<span class="dt">link =</span> <span class="st">&quot;logit&quot;</span>))
<span class="kw">tidy</span>(basic.fix)</code></pre></div>
<pre><code>##                term estimate std.error statistic   p.value
## 1       (Intercept)   -0.636   0.14228    -4.467  7.94e-06
## 2           pid_abs   -0.289   0.04995    -5.795  6.82e-09
## 3          feel_def    0.121   0.00324    37.324 6.61e-305
## 4        defect_cor    0.841   0.07826    10.752  5.77e-27
## 5  factor(year)1976   -0.343   0.18432    -1.863  6.24e-02
## 6  factor(year)1980    0.328   0.16738     1.961  4.99e-02
## 7  factor(year)1984   -0.164   0.16889    -0.974  3.30e-01
## 8  factor(year)1988   -0.275   0.17268    -1.592  1.11e-01
## 9  factor(year)1992    0.864   0.15178     5.692  1.26e-08
## 10 factor(year)1996    0.213   0.17052     1.249  2.12e-01
## 11 factor(year)2000   -0.708   0.18465    -3.836  1.25e-04
## 12 factor(year)2004   -0.403   0.21648    -1.863  6.25e-02
## 13 factor(year)2008   -0.683   0.17907    -3.815  1.36e-04</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">coefplot</span>(basic.fix,
         <span class="dt">title =</span> <span class="st">&quot;Fixed effects GLM of partisan defection&quot;</span>,
         <span class="dt">newNames =</span> <span class="kw">c</span>(<span class="st">&quot;pid_abs&quot;</span> =<span class="st"> &quot;PID intensity&quot;</span>,
                      <span class="st">&quot;feel_def&quot;</span> =<span class="st"> &quot;Relative favorability&quot;</span>,
                      <span class="st">&quot;defect_cor&quot;</span> =<span class="st"> &quot;Defect is correct&quot;</span>,
                      <span class="st">&quot;inc&quot;</span> =<span class="st"> &quot;Incumbent candidate&quot;</span>,
                      <span class="st">&quot;factor(year)1976&quot;</span> =<span class="st"> &quot;1976&quot;</span>,
                      <span class="st">&quot;factor(year)1980&quot;</span> =<span class="st"> &quot;1980&quot;</span>,
                      <span class="st">&quot;factor(year)1984&quot;</span> =<span class="st"> &quot;1984&quot;</span>,
                      <span class="st">&quot;factor(year)1988&quot;</span> =<span class="st"> &quot;1988&quot;</span>,
                      <span class="st">&quot;factor(year)1992&quot;</span> =<span class="st"> &quot;1992&quot;</span>,
                      <span class="st">&quot;factor(year)1996&quot;</span> =<span class="st"> &quot;1996&quot;</span>,
                      <span class="st">&quot;factor(year)2000&quot;</span> =<span class="st"> &quot;2000&quot;</span>,
                      <span class="st">&quot;factor(year)2004&quot;</span> =<span class="st"> &quot;2004&quot;</span>,
                      <span class="st">&quot;factor(year)2008&quot;</span> =<span class="st"> &quot;2008&quot;</span>),
         <span class="dt">decreasing =</span> <span class="ot">TRUE</span>)</code></pre></div>
<p><img src="persp015_multilevel_data_files/figure-html/fixed-eff-1.png" width="672" /></p>
<p>We estimate coefficients for the three explanatory variables, as well as dummy coefficients for each election in the model.<a href="#fn2" class="footnoteRef" id="fnref2"><sup>2</sup></a> Each dummy coefficient represents the comparison to the baseline category, 1972. For example, the odds of partisan defection in the 2000 election was 0.492 of the odds of defection in 1972.</p>
<p>One major problem with this approach is that is assumes all the variation in the outcome of interest is systematically explainable only by individual-level variables. That is, we cannot add group-level predictors to the model:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">summary</span>(<span class="kw">glm</span>(defect <span class="op">~</span><span class="st"> </span>pid_abs <span class="op">+</span><span class="st"> </span>feel_def <span class="op">+</span><span class="st"> </span>defect_cor <span class="op">+</span><span class="st"> </span>inc <span class="op">+</span><span class="st"> </span><span class="kw">factor</span>(year),
            <span class="dt">data =</span> anes, <span class="dt">family =</span> <span class="kw">binomial</span>(<span class="dt">link =</span> <span class="st">&quot;logit&quot;</span>)))</code></pre></div>
<pre><code>## 
## Call:
## glm(formula = defect ~ pid_abs + feel_def + defect_cor + inc + 
##     factor(year), family = binomial(link = &quot;logit&quot;), data = anes)
## 
## Deviance Residuals: 
##    Min      1Q  Median      3Q     Max  
## -3.089  -0.403  -0.188  -0.065   3.527  
## 
## Coefficients: (1 not defined because of singularities)
##                  Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept)      -1.31876    0.13258   -9.95  &lt; 2e-16 ***
## pid_abs          -0.28948    0.04995   -5.80  6.8e-09 ***
## feel_def          0.12097    0.00324   37.32  &lt; 2e-16 ***
## defect_cor        0.84148    0.07826   10.75  &lt; 2e-16 ***
## inc               0.68320    0.17907    3.82  0.00014 ***
## factor(year)1976 -0.34340    0.18432   -1.86  0.06244 .  
## factor(year)1980  0.32818    0.16738    1.96  0.04992 *  
## factor(year)1984 -0.16445    0.16889   -0.97  0.33020    
## factor(year)1988  0.40838    0.16883    2.42  0.01557 *  
## factor(year)1992  0.86387    0.15178    5.69  1.3e-08 ***
## factor(year)1996  0.21304    0.17052    1.25  0.21153    
## factor(year)2000 -0.02518    0.18129   -0.14  0.88953    
## factor(year)2004 -0.40321    0.21648   -1.86  0.06252 .  
## factor(year)2008       NA         NA      NA       NA    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 9118.6  on 10380  degrees of freedom
## Residual deviance: 5164.6  on 10368  degrees of freedom
##   (7842 observations deleted due to missingness)
## AIC: 5191
## 
## Number of Fisher Scoring iterations: 6</code></pre>
<p>The problem is one of <a href="http://cfss.uchicago.edu/persp012_regression_diagnostics.html#perfect_collinearity"><strong>perfect collinearity</strong></a>:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>(anes, <span class="kw">aes</span>(year, inc)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_point</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="st">&quot;Perfect prediction&quot;</span>,
       <span class="dt">x =</span> <span class="st">&quot;Election year&quot;</span>,
       <span class="dt">y =</span> <span class="st">&quot;Incumbent in election&quot;</span>)</code></pre></div>
<p><img src="persp015_multilevel_data_files/figure-html/year-inc-1.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># correlation coefficient, by year</span>
anes <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">group_by</span>(year) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">summarize</span>(<span class="dt">cor =</span> <span class="kw">cor</span>(year, inc))</code></pre></div>
<pre><code>## # A tibble: 15 x 2
##     year   cor
##    &lt;dbl&gt; &lt;lgl&gt;
##  1  1952    NA
##  2  1956    NA
##  3  1960    NA
##  4  1964    NA
##  5  1968    NA
##  6  1972    NA
##  7  1976    NA
##  8  1980    NA
##  9  1984    NA
## 10  1988    NA
## 11  1992    NA
## 12  1996    NA
## 13  2000    NA
## 14  2004    NA
## 15  2008    NA</code></pre>
<p>Because incumbency doesn’t vary at the election-level, it is completely constant within elections. You cannot even estimate a correlation coefficient because <code>inc</code> is a constant! Therefore R has to drop one of the dummy variables just to estimate the model.</p>
</div>
<div id="random-effects" class="section level2">
<h2>Random effects</h2>
<p>Multilevel regression can be thought of as a method for compromising between the two extremes of excluding a categorical predictor from a model (<strong>complete pooling</strong>), or estimating separate models within each level of the categorical predictor (<strong>no pooling</strong>). The former ignores within-unit variation, while the latter ignores between-unit variation. Additionally, the no pooling approach prevents the researcher from conducting inference using categorical predictors.</p>
<p>Instead, multilevel modeling uses a compromise between these two approaches. A <strong>soft constraint</strong> is applied to the coefficients: they are assigned a probability distribution.</p>
<div id="varying-intercepts" class="section level3">
<h3>Varying intercepts</h3>
<p>A varying intercept model could take the following form:</p>
<p><span class="math display">\[
\begin{align}
y_i &amp;\sim N(\alpha_{j[i]} + \beta x_{i}, \sigma^2_{y}), \: \text{for} \: i=1,\dotsc,n, \nonumber \\
\alpha_{j} &amp;\sim N(\mu_{\alpha},\sigma^2_{\alpha}), \: \text{for} \: j=1,\dotsc,J,
\end{align}
\]</span></p>
<p><span class="math inline">\(\alpha_{j}\)</span> is now assumed to be distributed normally with a mean <span class="math inline">\(\mu_{\alpha}\)</span> and variance <span class="math inline">\(\sigma^2_{\alpha}\)</span> estimated from the data. This pools the estimates of <span class="math inline">\(\alpha_{j}\)</span> toward the mean level <span class="math inline">\(\mu_{\alpha}\)</span>, but not all the way; thus, group-level estimates are the result of a <strong>partial-pooling</strong> compromise. In the limit of <span class="math inline">\(\sigma_{\alpha} \to \infty\)</span>, the soft constraints do nothing, and there is no pooling; as <span class="math inline">\(\sigma_{\alpha} \to 0\)</span>, they pull the estimates all the way to zero, yielding the complete-pooling estimate. This demonstrates that even if there is no dependence in the errors between groups, multilevel regression will still produce accurate point estimates and standard errors. <strong>Instead of making an assumption about independence, we can directly model it and let the data tell us if any exists.</strong></p>
<p>This approach also reduces the number of parameters needing to be estimated. For example, a fixed effects approach with 20 groups would require estimating <span class="math inline">\(J-1\)</span> parameters (19). Introducing a soft-constraint and modeling <span class="math inline">\(\alpha\)</span> as a random draw from a single distribution, all we need to estimate is the parameters for this distribution. Typically we assume that the distribution is normal, so we only need to estimate two parameters (the mean <span class="math inline">\(\mu\)</span> and the variance <span class="math inline">\(\sigma^2\)</span>). This increases our degrees of freedom (which could be an issue if we have lots of groups in the dataset). As seen below, this also allows us to introduce group-level predictors into the model.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co">#draw x and c from random normal distribution, adjusting for group differences</span>
x &lt;-<span class="st"> </span><span class="kw">runif</span>(obs, <span class="dv">0</span>, <span class="dv">3</span>) <span class="op">+</span><span class="st"> </span>groups
c &lt;-<span class="st"> </span><span class="kw">rep</span>(<span class="kw">rnorm</span>(group.obs, <span class="dv">2</span>, <span class="dv">1</span>), <span class="dt">times =</span> obs <span class="op">/</span><span class="st"> </span>group.obs) <span class="op">*</span><span class="st"> </span>groups
beta &lt;-<span class="st"> </span><span class="dv">2</span>       <span class="co">#set beta</span>
y &lt;-<span class="st"> </span>beta<span class="op">*</span>(x) <span class="op">-</span><span class="st"> </span>c <span class="co">#+ rnorm(obs,0,1)     #generate y from x, c, beta, and random noise</span>

sim_data_mlm &lt;-<span class="st"> </span><span class="kw">data_frame</span>(x, y, groups)

<span class="co"># plot data</span>
<span class="kw">ggplot</span>(sim_data_mlm, <span class="kw">aes</span>(x, y)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_point</span>(<span class="kw">aes</span>(<span class="dt">color =</span> <span class="kw">factor</span>(groups), <span class="dt">shape =</span> <span class="kw">factor</span>(groups))) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_smooth</span>(<span class="kw">aes</span>(<span class="dt">color =</span> <span class="kw">factor</span>(groups)), <span class="dt">method =</span> <span class="st">&quot;lm&quot;</span>, <span class="dt">se =</span> <span class="ot">FALSE</span>,
              <span class="dt">fullrange =</span> <span class="ot">TRUE</span>, <span class="dt">size =</span> .<span class="dv">5</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_smooth</span>(<span class="dt">method =</span> <span class="st">&quot;lm&quot;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="st">&quot;Simulated data with varying intercepts&quot;</span>,
       <span class="dt">x =</span> <span class="kw">expression</span>(X),
       <span class="dt">y =</span> <span class="kw">expression</span>(Y)) <span class="op">+</span>
<span class="st">  </span><span class="kw">theme</span>(<span class="dt">legend.position =</span> <span class="st">&quot;none&quot;</span>)</code></pre></div>
<p><img src="persp015_multilevel_data_files/figure-html/sim-vary-int-1.png" width="672" /></p>
<div id="adding-group-level-predictors" class="section level4">
<h4>Adding group-level predictors</h4>
<p>Perhaps we believe certain group-level variables systematically influence the baseline level of <span class="math inline">\(y_{ij}\)</span>. Using this flexible framework, we can simply include them directly in the model as predictors of <span class="math inline">\(\alpha_{J}\)</span>:</p>
<p><span class="math display">\[
\begin{align}
y_i &amp;\sim N(\alpha_{j[i]} + \beta x_{i}, \sigma^2_{y}), \: \text{for} \: i=1,\dotsc,n, \nonumber \\
\alpha_{j} &amp;\sim N(\gamma_{0} + \gamma_{1}u_{j},\sigma^2_{\alpha}), \: \text{for} \: j=1,\dotsc,J,
\end{align}
\]</span></p>
<p>where <span class="math inline">\(x_{i}\)</span> is a unit-specific indicator and <span class="math inline">\(u_{j}\)</span> is a group-level measure. Since we are leveraging multiple observations within each group, we can now include group-level measures without worrying about overspecifying the model or inducing perfect collinearity. The <span class="math inline">\(\gamma\)</span>s can be interpreted in the same manner as the <span class="math inline">\(\beta\)</span>s as they all are measured on the same dimension.</p>
<ul>
<li>For linear regression: a one-unit increase on <span class="math inline">\(u_{j}\)</span> is associated with a <span class="math inline">\(\gamma\)</span> increase in <span class="math inline">\(y_{ij}\)</span></li>
<li>For logistic regression: a one-unit increase on <span class="math inline">\(u_{j}\)</span> is associated with a <span class="math inline">\(\gamma\)</span> increase in the log-odds of the <span class="math inline">\(Pr(y_{ij}=1)\)</span>).</li>
</ul>
</div>
</div>
<div id="varying-slopes" class="section level3">
<h3>Varying slopes</h3>
<p>Now what if we believe the effects of <span class="math inline">\(\beta\)</span> vary by group? We can treat this the same as we did with the varying intercept; thus,</p>
<p><span class="math display">\[
\begin{align}
y_i &amp;\sim N(\alpha_{j[i]} + \beta_{j[i]} x_{i}, \sigma^2_{y}), \: \text{for} \: i=1,\dotsc,n, \nonumber \\
 \begin{pmatrix}
 \alpha_{j} \\
 \beta_{j} \\
 \end{pmatrix} &amp;\sim N\left(\begin{pmatrix}
 \mu_{\alpha} \\
 \mu_{\beta} \\
 \end{pmatrix},\begin{pmatrix}
 \sigma^2_{\alpha} &amp; \rho \sigma_{\alpha} \sigma_{\beta} \\
 \rho \sigma_{\alpha} \sigma_{\beta} &amp; \sigma^{2}_{\beta} &amp; \\
 \end{pmatrix}\right), \: \text{for} \: j=1,\dotsc,J,
\end{align}
\]</span></p>
<p>with variation in the <span class="math inline">\(\alpha_{j}\)</span>’s and the <span class="math inline">\(\beta_{j}\)</span>’s and also a between-group correlation parameter.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># constant intercept</span>
alpha &lt;-<span class="st"> </span><span class="dv">0</span>
beta &lt;-<span class="st"> </span><span class="kw">rnorm</span>(group.obs, <span class="dv">2</span>, <span class="dv">1</span>)
y &lt;-<span class="st"> </span>alpha <span class="op">+</span><span class="st"> </span>beta <span class="op">*</span><span class="st"> </span>x

sim_data_mlm &lt;-<span class="st"> </span><span class="kw">data_frame</span>(x, y, groups)

<span class="co"># plot data</span>
<span class="kw">ggplot</span>(sim_data_mlm, <span class="kw">aes</span>(x, y)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_point</span>(<span class="kw">aes</span>(<span class="dt">color =</span> <span class="kw">factor</span>(groups), <span class="dt">shape =</span> <span class="kw">factor</span>(groups))) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_smooth</span>(<span class="kw">aes</span>(<span class="dt">color =</span> <span class="kw">factor</span>(groups)), <span class="dt">method =</span> <span class="st">&quot;lm&quot;</span>, <span class="dt">se =</span> <span class="ot">FALSE</span>,
              <span class="dt">fullrange =</span> <span class="ot">TRUE</span>, <span class="dt">size =</span> .<span class="dv">5</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_smooth</span>(<span class="dt">method =</span> <span class="st">&quot;lm&quot;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="st">&quot;Simulated data with varying slopes&quot;</span>,
       <span class="dt">x =</span> <span class="kw">expression</span>(X),
       <span class="dt">y =</span> <span class="kw">expression</span>(Y)) <span class="op">+</span>
<span class="st">  </span><span class="kw">theme</span>(<span class="dt">legend.position =</span> <span class="st">&quot;none&quot;</span>)</code></pre></div>
<p><img src="persp015_multilevel_data_files/figure-html/sim-vary-slope-1.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># varying intercept</span>
alpha &lt;-<span class="st"> </span><span class="kw">rnorm</span>(group.obs,<span class="dv">2</span>,<span class="dv">2</span>)
y &lt;-<span class="st"> </span>alpha <span class="op">+</span><span class="st"> </span>beta <span class="op">*</span><span class="st"> </span>x

sim_data_mlm &lt;-<span class="st"> </span><span class="kw">data_frame</span>(x, y, groups)

<span class="co"># plot data</span>
<span class="kw">ggplot</span>(sim_data_mlm, <span class="kw">aes</span>(x, y)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_point</span>(<span class="kw">aes</span>(<span class="dt">color =</span> <span class="kw">factor</span>(groups), <span class="dt">shape =</span> <span class="kw">factor</span>(groups))) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_smooth</span>(<span class="kw">aes</span>(<span class="dt">color =</span> <span class="kw">factor</span>(groups)), <span class="dt">method =</span> <span class="st">&quot;lm&quot;</span>, <span class="dt">se =</span> <span class="ot">FALSE</span>,
              <span class="dt">fullrange =</span> <span class="ot">TRUE</span>, <span class="dt">size =</span> .<span class="dv">5</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_smooth</span>(<span class="dt">method =</span> <span class="st">&quot;lm&quot;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="st">&quot;Simulated data with varying intercepts and slopes&quot;</span>,
       <span class="dt">x =</span> <span class="kw">expression</span>(X),
       <span class="dt">y =</span> <span class="kw">expression</span>(Y)) <span class="op">+</span>
<span class="st">  </span><span class="kw">theme</span>(<span class="dt">legend.position =</span> <span class="st">&quot;none&quot;</span>)</code></pre></div>
<p><img src="persp015_multilevel_data_files/figure-html/sim-vary-slope-2.png" width="672" /></p>
<p>We can also include group-level predictors:</p>
<p><span class="math display">\[
\begin{align}
y_i &amp;\sim N(\alpha_{j[i]} + \beta_{j[i]} x_{i}, \sigma^2_{y}), \: \text{for} \: i=1,\dotsc,n, \nonumber \\
 \begin{pmatrix}
 \alpha_{j} \\
 \beta_{j} \\
 \end{pmatrix} &amp;\sim N\left(\begin{pmatrix}
 \gamma^{\alpha}_{0} + \gamma^{\alpha}_{1} u_{j} \\
 \gamma^{\beta}_{0} + \gamma^{\beta}_{1} u_{j} \\
 \end{pmatrix},\begin{pmatrix}
 \sigma^2_{\alpha} &amp; \rho \sigma_{\alpha} \sigma_{\beta} \\
 \rho \sigma_{\alpha} \sigma_{\beta} &amp; \sigma^{2}_{\beta} &amp; \\
 \end{pmatrix}\right), \: \text{for} \: j=1,\dotsc,J,
\end{align}
\]</span></p>
<p>In this formulation, we believe the same group-level variable explains both the intercept and the observation-specific covariate. This does not have to be true; we can include any combination of group-specific covariates in the separate models for unit-specific effects. Note that we should not use unit-specific variables in the models for <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span>. If we believe such a relationship exists, we should specify it as an interaction in the model specific to <span class="math inline">\(y_{ij}\)</span>.</p>
</div>
</div>
<div id="other-specifications-of-mlm" class="section level2">
<h2>Other specifications of MLM</h2>
<p>Multilevel modeling is an extremely flexible framework which allows researchers to introduce all sorts of interesting specifications of models. Different types of data structures and specifications include:</p>
<ul>
<li>Three or more levels of data</li>
<li>Repeated measures (i.e. panel data)</li>
<li>Time-series cross sections</li>
<li>Non-nested structures</li>
<li>Varying slopes without varying intercepts</li>
<li>Cross-level interactions</li>
<li>Discrete and non-normally distributed outcomes (e.g. dichotomous outcomes, binomial trials, count data, ordinal and multinomial outcomes)</li>
<li>Bayesian estimation procedures</li>
<li>Measuring sample size and power calculations</li>
<li>Multilevel regression, imputation, and post-stratification (MRP)</li>
</ul>
</div>
</div>
<div id="implementing-mlm-for-partisan-defection" class="section level1">
<h1>Implementing MLM for partisan defection</h1>
<p>Let the classical model be specified as</p>
<p><span class="math display">\[
\begin{align}
 Pr(Y_{ei} = 1) &amp;= \text{logit}^{-1}[\alpha + \beta_{1}\text{PID Intensity}_{ei} + \beta_{2}{\text{Relative Favorability}_{ei}} \nonumber \\
 &amp;\: + \beta_{3}\text{Defect is Correct}_{ei}  + \beta_{4}\text{Incumbent Candidate}_{e}] \nonumber \\
  \text{logit}^{-1}[x] &amp;= \frac{e^x}{1+e^x}
\end{align}
\]</span></p>
<p>where <span class="math inline">\(Y_{ei}\)</span> is a dichotomous variable coded one if the respondent reports voting for a presidential candidate from a different party other than his/her own, <span class="math inline">\(\alpha\)</span> is a constant, <span class="math inline">\(\beta\)</span>s represent the log-odds association of the covariates with the probability of defection, and observations are pooled over elections (<span class="math inline">\(e \in \{1976, 1980, 1984, 1988, 1992, 1996, 2000, 2004, 2008\}\)</span>). This is different from a traditional time series-cross sectional design in that respondents <span class="math inline">\(i\)</span> are not repeatedly interviewed: each election consists of a different representative sample of U.S. citizens. Therefore, the primary focus of the multilevel model should be on independence between elections. Will the baseline probability of defection differ across elections? Will the effects of partisan intensity or the other unit-specific measures differ across elections?</p>
<div id="classical-logistic-regression" class="section level2">
<h2>Classical logistic regression</h2>
<p>Let’s first examine the results of a classical logit model, without controlling for dependence within elections:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">basic &lt;-<span class="st"> </span><span class="kw">glm</span>(defect <span class="op">~</span><span class="st"> </span>pid_abs <span class="op">+</span><span class="st"> </span>feel_def <span class="op">+</span><span class="st"> </span>defect_cor <span class="op">+</span><span class="st"> </span>inc,
             <span class="dt">data =</span> anes, <span class="dt">family =</span> <span class="kw">binomial</span>(<span class="dt">link =</span> <span class="st">&quot;logit&quot;</span>))
<span class="kw">tidy</span>(basic)</code></pre></div>
<pre><code>##          term estimate std.error statistic   p.value
## 1 (Intercept)   -1.215   0.08654    -14.04  8.63e-45
## 2     pid_abs   -0.282   0.04936     -5.72  1.05e-08
## 3    feel_def    0.117   0.00313     37.45 6.16e-307
## 4  defect_cor    0.912   0.07530     12.11  9.30e-34
## 5         inc    0.749   0.08214      9.11  7.98e-20</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">coefplot</span>(basic,
         <span class="dt">title =</span> <span class="st">&quot;Classical GLM of partisan defection&quot;</span>,
         <span class="dt">newNames =</span> <span class="kw">c</span>(<span class="st">&quot;pid_abs&quot;</span> =<span class="st"> &quot;PID intensity&quot;</span>,
                      <span class="st">&quot;feel_def&quot;</span> =<span class="st"> &quot;Relative favorability&quot;</span>,
                      <span class="st">&quot;defect_cor&quot;</span> =<span class="st"> &quot;Defect is correct&quot;</span>,
                      <span class="st">&quot;inc&quot;</span> =<span class="st"> &quot;Incumbent candidate&quot;</span>),
         <span class="dt">decreasing =</span> <span class="ot">TRUE</span>)</code></pre></div>
<p><img src="persp015_multilevel_data_files/figure-html/anes-logit-1.png" width="672" /></p>
<p>These results indicate when the covariates equal zero, individuals are more likely to vote for a candidate from their own party rather than defect (<span class="math inline">\(\alpha=-0.83\)</span>). PID intensity is also negatively associated with partisan defection (consistent with the hypothesized direction), while relative favorability, defect is correct, and incumbent candidate are all positive. Importantly, the standard errors for each coefficient are low with <span class="math inline">\(\textit{p-value} &lt; 0.01\)</span>. A classical logistic regression model suggests our coefficient estimates are very precise, and effects do not vary across election years (because we did not allow them to vary).</p>
</div>
<div id="varying-intercept" class="section level2">
<h2>Varying intercept</h2>
<p>What if we relax the assumption that the baseline probability of defection is constant for all observations? Let us model <span class="math inline">\(\alpha\)</span> directly from the data, specifying the model as:</p>
<p><span class="math display">\[
\begin{align}
 Pr(Y_{ei} = 1) &amp;= \text{logit}^{-1}[\alpha_{e[i]} + \beta_{1}\text{PID Intensity}_{ei} \nonumber \\
 &amp;\quad + \beta_{2}{\text{Relative Favorability}_{ei}} + \beta_{3}\text{Defect is Correct}_{ei}]  \nonumber \\
 \alpha_{e} &amp;\sim N(\gamma_{0}^{\alpha} + \gamma_{1}^{\alpha}\text{Incumbent},\sigma^2_{\alpha}), \: \text{for} \: e=1976,\dotsc,2008, \nonumber \\
 \text{logit}^{-1}[x] &amp;= \frac{e^x}{1+e^x}
\end{align}
\]</span></p>
<p>We can use the <code>glmer()</code> function in <code>lme4</code> library to estimate generalized linear multilevel models:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">vary.int &lt;-<span class="st"> </span><span class="kw">glmer</span>(defect <span class="op">~</span><span class="st"> </span>pid_abs <span class="op">+</span><span class="st"> </span>feel_def <span class="op">+</span><span class="st"> </span>defect_cor <span class="op">+</span><span class="st"> </span>inc <span class="op">+</span><span class="st"> </span>(<span class="dv">1</span><span class="op">|</span>year),
                  <span class="dt">data =</span> anes,
                  <span class="dt">family =</span> <span class="kw">binomial</span>(<span class="dt">link =</span> <span class="st">&quot;logit&quot;</span>))
<span class="kw">tidy</span>(vary.int)</code></pre></div>
<pre><code>##                  term estimate std.error statistic   p.value group
## 1         (Intercept)   -1.197   0.21768     -5.50  3.86e-08 fixed
## 2             pid_abs   -0.288   0.04988     -5.77  8.04e-09 fixed
## 3            feel_def    0.121   0.00323     37.33 5.60e-305 fixed
## 4          defect_cor    0.850   0.07804     10.89  1.27e-27 fixed
## 5                 inc    0.640   0.25284      2.53  1.13e-02 fixed
## 6 sd_(Intercept).year    0.345        NA        NA        NA  year</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">summary</span>(vary.int)</code></pre></div>
<pre><code>## Generalized linear mixed model fit by maximum likelihood (Laplace
##   Approximation) [glmerMod]
##  Family: binomial  ( logit )
## Formula: defect ~ pid_abs + feel_def + defect_cor + inc + (1 | year)
##    Data: anes
## 
##      AIC      BIC   logLik deviance df.resid 
##     5209     5253    -2598     5197    10375 
## 
## Scaled residuals: 
##     Min      1Q  Median      3Q     Max 
## -11.315  -0.295  -0.135  -0.046  22.703 
## 
## Random effects:
##  Groups Name        Variance Std.Dev.
##  year   (Intercept) 0.119    0.345   
## Number of obs: 10381, groups:  year, 10
## 
## Fixed effects:
##             Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept) -1.19665    0.21768    -5.5  3.9e-08 ***
## pid_abs     -0.28768    0.04988    -5.8  8.0e-09 ***
## feel_def     0.12072    0.00323    37.3  &lt; 2e-16 ***
## defect_cor   0.85002    0.07804    10.9  &lt; 2e-16 ***
## inc          0.64021    0.25284     2.5    0.011 *  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Correlation of Fixed Effects:
##            (Intr) pid_bs fel_df dfct_c
## pid_abs    -0.206                     
## feel_def    0.039  0.012              
## defect_cor -0.143  0.178 -0.062       
## inc        -0.808 -0.011  0.025 -0.024</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">multiplot</span>(basic, vary.int,
         <span class="dt">title =</span> <span class="st">&quot;Regression models of partisan defection&quot;</span>,
         <span class="dt">newNames =</span> <span class="kw">c</span>(<span class="st">&quot;pid_abs&quot;</span> =<span class="st"> &quot;PID intensity&quot;</span>,
                      <span class="st">&quot;feel_def&quot;</span> =<span class="st"> &quot;Relative favorability&quot;</span>,
                      <span class="st">&quot;defect_cor&quot;</span> =<span class="st"> &quot;Defect is correct&quot;</span>,
                      <span class="st">&quot;inc&quot;</span> =<span class="st"> &quot;Incumbent candidate&quot;</span>),
         <span class="dt">names =</span> <span class="kw">c</span>(<span class="st">&quot;Classical GLM&quot;</span>, <span class="st">&quot;Varying intercept&quot;</span>),
         <span class="dt">decreasing =</span> <span class="ot">TRUE</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">theme</span>(<span class="dt">legend.position =</span> <span class="st">&quot;bottom&quot;</span>)</code></pre></div>
<p><img src="persp015_multilevel_data_files/figure-html/anes-vary-int-1.png" width="672" /></p>
<p>The top of the summary reports general model statistics such as log-likelihood, deviance, AIC, and BIC. These statistics can be used for model comparisons and relative goodness-of-fit tests. Below this is a table with the variance components. Since logistic regression assumes a constant unit-level variance of <span class="math inline">\(\frac{\pi^2}{3}\)</span>, there is no variance component for this level. The only variance component estimated in the model is <span class="math inline">\(\sigma^2_{\alpha}\)</span>. The standard deviation is 0.34, suggesting a significant amount of variance around the average intercept. This result confirms our expectation since we know the mean defection rate differs across elections.</p>
<p>The remaining estimated coefficients do not differ significantly from the original non-MLM results.</p>
<p>Multilevel modeling also allows us to retain group-specific estimates of varying coefficients. These estimates are combinations of the fixed and random effects.<a href="#fn3" class="footnoteRef" id="fnref3"><sup>3</sup></a> These group-specific effects can be generated using the <code>coef()</code> function:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">coef</span>(vary.int)</code></pre></div>
<pre><code>## $year
##      (Intercept) pid_abs feel_def defect_cor  inc
## 1972      -1.271  -0.288    0.121       0.85 0.64
## 1976      -1.570  -0.288    0.121       0.85 0.64
## 1980      -0.976  -0.288    0.121       0.85 0.64
## 1984      -1.423  -0.288    0.121       0.85 0.64
## 1988      -0.943  -0.288    0.121       0.85 0.64
## 1992      -0.455  -0.288    0.121       0.85 0.64
## 1996      -1.080  -0.288    0.121       0.85 0.64
## 2000      -1.329  -0.288    0.121       0.85 0.64
## 2004      -1.587  -0.288    0.121       0.85 0.64
## 2008      -1.308  -0.288    0.121       0.85 0.64
## 
## attr(,&quot;class&quot;)
## [1] &quot;coef.mer&quot;</code></pre>
<p>In every year, respondents on average have a low baseline log-odds of defecting (holding all covariates equal to zero). As suggested by <span class="math inline">\(\sigma^2_{\alpha}\)</span>, the magnitude of this coefficient varies significantly, ranging from a low of <span class="math inline">\(-1.587\)</span> to a high of <span class="math inline">\(-0.455\)</span>.</p>
</div>
<div id="varying-slopes-and-intercept" class="section level2">
<h2>Varying slope(s) and intercept</h2>
<p>Now let us consider potential varying slopes. While we could estimate the model with a varying slope for each coefficient, unless we have hundreds of thousands of observations we will quickly run into issues maximizing the log-likelihood (this will be covered shortly under the limitations section). First and foremost we should use theory to guide our selection of varying slopes. However if it seems plausible that any of the covariates should need a varying slope, we need a method to determine which coefficients should receive one in the final model.</p>
<p>One can use a likelihood ratio test to determine whether or not the additional variance component significantly improves overall model fit. Note that since GLM implementations of MLM utilize the Laplace approximation rather than full MLE, deviance or likelihood ratio tests can only be applied to models with identical constant parameters. That is, not only must the dataset be the same, the model specifications must be identical except for the variance component. The results of a test to determine if a varying slope should be utilized for PID intensity is:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">defect &lt;-<span class="st"> </span><span class="kw">glmer</span>(defect <span class="op">~</span><span class="st"> </span>pid_abs <span class="op">+</span><span class="st"> </span>feel_def <span class="op">+</span><span class="st"> </span>defect_cor <span class="op">+</span><span class="st"> </span>inc <span class="op">+</span><span class="st"> </span>(<span class="dv">1</span><span class="op">|</span>year),
                <span class="dt">data =</span> anes, <span class="dt">family =</span> <span class="kw">binomial</span>(<span class="dt">link =</span> <span class="st">&quot;logit&quot;</span>))
defect.pid3.rc1 &lt;-<span class="st"> </span><span class="kw">glmer</span>(defect <span class="op">~</span><span class="st"> </span>pid_abs <span class="op">+</span><span class="st"> </span>feel_def <span class="op">+</span><span class="st"> </span>defect_cor <span class="op">+</span><span class="st"> </span>inc <span class="op">+</span>
<span class="st">                           </span>(<span class="dv">1</span><span class="op">|</span>year) <span class="op">+</span><span class="st"> </span>(pid_abs<span class="op">|</span>year),
                         <span class="dt">data =</span> anes, <span class="dt">family =</span> <span class="kw">binomial</span>(<span class="dt">link =</span> <span class="st">&quot;logit&quot;</span>))
defect.pid3.test &lt;-<span class="st"> </span><span class="kw">anova</span>(defect, defect.pid3.rc1)
<span class="kw">tidy</span>(defect.pid3.test)</code></pre></div>
<pre><code>##              term df  AIC  BIC logLik deviance statistic Chi.Df p.value
## 1          defect  6 5209 5253  -2599     5197        NA     NA      NA
## 2 defect.pid3.rc1  9 5214 5280  -2598     5196     0.626      3    0.89</code></pre>
<p>The ANOVA test reports the degrees of freedom, AIC, BIC, and log-likelihood for each model. The <span class="math inline">\(\chi^2\)</span> test statistic is simply the difference in log-likelihoods, with the degrees of freedom for the test statistic derived from the difference in the overall models’ degrees of freedom. Notice that by specifying a varying slope for PID intensity, we’re actually adding three parameters to the model:</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(\mu_{\beta_{1}}\)</span></li>
<li><span class="math inline">\(\sigma^2_{\beta_{1}}\)</span></li>
<li>The correlation <span class="math inline">\(\rho\)</span> between <span class="math inline">\(\sigma^2_{\alpha}\)</span> and <span class="math inline">\(\sigma^2_{\beta_{1}}\)</span></li>
</ol>
<p>The p-value for the test statistic is <span class="math inline">\(0.89\)</span>, far above the traditional threshold of <span class="math inline">\(0.05\)</span>. Adding a varying slope for PID intensity does not significantly improve overall model fit, so it is unnecessary in the final model.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">defect &lt;-<span class="st"> </span><span class="kw">glmer</span>(defect <span class="op">~</span><span class="st"> </span>pid_abs <span class="op">+</span><span class="st"> </span>feel_def <span class="op">+</span><span class="st"> </span>defect_cor <span class="op">+</span><span class="st"> </span>inc <span class="op">+</span><span class="st"> </span>(<span class="dv">1</span><span class="op">|</span>year),
                <span class="dt">data =</span> anes, <span class="dt">family =</span> <span class="kw">binomial</span>(<span class="dt">link =</span> <span class="st">&quot;logit&quot;</span>))
defect.xfeel &lt;-<span class="st"> </span><span class="kw">glmer</span>(defect <span class="op">~</span><span class="st"> </span>pid_abs <span class="op">+</span><span class="st"> </span>feel_def <span class="op">+</span><span class="st"> </span>defect_cor <span class="op">+</span><span class="st"> </span>inc <span class="op">+</span>
<span class="st">                        </span>pid_abs <span class="op">*</span><span class="st"> </span>feel_def <span class="op">+</span><span class="st"> </span>(<span class="dv">1</span><span class="op">|</span>year),
                      <span class="dt">data =</span> anes, <span class="dt">family =</span> <span class="kw">binomial</span>(<span class="dt">link =</span> <span class="st">&quot;logit&quot;</span>))
defect.xdef &lt;-<span class="st"> </span><span class="kw">glmer</span>(defect <span class="op">~</span><span class="st"> </span>pid_abs <span class="op">+</span><span class="st"> </span>feel_def <span class="op">+</span><span class="st"> </span>defect_cor <span class="op">+</span><span class="st"> </span>inc <span class="op">+</span>
<span class="st">                       </span>pid_abs <span class="op">*</span><span class="st"> </span>defect_cor <span class="op">+</span><span class="st"> </span>(<span class="dv">1</span><span class="op">|</span>year),
                     <span class="dt">data =</span> anes, <span class="dt">family =</span> <span class="kw">binomial</span>(<span class="dt">link =</span> <span class="st">&quot;logit&quot;</span>))
defect.xinc &lt;-<span class="st"> </span><span class="kw">glmer</span>(defect <span class="op">~</span><span class="st"> </span>pid_abs <span class="op">+</span><span class="st"> </span>feel_def <span class="op">+</span><span class="st"> </span>defect_cor <span class="op">+</span><span class="st"> </span>inc <span class="op">+</span>
<span class="st">                       </span>pid_abs <span class="op">*</span><span class="st"> </span>inc_opp <span class="op">+</span><span class="st"> </span>(<span class="dv">1</span><span class="op">|</span>year),
                     <span class="dt">data =</span> anes, <span class="dt">family =</span> <span class="kw">binomial</span>(<span class="dt">link =</span> <span class="st">&quot;logit&quot;</span>))
defect.cor.rc1 &lt;-<span class="st"> </span><span class="kw">glmer</span>(defect <span class="op">~</span><span class="st"> </span>pid_abs <span class="op">+</span><span class="st"> </span>feel_def <span class="op">+</span><span class="st"> </span>defect_cor <span class="op">+</span>
<span class="st">                          </span>inc <span class="op">+</span><span class="st"> </span>(<span class="dv">1</span><span class="op">|</span>year) <span class="op">+</span><span class="st"> </span>(defect_cor<span class="op">|</span>year),
                        <span class="dt">data =</span> anes, <span class="dt">family =</span> <span class="kw">binomial</span>(<span class="dt">link =</span> <span class="st">&quot;logit&quot;</span>))
defect.pid3.rc1 &lt;-<span class="st"> </span><span class="kw">glmer</span>(defect <span class="op">~</span><span class="st"> </span>pid_abs <span class="op">+</span><span class="st"> </span>feel_def <span class="op">+</span><span class="st"> </span>defect_cor <span class="op">+</span>
<span class="st">                           </span>inc <span class="op">+</span><span class="st"> </span>(<span class="dv">1</span><span class="op">|</span>year) <span class="op">+</span><span class="st"> </span>(pid_abs<span class="op">|</span>year),
                         <span class="dt">data =</span> anes, <span class="dt">family =</span> <span class="kw">binomial</span>(<span class="dt">link =</span> <span class="st">&quot;logit&quot;</span>))
defect.feel.rc1 &lt;-<span class="st"> </span><span class="kw">glmer</span>(defect <span class="op">~</span><span class="st"> </span>pid_abs <span class="op">+</span><span class="st"> </span>feel_def <span class="op">+</span><span class="st"> </span>defect_cor <span class="op">+</span>
<span class="st">                           </span>inc <span class="op">+</span><span class="st"> </span>(<span class="dv">1</span><span class="op">|</span>year) <span class="op">+</span><span class="st"> </span>(feel_def<span class="op">|</span>year),
                         <span class="dt">data =</span> anes, <span class="dt">family =</span> <span class="kw">binomial</span>(<span class="dt">link =</span> <span class="st">&quot;logit&quot;</span>))
defect.inc.rc1 &lt;-<span class="st"> </span><span class="kw">glmer</span>(defect <span class="op">~</span><span class="st"> </span>pid_abs <span class="op">+</span><span class="st"> </span>feel_def <span class="op">+</span><span class="st"> </span>defect_cor <span class="op">+</span>
<span class="st">                          </span>inc <span class="op">+</span><span class="st"> </span>(<span class="dv">1</span><span class="op">|</span>year) <span class="op">+</span><span class="st"> </span>(inc<span class="op">|</span>year),
                        <span class="dt">data =</span> anes, <span class="dt">family =</span> <span class="kw">binomial</span>(<span class="dt">link =</span> <span class="st">&quot;logit&quot;</span>))
  
<span class="co"># likelihood ratio tests to compare model fit</span>
<span class="kw">anova</span>(defect, defect.xfeel)</code></pre></div>
<pre><code>## Data: anes
## Models:
## defect: defect ~ pid_abs + feel_def + defect_cor + inc + (1 | year)
## defect.xfeel: defect ~ pid_abs + feel_def + defect_cor + inc + pid_abs * feel_def + 
## defect.xfeel:     (1 | year)
##              Df  AIC  BIC logLik deviance Chisq Chi Df Pr(&gt;Chisq)
## defect        6 5209 5253  -2599     5197                        
## defect.xfeel  7 5208 5259  -2597     5194  2.68      1        0.1</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">anova</span>(defect, defect.xdef)</code></pre></div>
<pre><code>## Data: anes
## Models:
## defect: defect ~ pid_abs + feel_def + defect_cor + inc + (1 | year)
## defect.xdef: defect ~ pid_abs + feel_def + defect_cor + inc + pid_abs * defect_cor + 
## defect.xdef:     (1 | year)
##             Df  AIC  BIC logLik deviance Chisq Chi Df Pr(&gt;Chisq)
## defect       6 5209 5253  -2599     5197                        
## defect.xdef  7 5210 5260  -2598     5196   1.4      1       0.24</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">anova</span>(defect, defect.xinc)</code></pre></div>
<pre><code>## Data: anes
## Models:
## defect: defect ~ pid_abs + feel_def + defect_cor + inc + (1 | year)
## defect.xinc: defect ~ pid_abs + feel_def + defect_cor + inc + pid_abs * inc_opp + 
## defect.xinc:     (1 | year)
##             Df  AIC  BIC logLik deviance Chisq Chi Df Pr(&gt;Chisq)
## defect       6 5209 5253  -2599     5197                        
## defect.xinc  8 5211 5269  -2597     5195  2.28      2       0.32</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">anova</span>(defect, defect.cor.rc1)</code></pre></div>
<pre><code>## Data: anes
## Models:
## defect: defect ~ pid_abs + feel_def + defect_cor + inc + (1 | year)
## defect.cor.rc1: defect ~ pid_abs + feel_def + defect_cor + inc + (1 | year) + 
## defect.cor.rc1:     (defect_cor | year)
##                Df  AIC  BIC logLik deviance Chisq Chi Df Pr(&gt;Chisq)
## defect          6 5209 5253  -2599     5197                        
## defect.cor.rc1  9 5212 5277  -2597     5194  3.26      3       0.35</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">anova</span>(defect, defect.pid3.rc1)</code></pre></div>
<pre><code>## Data: anes
## Models:
## defect: defect ~ pid_abs + feel_def + defect_cor + inc + (1 | year)
## defect.pid3.rc1: defect ~ pid_abs + feel_def + defect_cor + inc + (1 | year) + 
## defect.pid3.rc1:     (pid_abs | year)
##                 Df  AIC  BIC logLik deviance Chisq Chi Df Pr(&gt;Chisq)
## defect           6 5209 5253  -2599     5197                        
## defect.pid3.rc1  9 5214 5280  -2598     5196  0.63      3       0.89</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">anova</span>(defect, defect.feel.rc1)</code></pre></div>
<pre><code>## Data: anes
## Models:
## defect: defect ~ pid_abs + feel_def + defect_cor + inc + (1 | year)
## defect.feel.rc1: defect ~ pid_abs + feel_def + defect_cor + inc + (1 | year) + 
## defect.feel.rc1:     (feel_def | year)
##                 Df  AIC  BIC logLik deviance Chisq Chi Df Pr(&gt;Chisq)    
## defect           6 5209 5253  -2599     5197                            
## defect.feel.rc1  9 5166 5232  -2574     5148  48.6      3    1.6e-10 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">anova</span>(defect, defect.inc.rc1)</code></pre></div>
<pre><code>## Data: anes
## Models:
## defect: defect ~ pid_abs + feel_def + defect_cor + inc + (1 | year)
## defect.inc.rc1: defect ~ pid_abs + feel_def + defect_cor + inc + (1 | year) + 
## defect.inc.rc1:     (inc | year)
##                Df  AIC  BIC logLik deviance Chisq Chi Df Pr(&gt;Chisq)
## defect          6 5209 5253  -2599     5197                        
## defect.inc.rc1  9 5213 5279  -2598     5195  1.75      3       0.63</code></pre>
<p>Sequential tests of the other covariates indicate relative favorability should vary across elections, so the final model is specified as:</p>
<p><span class="math display">\[
\begin{align}
 Pr(Y_{ei} = 1) &amp;= \text{logit}^{-1}[\alpha_{e[i]} + \beta_{1}\text{PID Intensity}_{ei} \nonumber \\
 &amp;\quad + \beta_{2e[i]}{\text{Relative Favorability}_{ei}} + \beta_{3}\text{Defect is Correct}_{ei}]  \nonumber \\
 \begin{pmatrix}
 \alpha_{e} \\
 \beta_{2e} \\
 \end{pmatrix} &amp;\sim N\left(\begin{pmatrix}
 \gamma_{0}^{\alpha} + \gamma_{1}^{\alpha}\text{Incumbent} \\
 \gamma_{0}^{\beta_{2}} \\
 \end{pmatrix},\begin{pmatrix}
 \sigma^2_{\alpha} &amp; \rho\sigma_{\alpha}\sigma_{\beta_2}  \\
 \rho\sigma_{\alpha}\sigma_{\beta_2} &amp; \sigma^{2}_{\beta_2}  \\
 \end{pmatrix}\right) \nonumber \\
  \text{logit}^{-1}[x] &amp;= \frac{e^x}{1+e^x}
\end{align}
\]</span></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">final &lt;-<span class="st"> </span><span class="kw">glmer</span>(defect <span class="op">~</span><span class="st"> </span>pid_abs <span class="op">+</span><span class="st"> </span>feel_def <span class="op">+</span><span class="st"> </span>defect_cor <span class="op">+</span><span class="st"> </span>inc <span class="op">+</span>
<span class="st">                 </span>(<span class="dv">1</span><span class="op">|</span>year) <span class="op">+</span><span class="st"> </span>(feel_def<span class="op">|</span>year),
               <span class="dt">data =</span> anes, <span class="dt">family =</span> <span class="kw">binomial</span>(<span class="dt">link =</span> <span class="st">&quot;logit&quot;</span>))
<span class="kw">tidy</span>(final)</code></pre></div>
<pre><code>##                              term estimate std.error statistic  p.value
## 1                     (Intercept)  -1.0123   0.14373     -7.04 1.88e-12
## 2                         pid_abs  -0.2967   0.04991     -5.94 2.78e-09
## 3                        feel_def   0.1286   0.00734     17.54 7.36e-69
## 4                      defect_cor   0.8230   0.07759     10.61 2.77e-26
## 5                             inc   0.3941   0.14674      2.69 7.24e-03
## 6             sd_(Intercept).year   0.0000        NA        NA       NA
## 7           sd_(Intercept).year.1   0.2534        NA        NA       NA
## 8              sd_feel_def.year.1   0.0202        NA        NA       NA
## 9 cor_(Intercept).feel_def.year.1  -1.0000        NA        NA       NA
##    group
## 1  fixed
## 2  fixed
## 3  fixed
## 4  fixed
## 5  fixed
## 6   year
## 7 year.1
## 8 year.1
## 9 year.1</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">multiplot</span>(basic, vary.int, final,
         <span class="dt">title =</span> <span class="st">&quot;Regression models of partisan defection&quot;</span>,
         <span class="dt">newNames =</span> <span class="kw">c</span>(<span class="st">&quot;pid_abs&quot;</span> =<span class="st"> &quot;PID intensity&quot;</span>,
                      <span class="st">&quot;feel_def&quot;</span> =<span class="st"> &quot;Relative favorability&quot;</span>,
                      <span class="st">&quot;defect_cor&quot;</span> =<span class="st"> &quot;Defect is correct&quot;</span>,
                      <span class="st">&quot;inc&quot;</span> =<span class="st"> &quot;Incumbent candidate&quot;</span>),
         <span class="dt">names =</span> <span class="kw">c</span>(<span class="st">&quot;Classical GLM&quot;</span>, <span class="st">&quot;Varying intercept&quot;</span>, <span class="st">&quot;Varying slopes&quot;</span>),
         <span class="dt">decreasing =</span> <span class="ot">TRUE</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">theme</span>(<span class="dt">legend.position =</span> <span class="st">&quot;bottom&quot;</span>)</code></pre></div>
<p><img src="persp015_multilevel_data_files/figure-html/final-1.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">coef</span>(final)</code></pre></div>
<pre><code>## $year
##      (Intercept) pid_abs feel_def defect_cor   inc
## 1972       -1.01  -0.297   0.1248      0.823 0.394
## 1976       -1.01  -0.297   0.1458      0.823 0.394
## 1980       -1.01  -0.297   0.1075      0.823 0.394
## 1984       -1.01  -0.297   0.1439      0.823 0.394
## 1988       -1.01  -0.297   0.1270      0.823 0.394
## 1992       -1.01  -0.297   0.0861      0.823 0.394
## 1996       -1.01  -0.297   0.1167      0.823 0.394
## 2000       -1.01  -0.297   0.1419      0.823 0.394
## 2004       -1.01  -0.297   0.1419      0.823 0.394
## 2008       -1.01  -0.297   0.1471      0.823 0.394
## 
## attr(,&quot;class&quot;)
## [1] &quot;coef.mer&quot;</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># generate posterior simulations of beta to estimate group-specific standard errors</span>
sim.final &lt;-<span class="st"> </span>arm<span class="op">::</span><span class="kw">sim</span>(final, <span class="dt">n.sims =</span> <span class="dv">1000</span>)

se.fix &lt;-<span class="st"> </span><span class="kw">apply</span>(sim.final<span class="op">@</span>fixef, <span class="dv">2</span>, <span class="cf">function</span>(x) <span class="kw">sd</span>(x, <span class="dt">na.rm =</span> <span class="ot">TRUE</span>))
se.fix &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">rep</span>(se.fix,<span class="dt">times=</span><span class="dv">10</span>),<span class="dt">nrow=</span><span class="kw">nrow</span>(<span class="kw">coef</span>(final)<span class="op">$</span>year),<span class="dt">ncol=</span><span class="kw">length</span>(se.fix),<span class="dt">byrow=</span><span class="ot">TRUE</span>)   <span class="co">#expand to matrix for addition with random se</span>

se.ran &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="ot">NA</span>, <span class="dt">nrow =</span> <span class="kw">nrow</span>(<span class="kw">coef</span>(final)<span class="op">$</span>year), <span class="dt">ncol =</span> <span class="kw">ncol</span>(<span class="kw">ranef</span>(final)<span class="op">$</span>year))
se.ran[, <span class="dv">1</span>] &lt;-<span class="st"> </span><span class="kw">apply</span>(sim.final<span class="op">@</span>ranef[[<span class="dv">1</span>]], <span class="dv">2</span>, <span class="cf">function</span>(x) <span class="kw">sd</span>(x, <span class="dt">na.rm =</span> <span class="ot">TRUE</span>))
se.ran[, <span class="dv">2</span>] &lt;-<span class="st"> </span><span class="kw">apply</span>(sim.final<span class="op">@</span>ranef[[<span class="dv">2</span>]][,,<span class="dv">1</span>], <span class="dv">2</span>, <span class="cf">function</span>(x) <span class="kw">sd</span>(x, <span class="dt">na.rm =</span> <span class="ot">TRUE</span>))
se.ran[, <span class="dv">3</span>] &lt;-<span class="st"> </span><span class="kw">apply</span>(sim.final<span class="op">@</span>ranef[[<span class="dv">2</span>]][,,<span class="dv">2</span>], <span class="dv">2</span>, <span class="cf">function</span>(x) <span class="kw">sd</span>(x, <span class="dt">na.rm =</span> <span class="ot">TRUE</span>))

se.ran &lt;-<span class="st"> </span><span class="kw">cbind</span>(se.ran[, <span class="dv">1</span>] <span class="op">+</span><span class="st"> </span>se.ran[, <span class="dv">2</span>], <span class="dv">0</span>, se.ran[, <span class="dv">3</span>], <span class="dv">0</span>, <span class="dv">0</span>)
se.final &lt;-<span class="st"> </span>se.fix <span class="op">+</span><span class="st"> </span>se.ran</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">final_year &lt;-<span class="st"> </span>anes <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">filter</span>(year <span class="op">&gt;=</span><span class="st"> </span><span class="dv">1972</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">group_by</span>(year) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">nest</span>() <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">model =</span> <span class="kw">map</span>(data, <span class="op">~</span><span class="st"> </span><span class="kw">glm</span>(defect <span class="op">~</span><span class="st"> </span>pid_abs <span class="op">+</span><span class="st"> </span>feel_def <span class="op">+</span><span class="st"> </span>defect_cor <span class="op">+</span><span class="st"> </span>inc,
               <span class="dt">data =</span> .x, <span class="dt">family =</span> <span class="kw">binomial</span>(<span class="dt">link =</span> <span class="st">&quot;logit&quot;</span>))),
         <span class="dt">coef =</span> <span class="kw">map</span>(model, tidy))

<span class="co"># get intercept and standard errors for MLM into a tidy data frame</span>
mlm_int &lt;-<span class="st"> </span><span class="kw">data_frame</span>(<span class="dt">year =</span> <span class="kw">rownames</span>(<span class="kw">coef</span>(final)<span class="op">$</span>year),
           <span class="dt">term =</span> <span class="st">&quot;(Intercept)&quot;</span>,
           <span class="dt">estimate =</span> <span class="kw">coef</span>(final)<span class="op">$</span>year[[<span class="st">&quot;(Intercept)&quot;</span>]],
           <span class="dt">std.error =</span> se.final[, <span class="dv">1</span>],
           <span class="dt">method =</span> <span class="st">&quot;MLM&quot;</span>)

<span class="co"># extract intercepts and standard errors</span>
glm_int &lt;-<span class="st"> </span>final_year <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">unnest</span>(coef) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">method =</span> <span class="st">&quot;Separate GLM&quot;</span>,
         <span class="dt">year =</span> <span class="kw">as.character</span>(year)) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">filter</span>(term <span class="op">==</span><span class="st"> &quot;(Intercept)&quot;</span>)

<span class="co"># join together</span>
<span class="kw">bind_rows</span>(glm_int,
          mlm_int,
          <span class="kw">tidy</span>(basic) <span class="op">%&gt;%</span>
<span class="st">            </span><span class="kw">mutate</span>(<span class="dt">year =</span> <span class="st">&quot;Overall&quot;</span>,
                   <span class="dt">method =</span> <span class="st">&quot;Separate GLM&quot;</span>),
          <span class="kw">tidy</span>(final) <span class="op">%&gt;%</span>
<span class="st">            </span><span class="kw">mutate</span>(<span class="dt">year =</span> <span class="st">&quot;Overall&quot;</span>,
                   <span class="dt">method =</span> <span class="st">&quot;MLM&quot;</span>)) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">filter</span>(term <span class="op">==</span><span class="st"> &quot;(Intercept)&quot;</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="kw">fct_rev</span>(year), estimate, <span class="dt">color =</span> method)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_pointrange</span>(<span class="kw">aes</span>(<span class="dt">ymin =</span> estimate <span class="op">-</span><span class="st"> </span><span class="fl">1.96</span> <span class="op">*</span><span class="st"> </span>std.error,
                      <span class="dt">ymax =</span> estimate <span class="op">+</span><span class="st"> </span><span class="fl">1.96</span> <span class="op">*</span><span class="st"> </span>std.error),
                  <span class="dt">position =</span> <span class="kw">position_dodge</span>(.<span class="dv">75</span>)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_linerange</span>(<span class="kw">aes</span>(<span class="dt">ymin =</span> estimate <span class="op">-</span><span class="st"> </span>.<span class="dv">67</span> <span class="op">*</span><span class="st"> </span>std.error,
                     <span class="dt">ymax =</span> estimate <span class="op">+</span><span class="st"> </span>.<span class="dv">67</span> <span class="op">*</span><span class="st"> </span>std.error),
                 <span class="dt">position =</span> <span class="kw">position_dodge</span>(.<span class="dv">75</span>),
                 <span class="dt">size =</span> <span class="dv">1</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_hline</span>(<span class="dt">yintercept =</span> <span class="dv">0</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">coord_flip</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="st">&quot;Election-specific estimates of the intercept&quot;</span>,
       <span class="dt">y =</span> <span class="st">&quot;Estimated coefficient with 50% and 95% CI&quot;</span>,
       <span class="dt">x =</span> <span class="ot">NULL</span>,
       <span class="dt">color =</span> <span class="ot">NULL</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">theme</span>(<span class="dt">legend.position =</span> <span class="st">&quot;bottom&quot;</span>)</code></pre></div>
<p><img src="persp015_multilevel_data_files/figure-html/final-by-year-1.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># feel_def plot now</span>
<span class="co"># get intercept and standard errors for MLM into a tidy data frame</span>
mlm_feel_def &lt;-<span class="st"> </span><span class="kw">data_frame</span>(<span class="dt">year =</span> <span class="kw">rownames</span>(<span class="kw">coef</span>(final)<span class="op">$</span>year),
                           <span class="dt">term =</span> <span class="st">&quot;feel_def&quot;</span>,
                           <span class="dt">estimate =</span> <span class="kw">coef</span>(final)<span class="op">$</span>year[[<span class="st">&quot;feel_def&quot;</span>]],
                           <span class="dt">std.error =</span> se.final[, <span class="dv">2</span>],
                           <span class="dt">method =</span> <span class="st">&quot;MLM&quot;</span>)

glm_feel_def &lt;-<span class="st"> </span>final_year <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">unnest</span>(coef) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">method =</span> <span class="st">&quot;Separate GLM&quot;</span>,
         <span class="dt">year =</span> <span class="kw">as.character</span>(year)) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">filter</span>(term <span class="op">==</span><span class="st"> &quot;feel_def&quot;</span>)

<span class="kw">bind_rows</span>(glm_feel_def,
          mlm_feel_def,
          <span class="kw">tidy</span>(basic) <span class="op">%&gt;%</span>
<span class="st">            </span><span class="kw">mutate</span>(<span class="dt">year =</span> <span class="st">&quot;Overall&quot;</span>,
                   <span class="dt">method =</span> <span class="st">&quot;Separate GLM&quot;</span>),
          <span class="kw">tidy</span>(final) <span class="op">%&gt;%</span>
<span class="st">            </span><span class="kw">mutate</span>(<span class="dt">year =</span> <span class="st">&quot;Overall&quot;</span>,
                   <span class="dt">method =</span> <span class="st">&quot;MLM&quot;</span>)) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">filter</span>(term <span class="op">==</span><span class="st"> &quot;feel_def&quot;</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="kw">fct_rev</span>(year), estimate, <span class="dt">color =</span> method)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_pointrange</span>(<span class="kw">aes</span>(<span class="dt">ymin =</span> estimate <span class="op">-</span><span class="st"> </span><span class="fl">1.96</span> <span class="op">*</span><span class="st"> </span>std.error,
                      <span class="dt">ymax =</span> estimate <span class="op">+</span><span class="st"> </span><span class="fl">1.96</span> <span class="op">*</span><span class="st"> </span>std.error),
                  <span class="dt">position =</span> <span class="kw">position_dodge</span>(.<span class="dv">75</span>)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_linerange</span>(<span class="kw">aes</span>(<span class="dt">ymin =</span> estimate <span class="op">-</span><span class="st"> </span>.<span class="dv">67</span> <span class="op">*</span><span class="st"> </span>std.error,
                     <span class="dt">ymax =</span> estimate <span class="op">+</span><span class="st"> </span>.<span class="dv">67</span> <span class="op">*</span><span class="st"> </span>std.error),
                 <span class="dt">position =</span> <span class="kw">position_dodge</span>(.<span class="dv">75</span>),
                 <span class="dt">size =</span> <span class="dv">1</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_hline</span>(<span class="dt">yintercept =</span> <span class="dv">0</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">coord_flip</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="st">&quot;Election-specific estimates of relative favorability&quot;</span>,
       <span class="dt">y =</span> <span class="st">&quot;Estimated coefficient with 50% and 95% CI&quot;</span>,
       <span class="dt">x =</span> <span class="ot">NULL</span>,
       <span class="dt">color =</span> <span class="ot">NULL</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">theme</span>(<span class="dt">legend.position =</span> <span class="st">&quot;bottom&quot;</span>)</code></pre></div>
<p><img src="persp015_multilevel_data_files/figure-html/final-by-year-2.png" width="672" /></p>
</div>
</div>
<div id="limitations-of-mlm" class="section level1">
<h1>Limitations of MLM</h1>
<div id="likelihood-maximization" class="section level2">
<h2>Likelihood maximization</h2>
<p>Multilevel modeling is not without its drawbacks. Perhaps the most important in this application is the requirement of approximations for the likelihood. MLE for a linear multi-level model works because the portions of the likelihood function which need maximizing are (relatively) easily calculated by optimizers. Specifically, all the variance components are additive combinations of several normally distributed variables which themselves have a normal distribution. The variance is the same for all possible predicted values of <span class="math inline">\(Y_{ei}\)</span>. This would also be true for a logistic regression with no multilevel component.</p>
<p>When you make a non-linear model multilevel, you have to add the group-level variability to the non-linear system of equations. There is a probability distribution of fitted values and a probability of a yes response goes with each fitted value (based upon the link function and probability process). The probabilities for each point along the linear predictor <span class="math inline">\(\eta\)</span> are weighted by group-level variability. Since the function is non-linear, the computation of probability for each case is terribly complex to integrate. Computing MLE is enormously more complex for generalized (nonlinear) multilevel models.</p>
<p>The <code>lme4</code> package uses an approximation of the likelihood through the Laplace approximation. Another approach is Restricted Estimation of Maximum Likelihood (REML). These approximations work pretty well for estimating <span class="math inline">\(\gamma\)</span>s, but less so for <span class="math inline">\(\sigma\)</span>s. If the researcher cares most about understanding how and why parameters vary across groups/elections, approximate likelihood approaches will not be useful. I cannot be confident that the <span class="math inline">\(\hat{\sigma}\)</span> are fully accurate, so any measures of statistical significance or associated confidence intervals could be suspect (though the decision to utilize such an interpretation remains with the researcher).</p>
<p>One could potentially optimize the full MLE, but this may be difficult to do and would require a significant amount of time to maximize the likelihood function if it is even possible. Alternatively we could use Bayesian estimation methods to simulate election-specific effects, but again these methods are computationally intensive and require an understanding of Bayesian approaches to inference.</p>
</div>
<div id="determining-which-coefficients-should-have-varying-slopes" class="section level2">
<h2>Determining which coefficients should have varying slopes</h2>
<p>We would like to have a theoretical guide to determining which coefficients should be allowed to vary. Ideally, we could apply soft constraints to each coefficient and allow the data to tell us whether or not there is significant variance between years. The more observations one has in the data, the more parameters that can be estimated (more data <span class="math inline">\(=\)</span> more information about the likelihood the model fits the data). However even with the reduced number of parameters compared to a fixed effects approach, estimating approximate likelihoods with more than two or three varying slopes becomes extremely lengthy and laborious. We would need hundreds of thousands of observations to estimate even a half-dozen varying slopes.</p>
</div>
</div>
<div id="acknowledgments" class="section level1 toc-ignore">
<h1>Acknowledgments</h1>
<ul>
<li><a href="http://www.stat.columbia.edu/~gelman/arm/">Gelman, Andrew, and Jennifer Hill. <em>Data analysis using regression and multilevel/hierarchical models</em>. Cambridge university press, 2006.</a></li>
</ul>
</div>
<div id="session-info" class="section level1 toc-ignore">
<h1>Session Info</h1>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">devtools<span class="op">::</span><span class="kw">session_info</span>()</code></pre></div>
<pre><code>##  setting  value                       
##  version  R version 3.4.1 (2017-06-30)
##  system   x86_64, darwin15.6.0        
##  ui       X11                         
##  language (EN)                        
##  collate  en_US.UTF-8                 
##  tz       America/Chicago             
##  date     2017-08-01                  
## 
##  package      * version    date       source                              
##  assertthat     0.2.0      2017-04-11 CRAN (R 3.4.0)                      
##  backports      1.1.0      2017-05-22 CRAN (R 3.4.0)                      
##  base         * 3.4.1      2017-07-07 local                               
##  base64enc      0.1-3      2015-07-28 CRAN (R 3.4.0)                      
##  bindr          0.1        2016-11-13 CRAN (R 3.4.0)                      
##  bindrcpp     * 0.2        2017-06-17 CRAN (R 3.4.0)                      
##  bit            1.1-12     2014-04-09 CRAN (R 3.4.0)                      
##  bit64          0.9-7      2017-05-08 CRAN (R 3.4.0)                      
##  blob           1.1.0      2017-06-17 CRAN (R 3.4.0)                      
##  boxes          0.0.0.9000 2017-07-19 Github (r-pkgs/boxes@03098dc)       
##  broom        * 0.4.2      2017-02-13 CRAN (R 3.4.0)                      
##  car          * 2.1-5      2017-07-04 CRAN (R 3.4.1)                      
##  caret        * 6.0-76     2017-04-18 CRAN (R 3.4.0)                      
##  cellranger     1.1.0      2016-07-27 CRAN (R 3.4.0)                      
##  class          7.3-14     2015-08-30 CRAN (R 3.4.1)                      
##  clisymbols     1.2.0      2017-05-21 cran (@1.2.0)                       
##  codetools      0.2-15     2016-10-05 CRAN (R 3.4.1)                      
##  coefplot     * 1.2.4      2016-01-10 CRAN (R 3.4.0)                      
##  colorspace     1.3-2      2016-12-14 CRAN (R 3.4.0)                      
##  compiler       3.4.1      2017-07-07 local                               
##  config         0.2        2016-08-02 CRAN (R 3.4.0)                      
##  crayon         1.3.2.9000 2017-07-19 Github (gaborcsardi/crayon@750190f) 
##  data.table     1.10.4     2017-02-01 CRAN (R 3.4.0)                      
##  datasets     * 3.4.1      2017-07-07 local                               
##  DBI            0.7        2017-06-18 CRAN (R 3.4.0)                      
##  dbplyr         1.1.0      2017-06-27 CRAN (R 3.4.1)                      
##  devtools       1.13.2     2017-06-02 CRAN (R 3.4.0)                      
##  digest         0.6.12     2017-01-27 CRAN (R 3.4.0)                      
##  dplyr        * 0.7.2      2017-07-20 CRAN (R 3.4.1)                      
##  e1071        * 1.6-8      2017-02-02 CRAN (R 3.4.0)                      
##  evaluate       0.10.1     2017-06-24 CRAN (R 3.4.1)                      
##  FNN          * 1.1        2013-07-31 CRAN (R 3.4.0)                      
##  forcats      * 0.2.0      2017-01-23 CRAN (R 3.4.0)                      
##  foreach      * 1.4.3      2015-10-13 CRAN (R 3.4.0)                      
##  foreign        0.8-69     2017-06-22 CRAN (R 3.4.1)                      
##  gam          * 1.14-4     2017-04-25 CRAN (R 3.4.0)                      
##  gapminder    * 0.2.0      2015-12-31 CRAN (R 3.4.0)                      
##  gbm          * 2.1.3      2017-03-21 CRAN (R 3.4.0)                      
##  gganimate    * 0.1.0.9000 2017-05-26 Github (dgrtwo/gganimate@bf82002)   
##  ggdendro     * 0.1-20     2016-04-27 CRAN (R 3.4.0)                      
##  ggplot2      * 2.2.1      2016-12-30 CRAN (R 3.4.0)                      
##  glue           1.1.1      2017-06-21 CRAN (R 3.4.1)                      
##  graphics     * 3.4.1      2017-07-07 local                               
##  grDevices    * 3.4.1      2017-07-07 local                               
##  grid         * 3.4.1      2017-07-07 local                               
##  gridExtra    * 2.2.1      2016-02-29 CRAN (R 3.4.0)                      
##  gtable         0.2.0      2016-02-26 CRAN (R 3.4.0)                      
##  haven        * 1.1.0      2017-07-09 CRAN (R 3.4.1)                      
##  highr          0.6        2016-05-09 CRAN (R 3.4.0)                      
##  hms            0.3        2016-11-22 CRAN (R 3.4.0)                      
##  htmltools      0.3.6      2017-04-28 CRAN (R 3.4.0)                      
##  htmlwidgets    0.9        2017-07-10 CRAN (R 3.4.1)                      
##  httpuv         1.3.5      2017-07-04 CRAN (R 3.4.1)                      
##  httr           1.2.1      2016-07-03 CRAN (R 3.4.0)                      
##  igraph         1.1.2      2017-07-21 CRAN (R 3.4.1)                      
##  ISLR         * 1.0        2013-06-11 CRAN (R 3.4.0)                      
##  iterators      1.0.8      2015-10-13 CRAN (R 3.4.0)                      
##  janeaustenr    0.1.5      2017-06-10 CRAN (R 3.4.0)                      
##  jsonlite       1.5        2017-06-01 CRAN (R 3.4.0)                      
##  kknn         * 1.3.1      2016-03-26 CRAN (R 3.4.0)                      
##  knitr        * 1.16       2017-05-18 CRAN (R 3.4.0)                      
##  labeling       0.3        2014-08-23 CRAN (R 3.4.0)                      
##  lattice      * 0.20-35    2017-03-25 CRAN (R 3.4.1)                      
##  lazyeval       0.2.0      2016-06-12 CRAN (R 3.4.0)                      
##  lme4         * 1.1-13     2017-04-19 CRAN (R 3.4.0)                      
##  lmtest       * 0.9-35     2017-02-11 CRAN (R 3.4.0)                      
##  lubridate      1.6.0      2016-09-13 CRAN (R 3.4.0)                      
##  magrittr       1.5        2014-11-22 CRAN (R 3.4.0)                      
##  MASS           7.3-47     2017-02-26 CRAN (R 3.4.1)                      
##  Matrix       * 1.2-10     2017-05-03 CRAN (R 3.4.1)                      
##  MatrixModels   0.4-1      2015-08-22 CRAN (R 3.4.0)                      
##  memoise        1.1.0      2017-04-21 CRAN (R 3.4.0)                      
##  methods      * 3.4.1      2017-07-07 local                               
##  mgcv           1.8-18     2017-07-28 CRAN (R 3.4.1)                      
##  mime           0.5        2016-07-07 CRAN (R 3.4.0)                      
##  minqa          1.2.4      2014-10-09 CRAN (R 3.4.0)                      
##  mnormt         1.5-5      2016-10-15 CRAN (R 3.4.0)                      
##  ModelMetrics   1.1.0      2016-08-26 CRAN (R 3.4.0)                      
##  modelr       * 0.1.1      2017-07-24 CRAN (R 3.4.1)                      
##  modeltools     0.2-21     2013-09-02 CRAN (R 3.4.0)                      
##  munsell        0.4.3      2016-02-13 CRAN (R 3.4.0)                      
##  nlme           3.1-131    2017-02-06 CRAN (R 3.4.1)                      
##  nloptr         1.0.4      2014-08-04 CRAN (R 3.4.0)                      
##  NLP          * 0.1-10     2017-02-21 CRAN (R 3.4.0)                      
##  nnet         * 7.3-12     2016-02-02 CRAN (R 3.4.1)                      
##  nycflights13   0.2.2      2017-01-27 CRAN (R 3.4.0)                      
##  parallel     * 3.4.1      2017-07-07 local                               
##  pbkrtest       0.4-7      2017-03-15 CRAN (R 3.4.0)                      
##  pkgconfig      2.0.1      2017-03-21 CRAN (R 3.4.0)                      
##  plotly       * 4.7.1      2017-07-29 CRAN (R 3.4.1)                      
##  plyr           1.8.4      2016-06-08 CRAN (R 3.4.0)                      
##  pROC         * 1.10.0     2017-06-10 CRAN (R 3.4.0)                      
##  psych          1.7.5      2017-05-03 CRAN (R 3.4.1)                      
##  purrr        * 0.2.2.2    2017-05-11 CRAN (R 3.4.0)                      
##  quantreg       5.33       2017-04-18 CRAN (R 3.4.0)                      
##  R6             2.2.2      2017-06-17 CRAN (R 3.4.0)                      
##  randomForest * 4.6-12     2015-10-07 CRAN (R 3.4.0)                      
##  rappdirs       0.3.1      2016-03-28 CRAN (R 3.4.0)                      
##  rcfss        * 0.1.5      2017-07-31 local                               
##  RColorBrewer * 1.1-2      2014-12-07 CRAN (R 3.4.0)                      
##  Rcpp           0.12.12    2017-07-15 CRAN (R 3.4.1)                      
##  readr        * 1.1.1      2017-05-16 CRAN (R 3.4.0)                      
##  readxl         1.0.0      2017-04-18 CRAN (R 3.4.0)                      
##  reshape2       1.4.2      2016-10-22 CRAN (R 3.4.0)                      
##  rlang          0.1.1      2017-05-18 CRAN (R 3.4.0)                      
##  rmarkdown      1.6        2017-06-15 CRAN (R 3.4.0)                      
##  rprojroot      1.2        2017-01-16 CRAN (R 3.4.0)                      
##  RSQLite      * 2.0        2017-06-19 CRAN (R 3.4.1)                      
##  rstudioapi     0.6        2016-06-27 CRAN (R 3.4.0)                      
##  rvest          0.3.2      2016-06-17 CRAN (R 3.4.0)                      
##  scales         0.4.1      2016-11-09 CRAN (R 3.4.0)                      
##  shiny          1.0.3      2017-04-26 CRAN (R 3.4.0)                      
##  slam           0.1-40     2016-12-01 CRAN (R 3.4.0)                      
##  SnowballC      0.5.1      2014-08-09 CRAN (R 3.4.0)                      
##  sparklyr     * 0.6.0      2017-07-29 CRAN (R 3.4.1)                      
##  SparseM        1.77       2017-04-23 CRAN (R 3.4.0)                      
##  splines      * 3.4.1      2017-07-07 local                               
##  stats        * 3.4.1      2017-07-07 local                               
##  stats4         3.4.1      2017-07-07 local                               
##  stringi        1.1.5      2017-04-07 CRAN (R 3.4.0)                      
##  stringr      * 1.2.0      2017-02-18 CRAN (R 3.4.0)                      
##  survival     * 2.41-3     2017-04-04 CRAN (R 3.4.1)                      
##  tibble       * 1.3.3      2017-05-28 CRAN (R 3.4.0)                      
##  tidyr        * 0.6.3      2017-05-15 CRAN (R 3.4.0)                      
##  tidytext     * 0.1.3      2017-06-19 CRAN (R 3.4.1)                      
##  tidyverse    * 1.1.1.9000 2017-07-19 Github (tidyverse/tidyverse@a028619)
##  titanic      * 0.1.0      2015-08-31 CRAN (R 3.4.0)                      
##  tm           * 0.7-1      2017-03-02 CRAN (R 3.4.0)                      
##  tokenizers     0.1.4      2016-08-29 CRAN (R 3.4.0)                      
##  tools          3.4.1      2017-07-07 local                               
##  topicmodels  * 0.2-6      2017-04-18 CRAN (R 3.4.0)                      
##  tree         * 1.0-37     2016-01-21 CRAN (R 3.4.0)                      
##  tweenr       * 0.1.5      2016-10-10 CRAN (R 3.4.0)                      
##  useful         1.2.3      2017-06-07 CRAN (R 3.4.0)                      
##  utils        * 3.4.1      2017-07-07 local                               
##  viridisLite    0.2.0      2017-03-24 CRAN (R 3.4.0)                      
##  withr          2.0.0      2017-07-28 CRAN (R 3.4.1)                      
##  xml2           1.1.1      2017-01-24 CRAN (R 3.4.0)                      
##  xtable         1.8-2      2016-02-05 CRAN (R 3.4.0)                      
##  yaml           2.1.14     2016-11-12 CRAN (R 3.4.0)                      
##  zoo          * 1.8-0      2017-04-12 CRAN (R 3.4.0)</code></pre>
</div>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>At least, to the extent that non-constancy isn’t specified in the model, e.g. through interaction terms.<a href="#fnref1">↩</a></p></li>
<li id="fn2"><p>We don’t have information on most of the explanatory variables for pre-1972 elections.<a href="#fnref2">↩</a></p></li>
<li id="fn3"><p>Alternative approaches include Bayesian shrinkage estimators, which are combinations of complete and no pooling estimates using the reliability of the group-specific estimate to determine the weighted average. Though biased towards the overall mean, this estimator is more precise than either complete or no pooling estimates since it accounts for uncertainty in the model. Groups with larger sample sizes will be weighted more heavily towards the mean value of the observations within the group since we possess more certainty about the group-specific contribution to the model.<a href="#fnref3">↩</a></p></li>
</ol>
</div>

<p>This work is licensed under the  <a href="http://creativecommons.org/licenses/by-nc/4.0/">CC BY-NC 4.0 Creative Commons License</a>.</p>


</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
