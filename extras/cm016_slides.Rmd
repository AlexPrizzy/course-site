---
title: "Text analysis: topic modeling"
author: |
  | MACS 30500
  | University of Chicago
date: "March 1, 2017"
output: rcfss::cfss_slides
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(cache = TRUE,
                      message = FALSE,
                      warning = FALSE,
                      include = FALSE,
                      echo = FALSE)

library(tidyverse)
theme_set(theme_minimal(base_size = 18))
```

## Topic modeling

* Keywords
* Links
* Themes
* Probabilistic topic models
    * Latent Dirichlet allocation

## Food and animals

1. I ate a banana and spinach smoothie for breakfast.
1. I like to eat broccoli and bananas.
1. Chinchillas and kittens are cute.
1. My sister adopted a kitten yesterday.
1. Look at this cute hamster munching on a piece of broccoli.

## LDA document structure

* Decide on the number of words N the document will have
    * [Dirichlet probability distribution](https://en.wikipedia.org/wiki/Dirichlet_distribution)
    * Fixed set of $k$ topics
* Generate each word in the document:
    * Pick a topic
    * Generate the word
* LDA backtracks from this assumption

## Food and animals

* Decide that $D$ will be 1/2 about food and 1/2 about cute animals.
* Pick 5 to be the number of words in $D$.
* Pick the first word to come from the food topic
* Pick the second word to come from the cute animals topic
* Pick the third word to come from the cute animals topic
* Pick the fourth word to come from the food topic
* Pick the fifth word to come from the food topic

## How does LDA learn?

* Randomly assign each word in the document to one of $K$ topics
* For each document $d$:
    * Go through each word $w$ in $d$
        * And for each topic $t$, compute two things:
            1. $p(t | d)$
            1. $p(w | t)$
        * Reassign $w$ a new topic $t$ with probability $p(t|d) \times p(w|t)$
* Rinse and repeat

* Estimate from LDA
    1. The topic mixtures of each document
    1. The words associated to each topic

## LDA with a known topic structure

* *Great Expectations* by Charles Dickens
* *The War of the Worlds* by H.G. Wells
* *Twenty Thousand Leagues Under the Sea* by Jules Verne
* *Pride and Prejudice* by Jane Austen

```{r topic_books}
titles <- c("Twenty Thousand Leagues under the Sea", "The War of the Worlds",
            "Pride and Prejudice", "Great Expectations")

library(gutenbergr)

books <- gutenberg_works(title %in% titles) %>%
  gutenberg_download(meta_fields = "title", mirror = "ftp://aleph.gutenberg.org/")
```

```{r word_counts, dependson = "topic_books", include = FALSE, echo = TRUE}
library(tidytext)
library(stringr)

by_chapter <- books %>%
  group_by(title) %>%
  mutate(chapter = cumsum(str_detect(text, regex("^chapter ", ignore_case = TRUE)))) %>%
  ungroup() %>%
  filter(chapter > 0)

by_chapter_word <- by_chapter %>%
  unite(title_chapter, title, chapter) %>%
  unnest_tokens(word, text)

word_counts <- by_chapter_word %>%
  anti_join(stop_words) %>%
  count(title_chapter, word, sort = TRUE) %>%
  ungroup()

word_counts
```

## `topicmodels`

```{r chapters_dtm, include = TRUE}
chapters_dtm <- word_counts %>%
  cast_dtm(title_chapter, word, n)
chapters_dtm
```

```{r chapters_lda}
library(topicmodels)
chapters_lda <- LDA(chapters_dtm, k = 4, control = list(seed = 1234))
chapters_lda
```

## Terms associated with each topic

```{r chapters_lda_td}
library(tidytext)

chapters_lda_td <- tidy(chapters_lda)
chapters_lda_td
```

```{r top_terms}
top_terms <- chapters_lda_td %>%
  group_by(topic) %>%
  top_n(5, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)
top_terms
```

```{r top_terms_plot, fig.height=6, fig.width=7, include = TRUE}
top_terms %>%
  mutate(term = reorder(term, beta)) %>%
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_bar(alpha = 0.8, stat = "identity", show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  coord_flip()
```

## Per-document classification

```{r chapters_lda_gamma_raw}
chapters_lda_gamma <- tidy(chapters_lda, matrix = "gamma")
chapters_lda_gamma
```

```{r chapters_lda_gamma}
chapters_lda_gamma <- chapters_lda_gamma %>%
  separate(document, c("title", "chapter"), sep = "_", convert = TRUE)
chapters_lda_gamma
```

```{r chapters_lda_gamma_plot, fig.width=8, fig.height=6, include = TRUE}
ggplot(chapters_lda_gamma, aes(gamma, fill = factor(topic))) +
  geom_histogram() +
  facet_wrap(~ title, nrow = 2)
```

```{r chapter_classifications}
chapter_classifications <- chapters_lda_gamma %>%
  group_by(title, chapter) %>%
  top_n(1, gamma) %>%
  ungroup() %>%
  arrange(gamma)

chapter_classifications
```

## Consensus topic

```{r book_topics, include = TRUE}
book_topics <- chapter_classifications %>%
  count(title, topic) %>%
  top_n(1, n) %>%
  ungroup() %>%
  transmute(consensus = title, topic)

book_topics
```

## Mis-identification

```{r, include = TRUE, echo = TRUE}
chapter_classifications %>%
  inner_join(book_topics, by = "topic") %>%
  count(title, consensus) %>%
  knitr::kable()
```

## Incorrectly classified words

```{r assignments}
assignments <- augment(chapters_lda, data = chapters_dtm)
```

```{r assignments2, dependson = "assignments"}
assignments <- assignments %>%
  separate(document, c("title", "chapter"), sep = "_", convert = TRUE) %>%
  inner_join(book_topics, by = c(".topic" = "topic"))

assignments
```

```{r dependson = "assignments2", include = TRUE}
assignments %>%
  count(title, consensus, wt = count) %>%
  spread(consensus, n, fill = 0) %>%
  knitr::kable()
```

## Most commonly mistaken words

```{r wrong_words, dependson = "assignments2", include = TRUE}
wrong_words <- assignments %>%
  filter(title != consensus)

wrong_words %>%
  count(title, consensus, term, wt = count) %>%
  ungroup() %>%
  arrange(desc(n))
```

```{r dependson = "word_counts"}
word_counts %>%
  filter(word == "flopson")
```

## LDA with an unknown topic structure

## Associated Press articles

```{r associated_press}
data("AssociatedPress", package = "topicmodels")

ap_td <- tidy(AssociatedPress)
ap_td
```

```{r ap_stopwords, dependson = "associated_press", include = TRUE}
ap_dtm <- ap_td %>%
  anti_join(stop_words, by = c(term = "word")) %>%
  cast_dtm(document, term, count)
ap_dtm
```

----

```{r ap_topic_4, dependson = "associated_press"}
ap_lda <- LDA(ap_dtm, k = 4, control = list(seed = 1234))
ap_lda
```

```{r ap_4_topn, include = TRUE}
ap_lda_td <- tidy(ap_lda)

top_terms <- ap_lda_td %>%
  group_by(topic) %>%
  top_n(5, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)

top_terms %>%
  mutate(term = reorder(term, beta)) %>%
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_bar(alpha = 0.8, stat = "identity", show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free", ncol = 2) +
  coord_flip()
```

----

```{r ap_topic_12, dependson = "associated_press"}
ap_lda <- LDA(ap_dtm, k = 12, control = list(seed = 1234))
ap_lda
```

```{r ap_12_topn, dependson="ap_topic_12", include = TRUE, fig.height = 8}
ap_lda_td <- tidy(ap_lda)

top_terms <- ap_lda_td %>%
  group_by(topic) %>%
  top_n(5, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)

top_terms %>%
  mutate(term = reorder(term, beta)) %>%
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_bar(alpha = 0.8, stat = "identity", show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free", ncol = 3) +
  coord_flip()
```

## Perplexity

* A statistical measure of how well a probability model predicts a sample
* Given the theoretical word distributions represented by the topics, compare that to the actual topic mixtures, or distribution of words in your documents
* Perplexity for LDA model with 12 topics
    * `r topicmodels::perplexity(ap_lda)`

```{r ap_12_perplex, dependson="ap_topic_12"}
perplexity(ap_lda)
```

----

```{r ap_lda_compare, dependson="associated_press"}
n_topics <- c(2, 4, 10, 20, 50, 100)

if(file.exists("../extras/ap_lda_compare.Rdata")){
  load(file = "../extras/ap_lda_compare.Rdata")
} else{
  ap_lda_compare <- n_topics %>%
    map(LDA, x = ap_dtm, control = list(seed = 1234))
}
```

```{r ap_lda_compare_viz, dependson="ap_lda_compare", include = TRUE} 
data_frame(k = n_topics,
           perplex = map_dbl(ap_lda_compare, perplexity)) %>%
  ggplot(aes(k, perplex)) +
  geom_point() +
  geom_line() +
  labs(title = "Evaluating LDA topic models",
       subtitle = "Optimal number of topics (smaller is better)",
       x = "Number of topics",
       y = "Perplexity")
```

## Topics from $k=100$

```{r ap_100_topn, dependson="ap_lda_compare", include = TRUE}
ap_lda_td <- tidy(ap_lda_compare[[6]])

top_terms <- ap_lda_td %>%
  group_by(topic) %>%
  top_n(5, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)

top_terms %>%
  filter(topic <= 12) %>%
  mutate(term = reorder(term, beta)) %>%
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_bar(alpha = 0.8, stat = "identity", show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free", ncol = 3) +
  coord_flip()
```

## Acknowledgments

* This page is derived in part from ["Tidy Text Mining with R"](http://tidytextmining.com/) and licensed under a [Creative Commons Attribution-NonCommercial-ShareAlike 3.0 United States License](https://creativecommons.org/licenses/by-nc-sa/3.0/us/).
* This page is derived in part from ["What is a good explanation of Latent Dirichlet Allocation?"](https://www.quora.com/What-is-a-good-explanation-of-Latent-Dirichlet-Allocation)


