<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />


<meta name="author" content="MACS 30200 - Perspectives on Computational Research" />


<title>Diagnostic tests for OLS/GLM</title>

<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/readable.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="site_libs/jqueryui-1.11.4/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<script src="site_libs/navigation-1.1/codefolding.js"></script>
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-45631879-2', 'auto');
  ga('send', 'pageview');

</script>


<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
div.sourceCode {
  overflow-x: visible;
}
</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>


<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>


</head>

<body>

<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
.tabbed-pane {
  padding-top: 12px;
}
button.code-folding-btn:focus {
  outline: none;
}
</style>


<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 66px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 71px;
  margin-top: -71px;
}

.section h2 {
  padding-top: 71px;
  margin-top: -71px;
}
.section h3 {
  padding-top: 71px;
  margin-top: -71px;
}
.section h4 {
  padding-top: 71px;
  margin-top: -71px;
}
.section h5 {
  padding-top: 71px;
  margin-top: -71px;
}
.section h6 {
  padding-top: 71px;
  margin-top: -71px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>


<div class="container-fluid main-container">

<!-- tabsets -->
<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});
</script>

<!-- code folding -->
<style type="text/css">
.code-folding-btn { margin-bottom: 4px; }
</style>
<script>
$(document).ready(function () {
  window.initializeCodeFolding("hide" === "show");
});
</script>




<script>
$(document).ready(function ()  {

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_').toLowerCase();
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}


.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
  padding-left: 25px;
  text-indent: 0;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>

<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row-fluid">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">Computing for the Social Sciences</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">Home</a>
</li>
<li>
  <a href="faq.html">FAQ</a>
</li>
<li>
  <a href="syllabus.html">Syllabus</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div class="fluid-row" id="header">

<div class="btn-group pull-right">
<button type="button" class="btn btn-default btn-xs dropdown-toggle" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false"><span>Code</span> <span class="caret"></span></button>
<ul class="dropdown-menu" style="min-width: 50px;">
<li><a id="rmd-show-all-code" href="#">Show All Code</a></li>
<li><a id="rmd-hide-all-code" href="#">Hide All Code</a></li>
</ul>
</div>



<h1 class="title toc-ignore">Diagnostic tests for OLS/GLM</h1>
<h4 class="author"><em>MACS 30200 - Perspectives on Computational Research</em></h4>

</div>


<div id="objectives" class="section level1">
<h1>Objectives</h1>
<ul>
<li>Identify key assumptions of linear regression models</li>
<li>Explain how to use residual plots for dianosing violations of regression assumptions</li>
<li>Introduce methods for detecting and resolving unusual and influential data</li>
<li>Introduce methods for detecting and resolving non-normally distributed errors</li>
<li>Introduce methods for detecting and resolving non-constant variance of error terms</li>
<li>Introduce methods for detecting and resolving non-linearity in the data</li>
<li>Introduce methods for detecting and resolving collinearity</li>
<li>Identify how to extend these methods to GLMs</li>
</ul>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(tidyverse)
<span class="kw">library</span>(forcats)
<span class="kw">library</span>(broom)
<span class="kw">library</span>(modelr)
<span class="kw">library</span>(stringr)
<span class="kw">library</span>(ISLR)
<span class="kw">library</span>(titanic)
<span class="kw">library</span>(rcfss)
<span class="kw">library</span>(haven)
<span class="kw">library</span>(car)

<span class="kw">options</span>(<span class="dt">digits =</span> <span class="dv">3</span>)
<span class="kw">set.seed</span>(<span class="dv">1234</span>)
<span class="kw">theme_set</span>(<span class="kw">theme_minimal</span>())</code></pre></div>
</div>
<div id="assumptions-of-linear-regression-models" class="section level1">
<h1>Assumptions of linear regression models</h1>
<p>Basic linear regression follows the functional form:</p>
<p><span class="math display">\[Y_i = \alpha + \beta x_i + \epsilon_i\]</span></p>
<p>where <span class="math inline">\(Y_i\)</span> is the value of the response variable <span class="math inline">\(Y\)</span> for the <span class="math inline">\(i\)</span>th observation, <span class="math inline">\(x_i\)</span> is the value for the explanatory variable <span class="math inline">\(X\)</span> for the <span class="math inline">\(i\)</span>th observation. The coefficients <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span> are <strong>population regression coefficients</strong> - our goal is to estimate these population parameters given the observed data. <span class="math inline">\(\epsilon_i\)</span> is the error representing the aggregated omitted causes of <span class="math inline">\(Y\)</span>, other explanatory variables that could be included in the model, measurement error in <span class="math inline">\(Y\)</span>, and any inherently random component of <span class="math inline">\(Y\)</span>.</p>
<p>The key assumptions of linear regression concern the behavior of the errors.</p>
<div id="linearity" class="section level4">
<h4>Linearity</h4>
<p>The expectation of the error<a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a> is 0:</p>
<p><span class="math display">\[E(\epsilon_i) \equiv E(\epsilon | x_i) = 0\]</span></p>
<p>This allows us to recover the expected value of the response variable as a linear function of the explanatory variable:</p>
<p><span class="math display">\[\mu_i \equiv E(Y_i) \equiv E(Y | x_i) = E(\alpha + \beta x_i + \epsilon)\]</span> <span class="math display">\[\mu_i = \alpha + \beta x_i + E(\epsilon)\]</span> <span class="math display">\[\mu_i = \alpha + \beta x_i + 0\]</span> <span class="math display">\[\mu_i = \alpha + \beta x_i\]</span></p>
<blockquote>
<p>Because <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span> are fixed parameters in the population, we can remove them from the expectation operator.</p>
</blockquote>
</div>
<div id="constant-variance" class="section level4">
<h4>Constant variance</h4>
<p>The variance of the errors is the same regardless of the values of <span class="math inline">\(X\)</span>:</p>
<p><span class="math display">\[V(\epsilon | x_i) = \sigma_{\epsilon}^2\]</span></p>
</div>
<div id="normality" class="section level4">
<h4>Normality</h4>
<p>The errors are assumped to be normally distributed:</p>
<p><span class="math display">\[\epsilon \sim N(0, \sigma_\epsilon^2)\]</span></p>
</div>
<div id="independence" class="section level4">
<h4>Independence</h4>
<p>Observations are sampled independently from one another. Any pair of errors <span class="math inline">\(\epsilon_i\)</span> and <span class="math inline">\(\epsilon_j\)</span> are independent for <span class="math inline">\(i \neq j\)</span>. Simple random sampling from a large population will ensure this assumption is met. However data collection procedures frequently (and explicitly) violate this assumption (e.g. time series data, panel survey data).</p>
</div>
<div id="fixed-x-or-x-measured-without-error-and-independent-of-the-error" class="section level4">
<h4>Fixed <span class="math inline">\(X\)</span>, or <span class="math inline">\(X\)</span> measured without error and independent of the error</h4>
<p><span class="math inline">\(X\)</span> is assumed to be fixed or measured without error and independent of the error. With a fixed <span class="math inline">\(X\)</span>, the researcher controls the precise value of <span class="math inline">\(X\)</span> for a given observation (think experimental design with treatment/control). In observational study, we assume <span class="math inline">\(X\)</span> is measured without error and that the explantory variable and the error are independent in the population from which the sample is drawn.</p>
<p><span class="math display">\[\epsilon_i \sim N(0, \sigma_\epsilon^2), \text{for } i = 1, \dots, n\]</span></p>
</div>
<div id="x-is-not-invariant" class="section level4">
<h4><span class="math inline">\(X\)</span> is not invariant</h4>
<p>If <span class="math inline">\(X\)</span> is fixed, it must vary (i.e. it’s values cannot all be the same). If <span class="math inline">\(X\)</span> is random, then in the population <span class="math inline">\(X\)</span> must vary. You cannot estimate a regression line for an invariant <span class="math inline">\(X\)</span>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">data_frame</span>(<span class="dt">x =</span> <span class="dv">1</span>,
           <span class="dt">y =</span> <span class="kw">rnorm</span>(<span class="dv">10</span>)) %&gt;%
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(x, y)) +
<span class="st">  </span><span class="kw">geom_point</span>() +
<span class="st">  </span><span class="kw">geom_smooth</span>(<span class="dt">method =</span> <span class="st">&quot;lm&quot;</span>) +
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="st">&quot;You cannot regress this&quot;</span>,
       <span class="dt">subtitle =</span> <span class="st">&quot;Slope is undefined&quot;</span>)</code></pre></div>
<p><img src="persp012_regression_diagnostics_files/figure-html/invariant-1.png" width="672" /></p>
</div>
<div id="handling-violations-of-assumptions" class="section level2">
<h2>Handling violations of assumptions</h2>
<p>If these assumptions are violated, conducting inference from linear regression becomes tricky, biased, inefficient, and/or error prone. You could move to a more robust inferential method such as nonparametric regression, decision trees, support vector machines, etc., but these methods are more tricky to generate inference about the explanatory variables. Instead, we can attempt to diagnose assumption violations and impose solutions while still constraining ourselves to a linear regression framework.</p>
</div>
</div>
<div id="unusual-and-influential-data" class="section level1">
<h1>Unusual and influential data</h1>
<p><strong>Outliers</strong> are observations that are somehow unusual, either in their value of <span class="math inline">\(Y_i\)</span>, of one or more <span class="math inline">\(X_i\)</span>s, or some combination thereof. Outliers have the potential to have a disproportionate influence on a regression model.</p>
<div id="terms" class="section level2">
<h2>Terms</h2>
<ul>
<li><strong>Outlier</strong> - an observation that has an unusual value on the dependent variable <span class="math inline">\(Y\)</span> given its particular combination of values on <span class="math inline">\(X\)</span></li>
<li><strong>Leverage</strong> - degree of potential influence on the coefficient estimates that a given observation can (but not necessarily does) have</li>
<li><strong>Discrepancy</strong> - extent to which an observation is “unusual” or “different” from the rest of the data</li>
<li><p><strong>Influence</strong> - how much effect a particular observation’s value(s) on <span class="math inline">\(Y\)</span> and <span class="math inline">\(X\)</span> have on the coefficient estimates. Influence is a function of leverage and discrepancy:</p>
<p><span class="math display">\[\text{Influence} = \text{Leverage} \times \text{Discrepancy}\]</span></p></li>
</ul>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">flintstones &lt;-<span class="st"> </span><span class="kw">tribble</span>(
  ~name,    ~x, ~y,
  <span class="st">&quot;Barney&quot;</span>, <span class="dv">13</span>, <span class="dv">75</span>,
  <span class="st">&quot;Dino&quot;</span>,   <span class="dv">24</span>, <span class="dv">300</span>,
  <span class="st">&quot;Betty&quot;</span>,  <span class="dv">14</span>, <span class="dv">250</span>,
  <span class="st">&quot;Fred&quot;</span>,   <span class="dv">10</span>, <span class="dv">220</span>,
  <span class="st">&quot;Wilma&quot;</span>,  <span class="dv">8</span>,  <span class="dv">210</span>
)

<span class="kw">ggplot</span>(flintstones, <span class="kw">aes</span>(x, y, <span class="dt">label =</span> name)) +
<span class="st">  </span><span class="kw">geom_smooth</span>(<span class="dt">data =</span> <span class="kw">filter</span>(flintstones, name %in%<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;Wilma&quot;</span>, <span class="st">&quot;Fred&quot;</span>, <span class="st">&quot;Betty&quot;</span>)),
              <span class="dt">method =</span> <span class="st">&quot;lm&quot;</span>, <span class="dt">se =</span> <span class="ot">FALSE</span>, <span class="dt">fullrange =</span> <span class="ot">TRUE</span>, <span class="dt">color =</span> <span class="st">&quot;gray&quot;</span>,
              <span class="kw">aes</span>(<span class="dt">linetype =</span> <span class="st">&quot;Betty + Fred + Wilma&quot;</span>)) +
<span class="st">  </span><span class="kw">geom_smooth</span>(<span class="dt">data =</span> <span class="kw">filter</span>(flintstones, name !=<span class="st"> &quot;Dino&quot;</span>),
              <span class="dt">method =</span> <span class="st">&quot;lm&quot;</span>, <span class="dt">se =</span> <span class="ot">FALSE</span>, <span class="dt">fullrange =</span> <span class="ot">TRUE</span>, <span class="dt">color =</span> <span class="st">&quot;gray&quot;</span>,
              <span class="kw">aes</span>(<span class="dt">linetype =</span> <span class="st">&quot;Barney + Betty + Fred + Wilma&quot;</span>)) +
<span class="st">  </span><span class="kw">geom_smooth</span>(<span class="dt">data =</span> <span class="kw">filter</span>(flintstones, name !=<span class="st"> &quot;Barney&quot;</span>),
              <span class="dt">method =</span> <span class="st">&quot;lm&quot;</span>, <span class="dt">se =</span> <span class="ot">FALSE</span>, <span class="dt">fullrange =</span> <span class="ot">TRUE</span>, <span class="dt">color =</span> <span class="st">&quot;gray&quot;</span>,
              <span class="kw">aes</span>(<span class="dt">linetype =</span> <span class="st">&quot;Betty + Dino + Fred + Wilma&quot;</span>)) +
<span class="st">  </span><span class="kw">scale_linetype_manual</span>(<span class="dt">values =</span> <span class="kw">c</span>(<span class="dv">3</span>,<span class="dv">2</span>,<span class="dv">1</span>),
                        <span class="dt">guide =</span> <span class="kw">guide_legend</span>(<span class="dt">nrow =</span> <span class="dv">3</span>,
                                             <span class="dt">reverse =</span> <span class="ot">TRUE</span>)) +
<span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">size =</span> <span class="dv">2</span>) +
<span class="st">  </span>ggrepel::<span class="kw">geom_label_repel</span>() +
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">linetype =</span> <span class="ot">NULL</span>) +
<span class="st">  </span><span class="kw">theme</span>(<span class="dt">legend.position =</span> <span class="st">&quot;bottom&quot;</span>)</code></pre></div>
<p><img src="persp012_regression_diagnostics_files/figure-html/flintstones-sim-1.png" width="672" /></p>
<ul>
<li>Dino is an observation with high leverage but low discrepancy (close to the regression line defined by Betty, Fred, and Wilma). Therefore he has little impact on the regression line (long dashed line); his influence is low because his discrepancy is low.</li>
<li>Barney has high leverage (though lower than Dino) and high discrepancy, so he substantially influences the regression results (short-dashed line).</li>
</ul>
</div>
<div id="measuring-leverage" class="section level2">
<h2>Measuring leverage</h2>
<p>Leverage is typically assessed using the <strong>leverage</strong> (<strong>hat</strong>) <strong>statistic</strong>:</p>
<p><span class="math display">\[h_i = \frac{1}{n} + \frac{(X_i - \bar{X})^2}{\sum_{i&#39;=1}^{n} (X_{i&#39;} - \bar{X})^2}\]</span></p>
<p>Generalized to the multivariate case:</p>
<p><span class="math display">\[h_i = \mathbf{X}_i (\mathbf{X&#39;X})^{-1} \mathbf{X&#39;}_i\]</span></p>
<ul>
<li>It is solely a function of <span class="math inline">\(X\)</span></li>
<li>Larger values indicate higher leverage</li>
<li><span class="math inline">\(\frac{1}{n} \leq h_i \leq 1\)</span></li>
<li><span class="math inline">\(\bar{h} = \frac{(p + 1)}{n}\)</span></li>
</ul>
<p>Observations with a leverage statistic greater than the average could have high leverage.</p>
</div>
<div id="measuring-discrepancy" class="section level2">
<h2>Measuring discrepancy</h2>
<p>Residuals are a natural way to look for discrepant or outlying observations (discrepant obserations typically have large residuals, or differences between actual and fitted values for <span class="math inline">\(y_i\)</span>.) The problem is that variability of the errors <span class="math inline">\(E_i\)</span> do not have equal variances, even if the actual errors <span class="math inline">\(\epsilon_i\)</span> do have equal variances:</p>
<p><span class="math display">\[V(E_i) = \sigma_\epsilon^2 (1 - h_i)\]</span></p>
<p>High leverage observations tend to have small residuals, which makes sense because they pull the regression line towards them. Alternatively we can calculate a <strong>standardized residual</strong> which parses out the variability in <span class="math inline">\(X_i\)</span> for <span class="math inline">\(E_i\)</span>:</p>
<p><span class="math display">\[E&#39;_i \equiv \frac{E_i}{S_{E} \sqrt{1 - h_i}}\]</span></p>
<p>where <span class="math inline">\(S_E\)</span> is the standard error of the regression:</p>
<p><span class="math display">\[S_E = \sqrt{\frac{E_i^2}{(n - k - 1)}}\]</span></p>
<p>The problem is that the numerator and the denominator are not independent - they both contain <span class="math inline">\(E_i\)</span>, so <span class="math inline">\(E&#39;_i\)</span> does not follow a <span class="math inline">\(t\)</span>-distribution. Instead, we can modify this measure by calculating <span class="math inline">\(S_{E(-i)}\)</span>; that is, refit the model deleting each <span class="math inline">\(i\)</span>th observation, estimating the standard error of the regression <span class="math inline">\(S_{E(-i)}\)</span> based on the remaining <span class="math inline">\(i-1\)</span> observations. We then calculate the <strong>studentized residual</strong>:</p>
<p><span class="math display">\[E_i^{\ast} \equiv \frac{E_i}{S_{E(-i)} \sqrt{1 - h_i}}\]</span></p>
<p>which now has an independent numerator and denominator and follows a <span class="math inline">\(t\)</span>-distribution with <span class="math inline">\(n-k-2\)</span> degrees of freedom. They are on a common scale and we should expect roughly 95% of the studentized residuals to fall within the interval <span class="math inline">\([-2,2]\)</span>.</p>
</div>
<div id="measuring-influence" class="section level2">
<h2>Measuring influence</h2>
<p>As described previously, influence is the a combination of an observation’s leverage and discrepancy. In other words, influence is the effect of a particular observation on the coefficient estimates. A simple measure of that influence is the difference between the coefficient estimate with and without the observation in question:</p>
<p><span class="math display">\[D_{ij} = \hat{\beta_j} - \hat{\beta}_{j(-i)}, \text{for } i=1, \dots, n \text{ and } j = 0, \dots, k\]</span></p>
<p>This measure is called <span class="math inline">\(\text{DFBETA}_{ij}\)</span>. Since coefficient estimates are scaled differently depending on how the variables are scaled, we can rescale <span class="math inline">\(\text{DFBETA}_{ij}\)</span> by the coefficient’s standard error to account for this fact:</p>
<p><span class="math display">\[D^{\ast}_{ij} = \frac{D_{ij}}{SE_{-i}(\beta_j)}\]</span></p>
<p>This measure is called <span class="math inline">\(\text{DFBETAS}_{ij}\)</span>.</p>
<ul>
<li>Positive values of <span class="math inline">\(\text{DFBETAS}_{ij}\)</span> correspond to observations which <strong>decrease</strong> the estimate of <span class="math inline">\(\hat{\beta}_j\)</span></li>
<li>Negative values of <span class="math inline">\(\text{DFBETAS}_{ij}\)</span> correspond to observations which <strong>increase</strong> the estimate of <span class="math inline">\(\hat{\beta}_j\)</span></li>
</ul>
<p>Frequently <span class="math inline">\(\text{DFBETA}\)</span>s are used to construct summary statistics of each observation’s influence on the regression model. <strong>Cook’s D</strong> is based on the theory that one could conduct an <span class="math inline">\(F\)</span>-test on each observation for the hypothesis that <span class="math inline">\(\beta_j = \hat{\beta}_{k(-i)} \forall j \in J\)</span>. The formula for this measure is:</p>
<p><span class="math display">\[D_i = \frac{E^{&#39;2}_i}{k + 1} \times \frac{h_i}{1 - h_i}\]</span></p>
<p>where <span class="math inline">\(\tilde{u}_i^2\)</span> is the squared standardized residual, <span class="math inline">\(k\)</span> is the number of parameters in the model, and <span class="math inline">\(\frac{h_i}{1 - h_i}\)</span> is the hat value. We look for values of <span class="math inline">\(D_i\)</span> that stand out from the rest.</p>
</div>
<div id="visualizing-leverage-discrepancy-and-influence" class="section level2">
<h2>Visualizing leverage, discrepancy, and influence</h2>
<p>For example, here are the results of a basic model of the number of federal laws struck down by the U.S. Supreme Court in each Congress, based on:</p>
<ol style="list-style-type: decimal">
<li><strong>Age</strong> - the mean age of the members of the Supreme Court</li>
<li><strong>Tenure</strong> - mean tenure of the members of the Court</li>
<li><strong>Unified</strong> - a dummy variable indicating whether or not the Congress was controlled by the same party in that period</li>
</ol>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># read in data and estimate model</span>
dahl &lt;-<span class="st"> </span><span class="kw">read_dta</span>(<span class="st">&quot;data/LittleDahl.dta&quot;</span>)
dahl_mod &lt;-<span class="st"> </span><span class="kw">lm</span>(nulls ~<span class="st"> </span>age +<span class="st"> </span>tenure +<span class="st"> </span>unified, <span class="dt">data =</span> dahl)
<span class="kw">tidy</span>(dahl_mod)</code></pre></div>
<pre><code>##          term estimate std.error statistic  p.value
## 1 (Intercept) -12.1034    2.5432     -4.76 6.57e-06
## 2         age   0.2189    0.0448      4.88 4.01e-06
## 3      tenure  -0.0669    0.0643     -1.04 3.00e-01
## 4     unified   0.7176    0.4584      1.57 1.21e-01</code></pre>
<p>A major concern with regression analysis of this data is that the results are being driven by outliers in the data.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">dahl &lt;-<span class="st"> </span>dahl %&gt;%
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">year =</span> congress *<span class="st"> </span><span class="dv">2</span> +<span class="st"> </span><span class="dv">1787</span>)

<span class="kw">ggplot</span>(dahl, <span class="kw">aes</span>(year, nulls)) +
<span class="st">  </span><span class="kw">geom_line</span>() +
<span class="st">  </span><span class="kw">geom_vline</span>(<span class="dt">xintercept =</span> <span class="dv">1935</span>, <span class="dt">linetype =</span> <span class="dv">2</span>) +
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">x =</span> <span class="st">&quot;Year&quot;</span>,
       <span class="dt">y =</span> <span class="st">&quot;Congressional laws struck down&quot;</span>)</code></pre></div>
<p><img src="persp012_regression_diagnostics_files/figure-html/dahl-time-1.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>(dahl, <span class="kw">aes</span>(year, age)) +
<span class="st">  </span><span class="kw">geom_line</span>() +
<span class="st">  </span><span class="kw">geom_vline</span>(<span class="dt">xintercept =</span> <span class="dv">1935</span>, <span class="dt">linetype =</span> <span class="dv">2</span>) +
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">x =</span> <span class="st">&quot;Year&quot;</span>,
       <span class="dt">y =</span> <span class="st">&quot;Mean age of justices on the Court&quot;</span>)</code></pre></div>
<p><img src="persp012_regression_diagnostics_files/figure-html/dahl-time-2.png" width="672" /></p>
<p>During the 74th Congress (1935-36), the New Deal/Court-packing crisis was associated with an abnormally large number of laws struck down by the court. We should determine whether or not this observation is driving our results.</p>
<p>By combining all three variables into a “bubble plot”, we can visualize all three variables simultaneously.</p>
<ul>
<li>Each observation’s leverage (<span class="math inline">\(h_i\)</span>) is plotted on the <span class="math inline">\(x\)</span> axis</li>
<li>Each observation’s discrepancy (i.e. Studentized residual) is plotted on the <span class="math inline">\(y\)</span> axis</li>
<li>Each symbol is drawn proportional to the observation’s Cook’s <span class="math inline">\(D_i\)</span></li>
</ul>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># add key statistics</span>
dahl_augment &lt;-<span class="st"> </span>dahl %&gt;%
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">hat =</span> <span class="kw">hatvalues</span>(dahl_mod),
         <span class="dt">student =</span> <span class="kw">rstudent</span>(dahl_mod),
         <span class="dt">cooksd =</span> <span class="kw">cooks.distance</span>(dahl_mod))

<span class="co"># draw bubble plot</span>
<span class="kw">ggplot</span>(dahl_augment, <span class="kw">aes</span>(hat, student)) +
<span class="st">  </span><span class="kw">geom_hline</span>(<span class="dt">yintercept =</span> <span class="dv">0</span>, <span class="dt">linetype =</span> <span class="dv">2</span>) +
<span class="st">  </span><span class="kw">geom_point</span>(<span class="kw">aes</span>(<span class="dt">size =</span> cooksd), <span class="dt">shape =</span> <span class="dv">1</span>) +
<span class="st">  </span><span class="kw">geom_text</span>(<span class="dt">data =</span> dahl_augment %&gt;%
<span class="st">              </span><span class="kw">arrange</span>(-cooksd) %&gt;%
<span class="st">              </span><span class="kw">slice</span>(<span class="dv">1</span>:<span class="dv">10</span>),
            <span class="kw">aes</span>(<span class="dt">label =</span> Congress)) +
<span class="st">  </span><span class="kw">scale_size_continuous</span>(<span class="dt">range =</span> <span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">20</span>)) +
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">x =</span> <span class="st">&quot;Leverage&quot;</span>,
       <span class="dt">y =</span> <span class="st">&quot;Studentized residual&quot;</span>) +
<span class="st">  </span><span class="kw">theme</span>(<span class="dt">legend.position =</span> <span class="st">&quot;none&quot;</span>)</code></pre></div>
<p><img src="persp012_regression_diagnostics_files/figure-html/bubble-1.png" width="672" /></p>
<p>The bubble plot tells us several things:</p>
<ul>
<li>The size/color of the symbols is proportional to Cook’s D, which is in turn a multiplicative function of the square of the Studentized residuals (Y axis) and the leverage (X axis), so observations farther away from <span class="math inline">\(Y=0\)</span> and/or have higher values of <span class="math inline">\(X\)</span> will have larger symbols.</li>
<li>The plot tells us whether the large influence of an observation is due to high discrepancy, high leverage, or both
<ul>
<li>The 104th Congress has relatively low leverage but is very discrepant</li>
<li>The 74th and 98th Congresses demonstrate both high discrepancy and high leverage</li>
</ul></li>
</ul>
</div>
<div id="numerical-rules-of-thumb" class="section level2">
<h2>Numerical rules of thumb</h2>
<p>These are not hard and fast rules rigorously defended by mathematical proofs; they are simply potential rules of thumb to follow when interpreting the above statistics.</p>
<div id="hat-values" class="section level3">
<h3>Hat-values</h3>
<p>Anything exceeding twice the average <span class="math inline">\(\bar{h} = \frac{k + 1}{n}\)</span> is noteworthy. In our example that would be the following observations:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">dahl_augment %&gt;%
<span class="st">  </span><span class="kw">filter</span>(hat &gt;<span class="st"> </span><span class="dv">2</span> *<span class="st"> </span><span class="kw">mean</span>(hat))</code></pre></div>
<pre><code>## # A tibble: 9 × 10
##   Congress congress nulls   age tenure unified  year    hat student
##      &lt;chr&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;
## 1      1st        1     0  49.8    0.8       1  1789 0.0974   0.330
## 2      3rd        3     0  52.8    4.2       0  1793 0.1132   0.511
## 3     12th       12     0  49.0    6.6       1  1811 0.0802   0.669
## 4     17th       17     0  59.0   16.6       1  1821 0.0887  -0.253
## 5     20th       20     0  61.7   17.4       1  1827 0.0790  -0.577
## 6     23rd       23     0  64.0   18.4       1  1833 0.0819  -0.844
## 7     34th       34     0  64.0   14.6       0  1855 0.0782  -0.561
## 8     36th       36     0  68.7   17.8       0  1859 0.1020  -1.072
## 9     99th       99     3  71.9   16.7       0  1985 0.0912   0.295
## # ... with 1 more variables: cooksd &lt;dbl&gt;</code></pre>
</div>
<div id="studentized-residuals" class="section level3">
<h3>Studentized residuals</h3>
<p>Anything outside of the range <span class="math inline">\([-2,2]\)</span> is discrepant.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">dahl_augment %&gt;%
<span class="st">  </span><span class="kw">filter</span>(<span class="kw">abs</span>(student) &gt;<span class="st"> </span><span class="dv">2</span>)</code></pre></div>
<pre><code>## # A tibble: 7 × 10
##   Congress congress nulls   age tenure unified  year    hat student cooksd
##      &lt;chr&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;
## 1     67th       67     6  66.0    9.0       1  1921 0.0361    2.14 0.0415
## 2     74th       74    10  71.1   14.2       1  1935 0.0514    4.42 0.2229
## 3     90th       90     6  64.7   13.3       1  1967 0.0195    2.49 0.0292
## 4     91st       91     6  65.1   13.0       1  1969 0.0189    2.42 0.0269
## 5     92nd       92     5  62.0    9.2       1  1971 0.0146    2.05 0.0150
## 6     98th       98     7  69.9   14.7       0  1983 0.0730    3.02 0.1655
## 7    104th      104     8  60.6   12.5       1  1995 0.0208    4.48 0.0897</code></pre>
</div>
<div id="influence" class="section level3">
<h3>Influence</h3>
<p><span class="math display">\[D_i &gt; \frac{4}{n - k - 1}\]</span></p>
<p>where <span class="math inline">\(n\)</span> is the number of observations and <span class="math inline">\(k\)</span> is the number of coefficients in the regression model.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">dahl_augment %&gt;%
<span class="st">  </span><span class="kw">filter</span>(cooksd &gt;<span class="st"> </span><span class="dv">4</span> /<span class="st"> </span>(<span class="kw">nrow</span>(.) -<span class="st"> </span>(<span class="kw">length</span>(<span class="kw">coef</span>(dahl_mod)) -<span class="st"> </span><span class="dv">1</span>) -<span class="st"> </span><span class="dv">1</span>))</code></pre></div>
<pre><code>## # A tibble: 4 × 10
##   Congress congress nulls   age tenure unified  year    hat student cooksd
##      &lt;chr&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;
## 1     67th       67     6  66.0    9.0       1  1921 0.0361    2.14 0.0415
## 2     74th       74    10  71.1   14.2       1  1935 0.0514    4.42 0.2229
## 3     98th       98     7  69.9   14.7       0  1983 0.0730    3.02 0.1655
## 4    104th      104     8  60.6   12.5       1  1995 0.0208    4.48 0.0897</code></pre>
</div>
</div>
<div id="how-to-treat-unusual-observations" class="section level2">
<h2>How to treat unusual observations</h2>
<div id="mistakes" class="section level3">
<h3>Mistakes</h3>
<p>If the data is just wrong (miscoded, mismeasured, misentered, etc.), then either fix the error, impute a plausible value for the observation, or omit the offending observation.</p>
</div>
<div id="weird-observations" class="section level3">
<h3>Weird observations</h3>
<p>If the data for a particular observation is just strange, then you may want to ask “why is it so strange?”</p>
<ol style="list-style-type: decimal">
<li>The data are strange because something unusual/weird/singular happened to that data point
<ul>
<li>If that “something” is important to the theory being tested, then you may want to respecify your model</li>
<li>If the answer is no, then you can drop the offending observation from the analysis</li>
</ul></li>
<li>The data are strange for apparent reason
<ul>
<li>Not really a good answer here. Try digging into the history of the observation to find out what is going on.</li>
<li>Dropping the observation is a judgment call</li>
<li>You could always rerun the model omitting the observation and including the results as a footnote (i.e. a robustness check)</li>
</ul></li>
</ol>
<p>For example, let’s reestimate the SCOTUS model and omit observations that were commonly identified as outliers:<a href="#fn2" class="footnoteRef" id="fnref2"><sup>2</sup></a></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">dahl_omit &lt;-<span class="st"> </span>dahl %&gt;%
<span class="st">  </span><span class="kw">filter</span>(!(congress %in%<span class="st"> </span><span class="kw">c</span>(<span class="dv">74</span>, <span class="dv">98</span>, <span class="dv">104</span>)))

dahl_omit_mod &lt;-<span class="st"> </span><span class="kw">lm</span>(nulls ~<span class="st"> </span>age +<span class="st"> </span>tenure +<span class="st"> </span>unified, <span class="dt">data =</span> dahl_omit)

coefplot::<span class="kw">multiplot</span>(dahl_mod, dahl_omit_mod,
                    <span class="dt">names =</span> <span class="kw">c</span>(<span class="st">&quot;All observations&quot;</span>,
                              <span class="st">&quot;Omit outliers&quot;</span>)) +
<span class="st">  </span><span class="kw">theme</span>(<span class="dt">legend.position =</span> <span class="st">&quot;bottom&quot;</span>)</code></pre></div>
<p><img src="persp012_regression_diagnostics_files/figure-html/dahl-reestimate-1.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># rsquared values</span>
<span class="kw">rsquare</span>(dahl_mod, dahl)</code></pre></div>
<pre><code>## [1] 0.232</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">rsquare</span>(dahl_omit_mod, dahl_omit)</code></pre></div>
<pre><code>## [1] 0.258</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># rmse values</span>
<span class="kw">rmse</span>(dahl_mod, dahl)</code></pre></div>
<pre><code>## [1] 1.68</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">rmse</span>(dahl_omit_mod, dahl_omit)</code></pre></div>
<pre><code>## [1] 1.29</code></pre>
<ul>
<li>Not much has changed from the original model
<ul>
<li>Estimate for age is a bit smaller, as well as a smaller standard error</li>
<li>Tenure is also smaller, but only fractionally</li>
<li>Unified is a bit larger and with a smaller standard error</li>
</ul></li>
<li><span class="math inline">\(R^2\)</span> is larger for the omitted observation model, and the RMSE is smaller</li>
<li>These three observations mostly influenced the precision of the estimates (i.e. standard errors), not the accuracy of them</li>
</ul>
</div>
</div>
</div>
<div id="non-normally-distributed-errors" class="section level1">
<h1>Non-normally distributed errors</h1>
<p>Recall that OLS assumes errors are distributed normally:</p>
<p><span class="math display">\[\epsilon \sim N(0, \sigma_\epsilon^2)\]</span></p>
<p>However according to the central limit theorem, inference based on the least-squares estimator is approximately valid under broad conditions.<a href="#fn3" class="footnoteRef" id="fnref3"><sup>3</sup></a> So while the <strong>validity</strong> of the estimates is robust to violating this assumption, the <strong>efficiency</strong> of the estimates is not robust. Recall that efficiency guarantees us the smallest possible sampling variance and therefore the smallest possible mean squared error (MSE). Heavy-tailed or skewed distributions of the errors will therefore give rise to outliers (which we just recognized as a problem). Alternatively, we interpret the least-squares fit as a conditional mean <span class="math inline">\(Y | X\)</span>. But arithmetic means are not good measures of the center of a highly skewed distribution.</p>
<div id="detecting-non-normally-distributed-errors" class="section level2">
<h2>Detecting non-normally distributed errors</h2>
<p>Graphical interpretations are easiest to detect non-normality in the errors. Consider a regression model using survey data from the 1994 wave of Statistics Canada’s Survey of Labour and Income Dynamics (SLID), explaining hourly wages as an outcome of sex, education, and age:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">(slid &lt;-<span class="st"> </span><span class="kw">read_tsv</span>(<span class="st">&quot;http://socserv.socsci.mcmaster.ca/jfox/Books/Applied-Regression-3E/datasets/SLID-Ontario.txt&quot;</span>))</code></pre></div>
<pre><code>## # A tibble: 3,997 × 4
##      age    sex compositeHourlyWages yearsEducation
##    &lt;int&gt;  &lt;chr&gt;                &lt;dbl&gt;          &lt;int&gt;
##  1    40   Male                10.56             15
##  2    19   Male                11.00             13
##  3    46   Male                17.76             14
##  4    50 Female                14.00             16
##  5    31   Male                 8.20             15
##  6    30 Female                16.97             13
##  7    61 Female                 6.70             12
##  8    46 Female                14.00             14
##  9    43   Male                19.20             18
## 10    17   Male                 7.25             11
## # ... with 3,987 more rows</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">slid_mod &lt;-<span class="st"> </span><span class="kw">lm</span>(compositeHourlyWages ~<span class="st"> </span>sex +<span class="st"> </span>yearsEducation +<span class="st"> </span>age, <span class="dt">data =</span> slid)
<span class="kw">tidy</span>(slid_mod)</code></pre></div>
<pre><code>##             term estimate std.error statistic   p.value
## 1    (Intercept)   -8.124   0.59898     -13.6  5.27e-41
## 2        sexMale    3.474   0.20701      16.8  4.04e-61
## 3 yearsEducation    0.930   0.03426      27.1 5.47e-149
## 4            age    0.261   0.00866      30.2 3.42e-180</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">car::<span class="kw">qqPlot</span>(slid_mod)</code></pre></div>
<p><img src="persp012_regression_diagnostics_files/figure-html/slid-1.png" width="672" /></p>
<p>The above figure is a <strong>quantile-comparison plot</strong>, graphing for each observation its studentized residual on the <span class="math inline">\(y\)</span> axis and the corresponding quantile in the <span class="math inline">\(t\)</span>-distribution on the <span class="math inline">\(x\)</span> axis. The dashed lines indicate 95% confidence intervals calculated under the assumption that the errors are normally distributed. If any observations fall outside this range, this is an indication that the assumption has been violated. Clearly, here that is the case.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">augment</span>(slid_mod, slid) %&gt;%
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">.student =</span> <span class="kw">rstudent</span>(slid_mod)) %&gt;%
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(.student)) +
<span class="st">  </span><span class="kw">geom_density</span>(<span class="dt">adjust =</span> .<span class="dv">5</span>) +
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">x =</span> <span class="st">&quot;Studentized residuals&quot;</span>,
       <span class="dt">y =</span> <span class="st">&quot;Estimated density&quot;</span>)</code></pre></div>
<p><img src="persp012_regression_diagnostics_files/figure-html/slid-density-1.png" width="672" /></p>
<p>From the density plot of the studentized residuals, we can also see that the residuals are positively skewed.</p>
</div>
<div id="fixing-non-normally-distributed-errors" class="section level2">
<h2>Fixing non-normally distributed errors</h2>
<p><a href="persp007_nonlinear.html#monotonic_transformations">Power and log transformations</a> are typically used to correct this problem. Here, trial and error reveals that by log transforming the wage variable, the distribution of the residuals becomes much more symmetric:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">slid &lt;-<span class="st"> </span>slid %&gt;%
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">wage_log =</span> <span class="kw">log</span>(compositeHourlyWages))

slid_log_mod &lt;-<span class="st"> </span><span class="kw">lm</span>(wage_log ~<span class="st"> </span>sex +<span class="st"> </span>yearsEducation +<span class="st"> </span>age, <span class="dt">data =</span> slid)
<span class="kw">tidy</span>(slid_log_mod)</code></pre></div>
<pre><code>##             term estimate std.error statistic   p.value
## 1    (Intercept)   1.0990  0.037965      28.9 1.97e-167
## 2        sexMale   0.2245  0.013121      17.1  2.16e-63
## 3 yearsEducation   0.0559  0.002171      25.7 2.95e-135
## 4            age   0.0182  0.000549      33.1 4.50e-212</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">car::<span class="kw">qqPlot</span>(slid_log_mod)</code></pre></div>
<p><img src="persp012_regression_diagnostics_files/figure-html/slid-log-1.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">augment</span>(slid_log_mod, slid) %&gt;%
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">.student =</span> <span class="kw">rstudent</span>(slid_log_mod)) %&gt;%
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(.student)) +
<span class="st">  </span><span class="kw">geom_density</span>(<span class="dt">adjust =</span> .<span class="dv">5</span>) +
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">x =</span> <span class="st">&quot;Studentized residuals&quot;</span>,
       <span class="dt">y =</span> <span class="st">&quot;Estimated density&quot;</span>)</code></pre></div>
<p><img src="persp012_regression_diagnostics_files/figure-html/slid-log-2.png" width="672" /></p>
</div>
</div>
<div id="non-constant-error-variance" class="section level1">
<h1>Non-constant error variance</h1>
</div>
<div id="non-linearity-in-the-data" class="section level1">
<h1>Non-linearity in the data</h1>
<p>By assuming the average error <span class="math inline">\(E(\epsilon)\)</span> is 0 everywhere implies that the regression line (surface) accurately reflects the relationship between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>. Violating this assumption means that the model fails to capture the systematic relationship between the response and explanatory variables. Therefore here, the term <strong>nonlinearity</strong> could mean a couple different things:</p>
<ul>
<li>The relationship between <span class="math inline">\(X_1\)</span> and <span class="math inline">\(Y\)</span> is nonlinear - that is, it is not constant and monotonic</li>
<li>The relationship between <span class="math inline">\(X_1\)</span> and <span class="math inline">\(Y\)</span> is conditional on <span class="math inline">\(X_2\)</span> - that is, the relationship is interactive rather than purely additive</li>
</ul>
<p>Detecting nonlinearity can be tricky in higher-dimensional regression models with multiple explanatory variables.</p>
<div id="partial-residual-plots" class="section level2">
<h2>Partial residual plots</h2>
<p>Define the <strong>partial residual</strong> for the <span class="math inline">\(j\)</span>th explanatory variable:</p>
<p><span class="math display">\[E_i^{(j)} = E_i + B_j X_{ij}\]</span></p>
<p>In essence, calculate the least-squares residual (<span class="math inline">\(E_i\)</span>) and add to it the linear component of the partial relationship between <span class="math inline">\(Y\)</span> and <span class="math inline">\(X_j\)</span>. Finally, we can plot <span class="math inline">\(X_j\)</span> versus <span class="math inline">\(E^{(j)}\)</span> and assess the relationship. For instance, consider the results of the logged wage model from earlier:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># get partial resids</span>
slid_resid &lt;-<span class="st"> </span><span class="kw">residuals</span>(slid_log_mod, <span class="dt">type =</span> <span class="st">&quot;partial&quot;</span>) %&gt;%
<span class="st">  </span>as_tibble
<span class="kw">names</span>(slid_resid) &lt;-<span class="st"> </span><span class="kw">str_c</span>(<span class="kw">names</span>(slid_resid), <span class="st">&quot;_resid&quot;</span>)

slid_diag &lt;-<span class="st"> </span><span class="kw">augment</span>(slid_log_mod, slid) %&gt;%
<span class="st">  </span><span class="kw">bind_cols</span>(slid_resid)

<span class="kw">ggplot</span>(slid_diag, <span class="kw">aes</span>(age, age_resid)) +
<span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">alpha =</span> .<span class="dv">1</span>) +
<span class="st">  </span><span class="kw">geom_smooth</span>(<span class="dt">se =</span> <span class="ot">FALSE</span>) +
<span class="st">  </span><span class="kw">geom_smooth</span>(<span class="dt">method =</span> <span class="st">&quot;lm&quot;</span>, <span class="dt">se =</span> <span class="ot">FALSE</span>, <span class="dt">linetype =</span> <span class="dv">2</span>) +
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">x =</span> <span class="st">&quot;Age&quot;</span>,
       <span class="dt">y =</span> <span class="st">&quot;Partial residual for age&quot;</span>)</code></pre></div>
<p><img src="persp012_regression_diagnostics_files/figure-html/part-resid-plot-1.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>(slid_diag, <span class="kw">aes</span>(yearsEducation, yearsEducation_resid)) +
<span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">alpha =</span> .<span class="dv">1</span>) +
<span class="st">  </span><span class="kw">geom_smooth</span>(<span class="dt">se =</span> <span class="ot">FALSE</span>) +
<span class="st">  </span><span class="kw">geom_smooth</span>(<span class="dt">method =</span> <span class="st">&quot;lm&quot;</span>, <span class="dt">se =</span> <span class="ot">FALSE</span>, <span class="dt">linetype =</span> <span class="dv">2</span>) +
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">x =</span> <span class="st">&quot;Education (years)&quot;</span>,
       <span class="dt">y =</span> <span class="st">&quot;Partial residual for education&quot;</span>)</code></pre></div>
<p><img src="persp012_regression_diagnostics_files/figure-html/part-resid-plot-2.png" width="672" /></p>
<p>The solid lines are GAMs, while the dashed lines are linear least-squares fits. For age, the partial relationship with logged wages is not linear - some transformation of age is necessary to correct this. For education, the relationship is more approximately linear except for the discrepancy for individual with very low education levels.</p>
<p>We can correct this by adding a squared polynomial term for age, and square the education term. The resulting regression model is:</p>
<p><span class="math display">\[\log(\text{Wage}) = \beta_0 + \beta_1(\text{Male}) + \beta_2 \text{Age} + \beta_3 \text{Age}^2 + \beta_4 \text{Education}^2\]</span></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">slid_log_trans &lt;-<span class="st"> </span><span class="kw">lm</span>(wage_log ~<span class="st"> </span>sex +<span class="st"> </span><span class="kw">I</span>(yearsEducation^<span class="dv">2</span>) +<span class="st"> </span>age +<span class="st"> </span><span class="kw">I</span>(age^<span class="dv">2</span>), <span class="dt">data =</span> slid)
<span class="kw">tidy</span>(slid_log_trans)</code></pre></div>
<pre><code>##                  term  estimate std.error statistic   p.value
## 1         (Intercept)  0.396819  5.78e-02      6.87  7.62e-12
## 2             sexMale  0.221458  1.24e-02     17.79  3.21e-68
## 3 I(yearsEducation^2)  0.001805  7.86e-05     22.96 1.19e-109
## 4                 age  0.083018  3.19e-03     26.05 2.93e-138
## 5            I(age^2) -0.000852  4.10e-05    -20.78  3.85e-91</code></pre>
<p>Because the model is now nonlinear in both age and education, we need to rethink how to draw the partial residuals plot. The easiest approach is to plot the partial residuals for both age and education against the original explanatory variable. For age, that is</p>
<p><span class="math display">\[E_i^{\text{Age}} = 0.083 \times \text{Age}_i -0.0008524 \times \text{Age}^2_i + E_i\]</span></p>
<p>and for education,</p>
<p><span class="math display">\[E_i^{\text{Education}} = 0.002 \times \text{Education}^2_i + E_i\]</span></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># get partial resids</span>
slid_trans_resid &lt;-<span class="st"> </span><span class="kw">residuals</span>(slid_log_trans, <span class="dt">type =</span> <span class="st">&quot;partial&quot;</span>) %&gt;%
<span class="st">  </span>as_tibble
<span class="kw">names</span>(slid_trans_resid) &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;sex&quot;</span>, <span class="st">&quot;education&quot;</span>, <span class="st">&quot;age&quot;</span>, <span class="st">&quot;age_sq&quot;</span>)
<span class="kw">names</span>(slid_trans_resid) &lt;-<span class="st"> </span><span class="kw">str_c</span>(<span class="kw">names</span>(slid_trans_resid), <span class="st">&quot;_resid&quot;</span>)

slid_trans_diag &lt;-<span class="st"> </span><span class="kw">augment</span>(slid_log_trans, slid) %&gt;%
<span class="st">  </span>as_tibble %&gt;%
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">age_resid =</span> <span class="kw">coef</span>(slid_log_trans)[[<span class="dv">4</span>]] *<span class="st"> </span>age +
<span class="st">           </span><span class="kw">coef</span>(slid_log_trans)[[<span class="dv">5</span>]] *<span class="st"> </span>age^<span class="dv">2</span> +<span class="st"> </span>.resid,
         <span class="dt">educ_resid =</span> <span class="kw">coef</span>(slid_log_trans)[[<span class="dv">5</span>]] *<span class="st"> </span>yearsEducation^<span class="dv">2</span> +<span class="st"> </span>.resid)

<span class="kw">ggplot</span>(slid_trans_diag, <span class="kw">aes</span>(age, age_resid)) +
<span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">alpha =</span> .<span class="dv">1</span>) +
<span class="st">  </span><span class="kw">geom_smooth</span>(<span class="dt">se =</span> <span class="ot">FALSE</span>) +
<span class="st">  </span><span class="kw">geom_smooth</span>(<span class="dt">method =</span> <span class="st">&quot;lm&quot;</span>, <span class="dt">se =</span> <span class="ot">FALSE</span>, <span class="dt">linetype =</span> <span class="dv">2</span>) +
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">x =</span> <span class="st">&quot;Age&quot;</span>,
       <span class="dt">y =</span> <span class="st">&quot;Partial residual for age&quot;</span>)</code></pre></div>
<p><img src="persp012_regression_diagnostics_files/figure-html/slid-part-trans-plot-1.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>(slid_trans_diag, <span class="kw">aes</span>(yearsEducation, educ_resid)) +
<span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">alpha =</span> .<span class="dv">1</span>) +
<span class="st">  </span><span class="kw">geom_smooth</span>(<span class="dt">se =</span> <span class="ot">FALSE</span>) +
<span class="st">  </span><span class="kw">geom_smooth</span>(<span class="dt">method =</span> <span class="st">&quot;lm&quot;</span>, <span class="dt">se =</span> <span class="ot">FALSE</span>, <span class="dt">linetype =</span> <span class="dv">2</span>) +
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">x =</span> <span class="st">&quot;Education (years)&quot;</span>,
       <span class="dt">y =</span> <span class="st">&quot;Partial residual for education&quot;</span>)</code></pre></div>
<p><img src="persp012_regression_diagnostics_files/figure-html/slid-part-trans-plot-2.png" width="672" /></p>
</div>
</div>
<div id="collinearity" class="section level1">
<h1>Collinearity</h1>
<p><strong>Collinearity</strong> (or <strong>multicollinearity</strong>) is a state of a model where explanatory variables are correlated with one another.</p>
<div id="perfect-collinearity" class="section level2">
<h2>Perfect collinearity</h2>
<p>Perfect collinearity is incredibly rare, and typically involves using transformed versions of a variable in the model <strong>along with the original variable</strong>. For example, let’s estimate a regression model explaining <code>mpg</code> as a function of <code>displ</code>, <code>wt</code>, and <code>cyl</code>:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">mtcars1 &lt;-<span class="st"> </span><span class="kw">lm</span>(mpg ~<span class="st"> </span>disp +<span class="st"> </span>wt +<span class="st"> </span>cyl, <span class="dt">data =</span> mtcars)
<span class="kw">summary</span>(mtcars1)</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = mpg ~ disp + wt + cyl, data = mtcars)
## 
## Residuals:
##    Min     1Q Median     3Q    Max 
## -4.403 -1.403 -0.495  1.339  6.072 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 41.10768    2.84243   14.46  1.6e-14 ***
## disp         0.00747    0.01184    0.63   0.5332    
## wt          -3.63568    1.04014   -3.50   0.0016 ** 
## cyl         -1.78494    0.60711   -2.94   0.0065 ** 
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 2.59 on 28 degrees of freedom
## Multiple R-squared:  0.833,  Adjusted R-squared:  0.815 
## F-statistic: 46.4 on 3 and 28 DF,  p-value: 5.4e-11</code></pre>
<p>Now let’s say we want to recode <code>displ</code> so it is centered around it’s mean and reestimate the model:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">mtcars &lt;-<span class="st"> </span>mtcars %&gt;%
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">disp_mean =</span> disp -<span class="st"> </span><span class="kw">mean</span>(disp))

mtcars2 &lt;-<span class="st"> </span><span class="kw">lm</span>(mpg ~<span class="st"> </span>disp +<span class="st"> </span>wt +<span class="st"> </span>cyl +<span class="st"> </span>disp_mean, <span class="dt">data =</span> mtcars)
<span class="kw">summary</span>(mtcars2)</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = mpg ~ disp + wt + cyl + disp_mean, data = mtcars)
## 
## Residuals:
##    Min     1Q Median     3Q    Max 
## -4.403 -1.403 -0.495  1.339  6.072 
## 
## Coefficients: (1 not defined because of singularities)
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 41.10768    2.84243   14.46  1.6e-14 ***
## disp         0.00747    0.01184    0.63   0.5332    
## wt          -3.63568    1.04014   -3.50   0.0016 ** 
## cyl         -1.78494    0.60711   -2.94   0.0065 ** 
## disp_mean         NA         NA      NA       NA    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 2.59 on 28 degrees of freedom
## Multiple R-squared:  0.833,  Adjusted R-squared:  0.815 
## F-statistic: 46.4 on 3 and 28 DF,  p-value: 5.4e-11</code></pre>
<p>Ooops. What’s the problem? <code>disp</code> and <code>disp_mean</code> are perfectly correlated with each other:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>(mtcars, <span class="kw">aes</span>(disp, disp_mean)) +
<span class="st">  </span><span class="kw">geom_point</span>()</code></pre></div>
<p><img src="persp012_regression_diagnostics_files/figure-html/mtcars-cor-1.png" width="672" /></p>
<p>Because they perfectly explain each other, we cannot estimate a linear regression model that contains both variables.<a href="#fn4" class="footnoteRef" id="fnref4"><sup>4</sup></a> Fortunately R automatically drops the second variable so it can estimate the model. Because of this, perfect multicollinearity is rarely problematic in social science.</p>
</div>
<div id="less-than-perfect-collinearity" class="section level2">
<h2>Less-than-perfect collinearity</h2>
<p>Instead consider the credit dataset:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">credit &lt;-<span class="st"> </span><span class="kw">read_csv</span>(<span class="st">&quot;data/Credit.csv&quot;</span>) %&gt;%
<span class="st">  </span><span class="kw">select</span>(-X1)
<span class="kw">names</span>(credit) &lt;-<span class="st"> </span><span class="kw">tolower</span>(<span class="kw">names</span>(credit))

<span class="kw">ggplot</span>(credit, <span class="kw">aes</span>(limit, age)) +
<span class="st">  </span><span class="kw">geom_point</span>()</code></pre></div>
<p><img src="persp012_regression_diagnostics_files/figure-html/credit-1.png" width="672" /></p>
<p>Age and limit are not strongly correlated with one another, so estimating a linear regression model to predict an individual’s balance as a function of age and limit is not a problem:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">age_limit &lt;-<span class="st"> </span><span class="kw">lm</span>(balance ~<span class="st"> </span>age +<span class="st"> </span>limit, <span class="dt">data =</span> credit)
<span class="kw">tidy</span>(age_limit)</code></pre></div>
<pre><code>##          term estimate std.error statistic   p.value
## 1 (Intercept) -173.411  43.82839     -3.96  9.01e-05
## 2         age   -2.291   0.67248     -3.41  7.23e-04
## 3       limit    0.173   0.00503     34.50 1.63e-121</code></pre>
<p>But what about using an individuals credit card rating instead of age? It is likely a good predictor of balance as well:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>(credit, <span class="kw">aes</span>(rating, balance)) +
<span class="st">  </span><span class="kw">geom_point</span>() +
<span class="st">  </span><span class="kw">geom_smooth</span>()</code></pre></div>
<p><img src="persp012_regression_diagnostics_files/figure-html/add-rating-1.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">limit_rate &lt;-<span class="st"> </span><span class="kw">lm</span>(balance ~<span class="st"> </span>limit +<span class="st"> </span>rating, <span class="dt">data =</span> credit)
<span class="kw">tidy</span>(limit_rate)</code></pre></div>
<pre><code>##          term  estimate std.error statistic  p.value
## 1 (Intercept) -377.5368   45.2542    -8.343 1.21e-15
## 2       limit    0.0245    0.0638     0.384 7.01e-01
## 3      rating    2.2017    0.9523     2.312 2.13e-02</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">coefplot::<span class="kw">multiplot</span>(age_limit, limit_rate)</code></pre></div>
<p><img src="persp012_regression_diagnostics_files/figure-html/add-rating-2.png" width="672" /></p>
<p>By replacing age with rating, we developed a problem in our model. The problem is that limit and rating are strongly correlated with one another:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>(credit, <span class="kw">aes</span>(limit, rating)) +
<span class="st">  </span><span class="kw">geom_point</span>() +
<span class="st">  </span><span class="kw">geom_smooth</span>()</code></pre></div>
<p><img src="persp012_regression_diagnostics_files/figure-html/limit-rate-1.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">coefplot::<span class="kw">multiplot</span>(age_limit, limit_rate, <span class="dt">predictors =</span> <span class="st">&quot;limit&quot;</span>)</code></pre></div>
<p><img src="persp012_regression_diagnostics_files/figure-html/limit-rate-2.png" width="672" /></p>
<p>In the regression model, it is difficult to parse out the independent effects of limit and rating on balance, because limit and rating tend to increase and decrease in association with one another. Because the accuracy of our estimates of the parameters is reduced, the standard errors increase. This is why you can see above that the standard error for limit is much larger in the second model compared to the first model.</p>
<div id="detecting-collinearity" class="section level3">
<h3>Detecting collinearity</h3>
<div id="scatterplot-matrix" class="section level4">
<h4>Scatterplot matrix</h4>
<p>A correlation or scatterplot matrix would help to reveal any strongly correlated variables:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">cormat_heatmap &lt;-<span class="st"> </span>function(data){
  <span class="co"># generate correlation matrix</span>
  cormat &lt;-<span class="st"> </span><span class="kw">round</span>(<span class="kw">cor</span>(data), <span class="dv">2</span>)
  
  <span class="co"># melt into a tidy table</span>
  get_upper_tri &lt;-<span class="st"> </span>function(cormat){
    cormat[<span class="kw">lower.tri</span>(cormat)]&lt;-<span class="st"> </span><span class="ot">NA</span>
    <span class="kw">return</span>(cormat)
  }
  
  upper_tri &lt;-<span class="st"> </span><span class="kw">get_upper_tri</span>(cormat)
  
  <span class="co"># reorder matrix based on coefficient value</span>
  reorder_cormat &lt;-<span class="st"> </span>function(cormat){
    <span class="co"># Use correlation between variables as distance</span>
    dd &lt;-<span class="st"> </span><span class="kw">as.dist</span>((<span class="dv">1</span>-cormat)/<span class="dv">2</span>)
    hc &lt;-<span class="st"> </span><span class="kw">hclust</span>(dd)
    cormat &lt;-cormat[hc$order, hc$order]
  }
  
  cormat &lt;-<span class="st"> </span><span class="kw">reorder_cormat</span>(cormat)
  upper_tri &lt;-<span class="st"> </span><span class="kw">get_upper_tri</span>(cormat)
  
  <span class="co"># Melt the correlation matrix</span>
  melted_cormat &lt;-<span class="st"> </span>reshape2::<span class="kw">melt</span>(upper_tri, <span class="dt">na.rm =</span> <span class="ot">TRUE</span>)
  
  <span class="co"># Create a ggheatmap</span>
  ggheatmap &lt;-<span class="st"> </span><span class="kw">ggplot</span>(melted_cormat, <span class="kw">aes</span>(Var2, Var1, <span class="dt">fill =</span> value))+
<span class="st">    </span><span class="kw">geom_tile</span>(<span class="dt">color =</span> <span class="st">&quot;white&quot;</span>)+
<span class="st">    </span><span class="kw">scale_fill_gradient2</span>(<span class="dt">low =</span> <span class="st">&quot;blue&quot;</span>, <span class="dt">high =</span> <span class="st">&quot;red&quot;</span>, <span class="dt">mid =</span> <span class="st">&quot;white&quot;</span>, 
                         <span class="dt">midpoint =</span> <span class="dv">0</span>, <span class="dt">limit =</span> <span class="kw">c</span>(-<span class="dv">1</span>,<span class="dv">1</span>), <span class="dt">space =</span> <span class="st">&quot;Lab&quot;</span>, 
                         <span class="dt">name=</span><span class="st">&quot;Pearson</span><span class="ch">\n</span><span class="st">Correlation&quot;</span>) +
<span class="st">    </span><span class="kw">theme_minimal</span>()+<span class="st"> </span><span class="co"># minimal theme</span>
<span class="st">    </span><span class="kw">theme</span>(<span class="dt">axis.text.x =</span> <span class="kw">element_text</span>(<span class="dt">angle =</span> <span class="dv">45</span>, <span class="dt">vjust =</span> <span class="dv">1</span>, 
                                     <span class="dt">size =</span> <span class="dv">12</span>, <span class="dt">hjust =</span> <span class="dv">1</span>))+
<span class="st">    </span><span class="kw">coord_fixed</span>()
  
  <span class="co"># add correlation values to graph</span>
  ggheatmap +<span class="st"> </span>
<span class="st">    </span><span class="kw">geom_text</span>(<span class="kw">aes</span>(Var2, Var1, <span class="dt">label =</span> value), <span class="dt">color =</span> <span class="st">&quot;black&quot;</span>, <span class="dt">size =</span> <span class="dv">4</span>) +
<span class="st">    </span><span class="kw">theme</span>(
      <span class="dt">axis.title.x =</span> <span class="kw">element_blank</span>(),
      <span class="dt">axis.title.y =</span> <span class="kw">element_blank</span>(),
      <span class="dt">panel.grid.major =</span> <span class="kw">element_blank</span>(),
      <span class="dt">panel.border =</span> <span class="kw">element_blank</span>(),
      <span class="dt">panel.background =</span> <span class="kw">element_blank</span>(),
      <span class="dt">axis.ticks =</span> <span class="kw">element_blank</span>(),
      <span class="dt">legend.position =</span> <span class="st">&quot;bottom&quot;</span>)
}

<span class="kw">cormat_heatmap</span>(<span class="kw">select_if</span>(credit, is.numeric))</code></pre></div>
<p><img src="persp012_regression_diagnostics_files/figure-html/credit-cor-mat-1.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(GGally)
<span class="kw">ggpairs</span>(<span class="kw">select_if</span>(credit, is.numeric))</code></pre></div>
<p><img src="persp012_regression_diagnostics_files/figure-html/credit-scatter-mat-1.png" width="672" /></p>
<p>Here it is very clear that limit and rating are strongly correlated with one another.</p>
</div>
<div id="variance-inflation-factor-vif" class="section level4">
<h4>Variance inflation factor (VIF)</h4>
<p>Unfortunately correlation matricies may not be sufficient to detect collinearity if the correlation exists between three or more variables (aka <strong>multicollinearity</strong>) <em>while not existing between any two pairs of these variables</em>. Instead, we can calculate the variance inflation factor (VIF) which is the ratio of the variance of <span class="math inline">\(\hat{\beta}_j\)</span> when fitting the full model divided by the variance of <span class="math inline">\(\hat{\beta}_j\)</span> if fit on its own model. We can use the <code>car::vif()</code> function in R to calculate this statistic for each coefficient. A good rule of thumb is that a VIF statistic greater than 10 indicates potential multicollinearity in the model. Applied to the <code>credit</code> regression models above:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">vif</span>(age_limit)</code></pre></div>
<pre><code>##   age limit 
##  1.01  1.01</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">vif</span>(limit_rate)</code></pre></div>
<pre><code>##  limit rating 
##    160    160</code></pre>
</div>
</div>
</div>
<div id="fixing-multicollinearity" class="section level2">
<h2>Fixing multicollinearity</h2>
<div id="what-not-to-do" class="section level3">
<h3>What not to do</h3>
<blockquote>
<p>Drop one or more of the collinear variables from the model</p>
</blockquote>
<p>This is not a good idea, even if it makes your results “significant”. By omitting the variable, you are completely respecifying your model <strong>in direct contradiction to your theory</strong>. If your theory suggests that a variable can be dropped, go ahead. But if not, then don’t do it.</p>
</div>
<div id="what-you-could-do-instead" class="section level3">
<h3>What you could do instead</h3>
<div id="add-data" class="section level4">
<h4>Add data</h4>
<p>The more observations, the better. It could at least decrease your standard errors and give you more precise estimates. And if you add “odd” or unusual observations, it could also reduce the degree of multicollinearity.</p>
</div>
<div id="transform-the-covariates" class="section level4">
<h4>Transform the covariates</h4>
<p>If the variables are indicators of the same underlying concept, you can combine them into an index variable. This could be an <strong>additive index</strong> where you sum up comparable covariates or binary indicators. Alternatively, you could create an index via <a href="persp011_unsupervised.html#principal_components_analysis"><strong>principal components analysis</strong></a>.</p>
</div>
<div id="shrinkage-methods" class="section level4">
<h4>Shrinkage methods</h4>
<p>See chapter 6 in ISLR.</p>
</div>
</div>
</div>
</div>
<div id="applying-to-glms" class="section level1">
<h1>Applying to GLMs</h1>
</div>
<div id="session-info" class="section level1 toc-ignore">
<h1>Session Info</h1>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">devtools::<span class="kw">session_info</span>()</code></pre></div>
<pre><code>##  setting  value                       
##  version  R version 3.3.3 (2017-03-06)
##  system   x86_64, darwin13.4.0        
##  ui       X11                         
##  language (EN)                        
##  collate  en_US.UTF-8                 
##  tz       America/Chicago             
##  date     2017-05-02                  
## 
##  package      * version    date       source                           
##  assertthat     0.2.0      2017-04-11 cran (@0.2.0)                    
##  backports      1.0.5      2017-01-18 CRAN (R 3.3.2)                   
##  broom        * 0.4.2      2017-02-13 CRAN (R 3.3.2)                   
##  car          * 2.1-4      2016-12-02 CRAN (R 3.3.2)                   
##  codetools      0.2-15     2016-10-05 CRAN (R 3.3.3)                   
##  colorspace     1.3-2      2016-12-14 CRAN (R 3.3.2)                   
##  DBI            0.6        2017-03-09 CRAN (R 3.3.3)                   
##  devtools       1.12.0     2016-06-24 CRAN (R 3.3.0)                   
##  digest         0.6.12     2017-01-27 CRAN (R 3.3.2)                   
##  dplyr        * 0.5.0      2016-06-24 CRAN (R 3.3.0)                   
##  evaluate       0.10       2016-10-11 CRAN (R 3.3.0)                   
##  forcats      * 0.2.0      2017-01-23 CRAN (R 3.3.2)                   
##  foreign        0.8-67     2016-09-13 CRAN (R 3.3.3)                   
##  GGally       * 1.3.0      2016-11-13 CRAN (R 3.3.2)                   
##  ggplot2      * 2.2.1.9000 2017-05-01 Github (hadley/ggplot2@f4398b6)  
##  gtable         0.2.0      2016-02-26 CRAN (R 3.3.0)                   
##  haven        * 1.0.0      2016-09-23 cran (@1.0.0)                    
##  hms            0.3        2016-11-22 CRAN (R 3.3.2)                   
##  htmltools      0.3.6      2017-04-28 cran (@0.3.6)                    
##  httr           1.2.1      2016-07-03 CRAN (R 3.3.0)                   
##  ISLR         * 1.0        2013-06-11 CRAN (R 3.3.0)                   
##  jsonlite       1.4        2017-04-08 cran (@1.4)                      
##  knitr          1.15.1     2016-11-22 cran (@1.15.1)                   
##  lattice        0.20-34    2016-09-06 CRAN (R 3.3.3)                   
##  lazyeval       0.2.0      2016-06-12 CRAN (R 3.3.0)                   
##  lme4           1.1-12     2016-04-16 cran (@1.1-12)                   
##  lubridate      1.6.0      2016-09-13 CRAN (R 3.3.0)                   
##  magrittr       1.5        2014-11-22 CRAN (R 3.3.0)                   
##  MASS           7.3-45     2016-04-21 CRAN (R 3.3.0)                   
##  Matrix         1.2-8      2017-01-20 CRAN (R 3.3.3)                   
##  MatrixModels   0.4-1      2015-08-22 CRAN (R 3.3.0)                   
##  memoise        1.0.0      2016-01-29 CRAN (R 3.3.0)                   
##  mgcv           1.8-17     2017-02-08 CRAN (R 3.3.3)                   
##  minqa          1.2.4      2014-10-09 cran (@1.2.4)                    
##  mnormt         1.5-5      2016-10-15 CRAN (R 3.3.0)                   
##  modelr       * 0.1.0      2016-08-31 CRAN (R 3.3.0)                   
##  munsell        0.4.3      2016-02-13 CRAN (R 3.3.0)                   
##  nlme           3.1-131    2017-02-06 CRAN (R 3.3.3)                   
##  nloptr         1.0.4      2014-08-04 cran (@1.0.4)                    
##  nnet           7.3-12     2016-02-02 CRAN (R 3.3.3)                   
##  pbkrtest       0.4-6      2016-01-27 CRAN (R 3.3.0)                   
##  plyr           1.8.4      2016-06-08 CRAN (R 3.3.0)                   
##  psych          1.7.3.21   2017-03-22 CRAN (R 3.3.2)                   
##  purrr        * 0.2.2      2016-06-18 CRAN (R 3.3.0)                   
##  quantreg       5.29       2016-09-04 CRAN (R 3.3.0)                   
##  R6             2.2.0      2016-10-05 CRAN (R 3.3.0)                   
##  rcfss        * 0.1.4      2017-02-28 local                            
##  RColorBrewer   1.1-2      2014-12-07 CRAN (R 3.3.0)                   
##  Rcpp           0.12.10    2017-03-19 cran (@0.12.10)                  
##  readr        * 1.1.0      2017-03-22 cran (@1.1.0)                    
##  readxl         0.1.1      2016-03-28 CRAN (R 3.3.0)                   
##  reshape        0.8.6      2016-10-21 CRAN (R 3.3.0)                   
##  reshape2       1.4.2      2016-10-22 CRAN (R 3.3.0)                   
##  rlang          0.0.0.9018 2017-05-01 Github (hadley/rlang@460323e)    
##  rmarkdown      1.3        2016-12-21 CRAN (R 3.3.2)                   
##  rprojroot      1.2        2017-01-16 CRAN (R 3.3.2)                   
##  rvest          0.3.2      2016-06-17 CRAN (R 3.3.0)                   
##  scales         0.4.1      2016-11-09 CRAN (R 3.3.1)                   
##  SparseM        1.74       2016-11-10 CRAN (R 3.3.2)                   
##  stringi        1.1.2      2016-10-01 CRAN (R 3.3.0)                   
##  stringr      * 1.2.0      2017-02-18 CRAN (R 3.3.2)                   
##  tibble       * 1.3.0.9001 2017-05-01 Github (tidyverse/tibble@08af6b0)
##  tidyr        * 0.6.1      2017-01-10 CRAN (R 3.3.2)                   
##  tidyverse    * 1.1.1      2017-01-27 CRAN (R 3.3.2)                   
##  titanic      * 0.1.0      2015-08-31 CRAN (R 3.3.0)                   
##  withr          1.0.2      2016-06-20 CRAN (R 3.3.0)                   
##  xml2           1.1.1      2017-01-24 CRAN (R 3.3.2)                   
##  yaml           2.1.14     2016-11-12 cran (@2.1.14)</code></pre>
</div>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>The average value of <span class="math inline">\(\epsilon\)</span> given <span class="math inline">\(XS\)</span><a href="#fnref1">↩</a></p></li>
<li id="fn2"><p>74th (1935-36), 98th (1983-84), and 104th (1995-96).<a href="#fnref2">↩</a></p></li>
<li id="fn3"><p>Assuming the sample size is sufficiently large.<a href="#fnref3">↩</a></p></li>
<li id="fn4"><p>Basically we cannot invert the variance-covariance matrix of <span class="math inline">\(\mathbf{X}\)</span> because the collinear columns in <span class="math inline">\(\mathbf{X}\)</span> are perfectly linearly dependent on each other. Because of this, we cannot get parameter estimates or standard errors for the model.<a href="#fnref4">↩</a></p></li>
</ol>
</div>

<p>This work is licensed under the  <a href="http://creativecommons.org/licenses/by-nc/4.0/">CC BY-NC 4.0 Creative Commons License</a>.</p>


</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
