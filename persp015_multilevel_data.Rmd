---
title: "Multilevel data"
author: "MACS 30200 - Perspectives on Computational Research"
output:
  html_document:
    toc: true
    toc_float: true
    code_folding: hide
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(cache = TRUE,
                      message = FALSE,
                      warning = FALSE)
```

# Objectives

* Define missing data and patterns of missingness
* Identify traditional approaches to missing data
* Define imputation and multiple imputation
* Summarize maximum-likelihood estimation for MAR data
* Define Bayesian multiple imputation
* Demonstrate how to conduct inference on MI datasets

```{r packages, cache = FALSE, message = FALSE, warning = FALSE}
library(tidyverse)
library(broom)
library(forcats)
library(modelr)
library(stringr)
library(car)
library(rcfss)
library(RColorBrewer)

options(digits = 3)
set.seed(1234)
theme_set(theme_minimal())
```

# What Influences Partisan Defection in Voters?

What motivates partisan individuals to cross party lines and vote for a presidential candidate from the opposition? In an era of heightened partisan polarization Democrats and Republicans are increasingly reluctant to cast ballots for their partisan opponents, making these individuals even stronger of outliers. While only a relatively small number of self-identified partisan individuals cast a defecting vote in the United States, the outcome of closely contested elections may be decided by these actions. Understanding individual motivations to cast aside partisanship improves our understanding of how deeply PID influences individual political behavior.

Based upon previous studies of partisan defection, I formed several hypotheses for how these factors influence vote selection in partisan individuals:

* **Partisan intensity** - individuals who more strongly identify with a political party have a lower probability of defecting. If partisan identification is related to vote choice and individuals can possess different intensities of PID, stronger partisans should have more attachment to their party's candidate and be less likely to defect compared to weak or leaning partisans. This effect should be present for individuals affiliated with either major party and can apply in any election. Since partisanship is strongly associated with vote choice and is theorized to be a core psychological predisposition, PID intensity could moderate the effects of other causes of partisan defection.
* **Relative favorability** - as the perceived favorability of an opposing party's candidate increases relative to the own party's candidate, the probability of a voter's partisan defection increases. Under certain circumstances short-term influences have been shown to significantly impact an individual's vote choice. Intangible traits such as likability or relative warmth might conceivably influence individual vote decisions. Cited as the "with whom would you like to have a beer?" test, political commentators and scholars have explored this relationship with mixed results. If the opposing party's candidate seems more appealing, regardless of policy views, individuals may be persuaded to defect for the more charismatic candidate. This might potentially explain specific episodes of defection such as the Reagan Democrats in 1984 and Republicans for Obama in 2008 where one candidate was perceived as more charismatic than the other.
* **Correct is defect** - if an individual's correct vote decision requires him to defect, then the probability of defection increases. Correct voting decision factors include policy preferences, evaluation's of candidates' traits, and social group identification. As such, correct voting should be, and in fact is, closely associated with actual voting decisions. If an individual's correct vote decision requires the voter to defect, that individual should have a higher probability of actually defecting relative to voters whose correct vote decisions match their partisan preference. This influence should be relevant to all voters in any election.
* **Incumbency** - when the incumbent is a candidate for election, voters who identify with the opposition party have a higher probability of defecting. When an election occurs and the incumbent does not participate, incumbency advantage will have no effect (the candidate from the incumbent's party is not expected to receive any advantage). When the incumbent does participate, the probability of defection should only increase for voters who identify with the opposition's party. Incumbents are unlikely to consistently cause voters from their own party to defect, so under this hypothesis the probability of defection differs across parties. There should be no effect on voters affiliated with the incumbent's party (e.g. Republicans and Reagan in 1984), but the effect should be positive for opposition party members (Democrats in '84).


## Traditional GLM: Issues Related to Pooling

**Pooling** is nothing more than combining data, either across units or time. In this instance, I am pooling [American National Election Studies (ANES)](http://www.electionstudies.org/) observations over time. The key to pooling is exchangability: the notion that, conditional on the values of the covariates, any two observations within the data are considered to be the same (exchangable).

### Why Pool?

Several reasons:

* **Pooling adds data** - this is the number one reason for pooling data. If the assumption of poolability holds, we can get "better" (read: more precise) estimates of our $\hat{\beta}$s.
* **Generalizability** - again, if a case can be made for model-conditional poolability, adding different cases means we can be more sure that our data generalized to broader sets of cases and/or longer time periods.
* **Readability** - if I estimate separate models for each election year, I have to report results for 10 different models.

In this study, I want to understand how partisan defection has occurred over time, not in a single election. Instead of having 752 observations from 2004, I can utilize over 10,000 data points from 10 different elections. I'd rather estimate and report a single model than have to report results from 10 different models. Pooling the data will also produce more generalizable results and only require interpretation of a single model.

### Issues with Pooling

Implicit in this analysis, then, is the idea that the coefficients $\beta$ do not vary over subsets of the data defined along $\E \in \{1972 ,\dotsc, 2008 \}$. In (say) the general, restrictive model:

$$
\begin{align}
 Pr(Y_{ei} = 1) &= \text{logit}^{-1}[\alpha + \beta_{1}\text{PID Intensity}_{ei} + \beta_{2}{\text{Relative Favorability}_{ei}} \nonumber \\
 &\: + \beta_{3}\text{Defect is Correct}_{ei}  + \beta_{4}\text{Incumbent Candidate}_{ei}] \nonumber \\
  \text{logit}^{-1}[x] &= \frac{e^x}{1+e^x} \label{eq:restrict}
\end{align}
$$

the implicit assumption is one of **exchangability** -- i.e., that all of the data come from the same ``regime,'' that is,

* that the process governing the relationship between $X$ and $Y$ is exactly the same for each $e$,
* that the process governing the relationship between $X$ and $Y$ is the same for all $i$,
* that the process governing the $u$s is the same $\forall \: e$ and $i$ as well.

If any of these assumptions are not true, the pooled estimator $\hat{\beta}_P$ will be biased towards the regime with:

* the larger $N$,
* the larger values of the coefficients, and/or
* the smaller standard errors of $\bhat$

### The Error Term

Note as well that, throughout all this discussion, we've been assuming that the error term $u_{it}$ is homoscedastic and uncorrelated, both within and across $i$ and $t$. Formally, that means we need to have:

$$u_{ij} \sim i.i.d.N(0, \sigma^2), \: \forall\: i,j$$

This is exactly what we've done in the previous model specifications with $\sigma^2_y$. If you stop and think about it, that's a pretty tall order. In particular, it requires that:

$$
\begin{align}
Var(u_{ab}) &= Var(u_{cb}) \: \forall \: a \neq b  \text{ (i.e., no cross-unit heteroscedasticity)},  \\
Var(u_{ab}) &= Var(u_{ac}) \: \forall \: b \neq c \text{ (i.e., no temporal heteroscedasticity)},  \\
Cov(u_{ab},u_{cd}) &= 0 \: \forall \: a \neq c,\: \forall \: b \neq d \text{ (i.e., no auto- or spatial correlation)}
\end{align}
$$

Remember: Residuals are (among other things) just an indicator of how good a job the model does of explaining $Y$ with $\X$. In that light, these assumptions are violated if (for example):

* Cross-unit differences mean that the model does a better job of explaining some units than others,
* Time effects (such as socialization, institutionalization, learning, or other such dynamics) cause the model to do a better or worse job of explaining $Y$ over time,
* Omitted variables lead to residual correlation, either across units or (more commonly) over time.

While in a linear model, at least, problems with the error term don't bias coefficient estimates, they can screw up one's inferences pretty badly. And in nonlinear models (i.e. logits and all other GLMs) they can also lead to biases in the point estimates as well.

Additionally, pooling can lead to biased standard errors. This is due to the assumption of GLMs that the errors for each observation are independent of one another. Formally, $E(u_i*u_j)=0 \: \forall \: i,j \: \text{combinations}$. If this assumption is violated and errors in some observations are related to the errors in other observations (likely within election year), the model appears to have more power than it otherwise should. This will create artificially deflated standard errors and make point estimates appear more precise.

### Fake data

$$
\begin{align}
y_{i} &\sim N(\alpha_{j[i]} + \beta x_{i},\sigma^2_y), &\text{for } i=1,\dotsc,n \nonumber \\
\alpha_j &\sim N(\mu_{\alpha},\sigma^2_{\alpha}), &\text{for } j=1,\dotsc,J
\end{align}
$$

```{r sim-mlm}
obs <- 60
group.obs <- 6
groups <- rep(1:group.obs, times = obs / group.obs)

#varying intercept
beta <- 2
alpha <- rnorm(group.obs,2,2)

vary_int <- data_frame(x = runif(obs,0,3),
                       y = alpha + beta * x,
                       groups = factor(groups))

p <- ggplot(vary_int, aes(x, y)) +
  geom_point(aes(color = groups, shape = groups)) +
  labs(title = "Simulated data with varying intercepts",
       x = expression(X),
       y = expression(Y)) +
  theme(legend.position = "none")
p

p +
  geom_smooth(method = "lm") +
  labs(subtitle = "OLS")
```

## Partisan Voting: Pooling Issues

Am I likely to violate any of these assumptions using this pooled dataset? Yes. The mean probability of partisan defection differs significantly across elections.

```{r anes-data}
library(haven)

# read in data
anes <- read_dta("data/anes_pres.dta")

# generate variables
anes <- anes %>%
  #generate binary party measures
  mutate(rep = ifelse(pid < 4, 0,
                      ifelse(pid > 4, 1, NA))) %>%
  #generate measure of defection and whether or not respondent actually voted correctly
  mutate(defect = NA,
    defect = replace(defect, which((pres_vote_r == 0) &
                                     rep == 0), 0),
    defect = replace(defect, which((pres_vote_r == 1) &
                                     rep == 1), 0),
    defect = replace(defect, which((pres_vote_r == 1 | pres_vote_r == 6) &
                                     rep == 0), 1),
    defect = replace(defect, which((pres_vote_r == 0 | pres_vote_r == 6) &
                                     rep == 1), 1),
    vote_cor_actual = ifelse(pres_vote_r == vote_cor, 1, 0)) %>%
  # does the correct vote require one to defect?
  mutate(defect_cor = ifelse(vote_cor != rep, 1, 0)) %>%
  # generate index of PID intensity
  mutate(pid_abs = abs(pid - 4) - 1) %>%
  # remove 1948
  filter(year != 1948) %>%
  # does feeling thermometer towards candidates explain defection?
  mutate(feel_def = ifelse(rep == 1, (dem_feel - rep_feel) / 2,
                           ifelse(rep == 0, (rep_feel - dem_feel) / 2, NA))) %>%
  #does incumbency influence partisan defection?
  mutate(inc = ifelse(year %in% c(1956, 1964, 1972, 1976,
                                  1980, 1984, 1992, 1996, 2004), 1, 0),
         inc_opp = ifelse((rep == 1 & year %in% c(1964, 1980, 1996)) |
                            (rep == 0 & year %in% c(1956, 1972, 1976,
                                                    1984, 1992, 2004)), 1, 0))
```

```{r prop-defect}
#generate a plot of the percentage of partisan defectors in each election
##get weighted proportions for all, dems only, and reps only
anes %>%
  group_by(year) %>%
  summarize(defect = weighted.mean(defect, std_wt, na.rm = TRUE)) %>%
  ggplot(aes(year, defect)) +
  geom_col() +
  scale_y_continuous(labels = scales::percent) +
  labs(title = "Percentage of partisan defectors",
       x = "Presidential election year",
       y = "Percentage of partisan defectors")
```

Unless the composition of the electorate is changing significantly from year-to-year in a specific way, this assumption will not hold. Regardless, this assumption should be tested and explored, which is not possible under a standard GLM approach.

\subs{Multilevel Data Structures \& Problems With Classical Regression}
\nin Consider the classical linear model
\begin{equation}
y_{i} \sim N(\alpha + \beta x_{i}, \sigma^2_{y})
\end{equation}
\nin Implicitly, this model assumes (along with classical OLS assumptions)
\begin{itemize}
  \item that the constant term is constant across different $i$s, and
  \item that the effect of any given variable $X$ on $Y$ is constant across observations (at least, to the extent that non-constancy isn't specified in the model, e.g. through interaction terms).
\end{itemize}

\nin We can write a similar model in the panel context as follows:
\begin{equation}\label{eq:std_mod}
y_{ij} \sim N(\alpha + \beta x_{ij}, \sigma^2_{y}), \: for\: i=1,\dotsc,n
\end{equation}

\nin Note that this model assumes the same things as the earlier ones, especially about the effects of constants and covariates.

\nin In \emph{any} regression context, the two assumptions mentioned are critical; violating them leads to a form of specification bias. In the panel context, these two assumptions are often going to be problematic. This is because, since we're observing multiple units over time or across groups, there's often (in fact, usually) some reason to believe that there may be differences in either $\alpha$ or $\beta$ over either $i$ or $j$. How could we correct for these possibilities?


\subss{Variable Intercepts}
\nin One possible violation of the above assumptions is that the intercepts vary. The most common way this occurs is for different units to have varying intercepts:
\begin{equation}\label{eq:vary_int}
y_{ij} \sim N(\alpha_i + \beta x_{ij}, \sigma^2_{y}), \: for\: i=1,\dotsc,n
\end{equation}
\nin The slopes for each unit are the same, but the intercepts are different. Its also possible that the intercepts vary over time, rather than over units:
\begin{equation}
y_{ij} \sim N(\alpha_t + \beta x_{ij}, \sigma^2_{y}), \: for\: i=1,\dotsc,n
\end{equation}
\nin If we have data that corresponds to \eqref{eq:vary_int}, but estimate a model like \eqref{eq:std_mod}, we can get biased coefficients.


\subss{Variable Slopes}
\nin The other obvious possibility is that we have a constant intercept, but the effects of $X$ on $Y$ differs across either units or (less likely) time; e.g.:
\begin{equation}\label{eq:vary_slope}
y_{it} \sim N(\alpha + \beta_{i} x_{ij}, \sigma^2_{y}), \: for\: i=1,\dotsc,n
\end{equation}
\nin We could also have variation in $\beta$ over time, or even over both units and time.

\nin A model like in \eqref{eq:vary_slope} assumes that the regression lines all pass through the same point on the Y-axis, but that their slopes differ. The idea of a common intercept, however, is a bit strange (at least to social scientists). Instead what is more likely...

\subss{Variable Slopes and Intercepts}
\nin This is when things really start to get difficult. We might, for example, have variable slopes and intercepts for each unit $i$:
\begin{equation}\label{eq:vary_intslope}
y_{ij} \sim N(\alpha_{i} + \beta_{i} x_{ij}, \sigma^2_{y}), \: for\: i=1,\dotsc,n
\end{equation}
\nin Moreover, we could instead have different $\alpha$s and $\beta$s for every time point, rather than for every unit:
\begin{equation}\label{eq:vary_intslope}
y_{ij} \sim N(\alpha_{t} + \beta_{j} x_{ij}, \sigma^2_{y}), \: for\: i=1,\dotsc,n
\end{equation}
or for both different units and time points:
\begin{equation}\label{eq:vary_intslope}
y_{ij} \sim N(\alpha_{it} + \beta_{it} x_{ij}, \sigma^2_{y}), \: for\: i=1,\dotsc,n
\end{equation}
\nin Using the incorrect model specification for the data can lead to odd, even nonsensical, results: underestimating some slopes, overestimating others, and in some cases even getting the sign wrong.

\nin All of this leads to...

\subs{Multilevel Modeling}
\nin What many social scientists refer to as ``random effects'', \citet{gelman2007} simply call ``multilevel modeling''. \emph{Fixed effects} are usually defined as varying coefficients that are not themselves modeled. For example, a classical regression including $J-1$ unit indicators as regression predictors is sometimes called a ``fixed-effects model''. \citeauthor{gelman2007} recommend always using ``random effects'' and focusing on the description of the model itself (for example, varying intercepts and constant slopes), with the understanding that batches of coefficients (for example, $\alpha_1,\dotsc,\alpha_J$) will themselves be modeled.

\nin Multilevel regression can be thought of as a method for compromising between the two extremes of excluding a categorical predictor from a model (\emph{complete pooling}), or estimating separate models within each level of the categorical predictor (\emph{no pooling}). The former ignores within-unit variation, while the latter ignores between-unit variation. Additionally, the no pooling approach prevents the researcher from conducting inference using categorical predictors (this is the ``fixed effects'' approach and any categorical covariates would need to be excluded due to perfect collinearity with the indicator variable or being a constant within the separate model).

\nin Instead, multilevel modeling uses a compromise between these two approaches. A ``soft constraint'' is applied to the coefficients: they are assigned a probability distribution. A varying intercept model could take the following form:
\begin{align}
y_i &\sim N(\alpha_{j[i]} + \beta x_{i}, \sigma^2_{y}), \: \text{for} \: i=1,\dotsc,n, \nonumber \\
\alpha_{j} &\sim N(\mu_{\alpha},\sigma^2_{\alpha}), \: \text{for} \: j=1,\dotsc,J,
\end{align}
\nin $\alpha_{j}$ is now assumed to be distributed normally with a mean $\mu_{\alpha}$ and variance $\sigma^2_{\alpha}$ estimated from the data. This pools the estimates of $\alpha_{j}$ toward the mean level $\mu_{\alpha}$, but not all the way--thus, group-level estimates are the result of a \emph{partial-pooling} compromise. In the limit of $\sigma_{\alpha} \to \infty$, the soft constraints do nothing, and there is no pooling; as $\sigma_{\alpha} \to 0$, they pull the estimates all the way to zero, yielding the complete-pooling estimate. This demonstrates that even if there is no dependence in the errors between groups, multilevel regression will still produce accurate point estimates and standard errors. \emph{Instead of making an assumption about independence, we can directly model it and let the data tell us if any exists.}

\nin This approach also reduces the number of parameters needing to be estimated. For example, a fixed effects approach with 20 groups would require estimating $J-1$ parameters (19). Introducing a soft-constraint and modeling $\alpha$ as a random draw from a single distribution, all we need to estimate is the parameters for this distribution. Typically we assume that the distribution is normal, so we only need to estimate two parameters (the mean $\mu$ and the variance $\sigma^2$). This increases our degrees of freedom (which could be an issue if we have lots of groups in the dataset). As seen below, this also allows us to introduce group-level predictors into the model.

\subss{Adding group-level predictors}
\nin Perhaps we believe certain group-level variables systematically influence the baseline level of $y_{ij}$. Using this flexible framework, we can simply include them directly in the model as predictors of $\alpha_{J}$:
\begin{align}
y_i &\sim N(\alpha_{j[i]} + \beta x_{i}, \sigma^2_{y}), \: \text{for} \: i=1,\dotsc,n, \nonumber \\
\alpha_{j} &\sim N(\gamma_{0} + \gamma_{1}u_{j},\sigma^2_{\alpha}), \: \text{for} \: j=1,\dotsc,J,
\end{align}
\nin where $x_{i}$ is a unit-specific indicator and $u_{j}$ is a group-level measure. Since we are leveraging multiple observations within each group, we can now include group-level measures without worrying about overspecifying the model or inducing perfect collinearity. The $\gamma$s can be interpreted in the same manner as the $\beta$s as they all are measured on the same dimension (e.g. linear regression--a one-unit increase on $u_{j}$ is associated with a $\gamma$ increase in $y_{ij}$; logit regression--a one-unit increase on $u_{j}$ is associated with a $\gamma$ increase in the log-odds of the $Pr(y_{ij}=1)$).


\subss{Varying slopes}
\nin Now what if we believe the effects of $\beta$ vary by group? We can treat this the same as we did with the varying intercept; thus,
\begin{align}
y_i &\sim N(\alpha_{j[i]} + \beta_{j[i]} x_{i}, \sigma^2_{y}), \: \text{for} \: i=1,\dotsc,n, \nonumber \\
 \begin{pmatrix}
 \alpha_{j} \\
 \beta_{j} \\
 \end{pmatrix} &\sim N\left(\begin{pmatrix}
 \mu_{\alpha} \\
 \mu_{\beta} \\
 \end{pmatrix},\begin{pmatrix}
 \sigma^2_{\alpha} & \rho \sigma_{\alpha} \sigma_{\beta} \\
 \rho \sigma_{\alpha} \sigma_{\beta} & \sigma^{2}_{\beta} & \\
 \end{pmatrix}\right), \: \text{for} \: j=1,\dotsc,J,
\end{align}
\nin with variation in the $\alpha_{j}$'s and the $\beta_{j}$'s and also a between-group correlation parameter. We can also include group-level predictors:
\begin{align}
y_i &\sim N(\alpha_{j[i]} + \beta_{j[i]} x_{i}, \sigma^2_{y}), \: \text{for} \: i=1,\dotsc,n, \nonumber \\
 \begin{pmatrix}
 \alpha_{j} \\
 \beta_{j} \\
 \end{pmatrix} &\sim N\left(\begin{pmatrix}
 \gamma^{\alpha}_{0} + \gamma^{\alpha}_{1} u_{j} \\
 \gamma^{\beta}_{0} + \gamma^{\beta}_{1} u_{j} \\
 \end{pmatrix},\begin{pmatrix}
 \sigma^2_{\alpha} & \rho \sigma_{\alpha} \sigma_{\beta} \\
 \rho \sigma_{\alpha} \sigma_{\beta} & \sigma^{2}_{\beta} & \\
 \end{pmatrix}\right), \: \text{for} \: j=1,\dotsc,J,
\end{align}
\nin In this formulation, we believe the same group-level variable explains both the intercept and the observation-specific covariate. This does not have to be true; we can include any combination of group-specific covariates in the separate models for unit-specific effects. Note that we should not use unit-specific variables in the models for $\alpha$ and $\beta$. If we believe such a relationship exists, we should specify it as an interaction in the model specific to $y_{ij}$.

\subss{Other specifications of MLM}
\nin Multilevel modeling is an extremely flexible framework which allows researchers to introduce all sorts of interesting specifications of models. Different types of data structures and specifications include:
\begin{itemize}
  \item Three or more levels of data,
  \item Repeated measures (i.e. panel data),
  \item Time-series cross sections,
  \item Non-nested structures
  \item Varying slopes without varying intercepts,
  \item Cross-level interactions,
  \item Discrete and non-normally distributed outcomes (e.g. dichotomous outcomes, binomial trials, count data, ordinal and multinomial outcomes),
  \item Bayesian estimation procedures,
  \item Measuring sample size and power calculations,
  \item Multilevel Regression, Imputation, and Post-Stratification (MRP)
\end{itemize}

\vspace{10cm}

\subss{Fake Data Revisited}
\begin{center}
\begin{figure}[H]\caption{Fake Data: MLM Estimation}\label{fig:Fdata_mlm}
\begin{center}
\resizebox{4.5in}{!}{\includegraphics{../Fdata_mlm.pdf}}
\end{center}
\end{figure}
\end{center}



\subs{Implementing MLM for Partisan Defection}
\nin Let the classical model be specified as
\begin{align}
 Pr(Y_{ei} = 1) &= \text{logit}^{-1}[\alpha + \beta_{1}\text{PID Intensity}_{ei} + \beta_{2}{\text{Relative Favorability}_{ei}} \nonumber \\
 &\: + \beta_{3}\text{Defect is Correct}_{ei}  + \beta_{4}\text{Incumbent Candidate}_{e}] \nonumber \\
  \text{logit}^{-1}[x] &= \frac{e^x}{1+e^x}
\end{align}
\nin where $Y_{ei}$ is a dichotomous variable coded one if the respondent reports voting for a presidential candidate from a different party other than his/her own, $\alpha$ is a constant, $\beta$s represent the log-odds association of the covariates with the probability of defection, and observations are pooled over elections ($e \in \{1976, 1980, 1984, 1988, 1992, 1996, 2000, 2004, 2008\}$). This is different from a traditional time series-cross sectional design in that respondents $i$ are not repeatedly interviewed: each election consists of a different representative sample of U.S. citizens. Therefore, the primary focus of the multilevel model should be on independence between elections. Will the baseline probability of defection differ across elections? Will the effects of partisan intensity or the other unit-specific measures differ across elections?

\subss{Classical Logistic Regression}
\nin Let's first examine the results of a classical logit model, without controlling for dependence within elections. We can estimate this model in \RR using the \texttt{glm} function.
\begin{verbatim}
> basic <- zelig(defect ~ pid_abs + feel_def + defect_cor + inc, data=data, model="logit")
> summary(basic)

Call:
glm(formula = formula, weights = w, family = binomial(link = "logit"),
    model = F, data = data)

Deviance Residuals:
    Min       1Q   Median       3Q      Max
-3.2426  -0.4235  -0.2060  -0.0746   3.6657

Coefficients:
             Estimate Std. Error z value Pr(>|z|)
(Intercept) -1.215209   0.086541 -14.042  < 2e-16 ***
pid_abs     -0.282456   0.049365  -5.722 1.05e-08 ***
feel_def     0.117239   0.003131  37.449  < 2e-16 ***
defect_cor   0.911967   0.075304  12.110  < 2e-16 ***
inc          0.748584   0.082140   9.113  < 2e-16 ***
---
Signif. codes:  0 �***� 0.001 �**� 0.01 �*� 0.05 �.� 0.1 � � 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 9118.6  on 10380  degrees of freedom
Residual deviance: 5288.9  on 10376  degrees of freedom
AIC: 5298.9

Number of Fisher Scoring iterations: 6
\end{verbatim}

\begin{center}
\begin{figure}[H]\caption{Logistic Regression Models of Partisan Defection}\label{fig:results_1}
\begin{center}
\resizebox{5.5in}{!}{\includegraphics{../results_1.pdf}}
\end{center}
\end{figure}
\end{center}

\nin These results indicate when the covariates equal zero, individuals are more likely to vote for a candidate from their own party rather than defect ($\alpha=-0.83$). PID intensity is also negatively associated with partisan defection (consistent with the hypothesized direction), while relative favorability, defect is correct, and incumbent candidate are all positive. Importantly, the standard errors for each coefficient are low with $\textit{p-value} < 0.01$. A classical logistic regression model suggests our coefficient estimates are very precise, and effects do not vary across election years.


\subss{Varying Intercept}
\nin What if we relax the assumption that the baseline probability of defection is constant for all observations? Let us model $\alpha$ directly from the data, specifying the model as
\begin{align}
 Pr(Y_{ei} = 1) &= \text{logit}^{-1}[\alpha_{e[i]} + \beta_{1}\text{PID Intensity}_{ei} \nonumber \\
 &\quad + \beta_{2}{\text{Relative Favorability}_{ei}} + \beta_{3}\text{Defect is Correct}_{ei}]  \nonumber \\
 \alpha_{e} &\sim N(\gamma_{0}^{\alpha} + \gamma_{1}^{\alpha}\text{Incumbent},\sigma^2_{\alpha}), \: \text{for} \: e=1976,\dotsc,2008, \nonumber \\
 \text{logit}^{-1}[x] &= \frac{e^x}{1+e^x}
\end{align}
In \RR, we can use the \texttt{glmer} function in the \texttt{lme4} library to estimate generalized linear multilevel models.
\begin{verbatim}
> vary.int <- zelig(defect ~ pid_abs + feel_def + defect_cor + inc + tag(1|year), data=data, model="logit.mixed")
> summary(vary.int)
Generalized linear mixed model fit by the Laplace approximation
Formula: defect ~ pid_abs + feel_def + defect_cor + inc + tag(1 | year)
   Data: data
  AIC  BIC logLik deviance
 5209 5253  -2599     5197
Random effects:
 Groups Name        Variance Std.Dev.
 year   (Intercept) 0.11903  0.34501
Number of obs: 10381, groups: year, 10

Fixed effects:
             Estimate Std. Error z value Pr(>|z|)
(Intercept) -1.196652   0.217673   -5.50 3.85e-08 ***
pid_abs     -0.287671   0.049890   -5.77 8.11e-09 ***
feel_def     0.120722   0.003229   37.39  < 2e-16 ***
defect_cor   0.850027   0.077978   10.90  < 2e-16 ***
inc          0.640060   0.252780    2.53   0.0113 *
---
Signif. codes:  0 �***� 0.001 �**� 0.01 �*� 0.05 �.� 0.1 � � 1

Correlation of Fixed Effects:
           (Intr) pid_bs fel_df dfct_c
pid_abs    -0.206
feel_def    0.038  0.013
defect_cor -0.143  0.177 -0.060
inc        -0.808 -0.011  0.026 -0.025
\end{verbatim}

\begin{center}
\begin{figure}[H]\caption{Logistic Regression Models of Partisan Defection}\label{fig:results_2}
\begin{center}
\resizebox{5.5in}{!}{\includegraphics{../results_2.pdf}}
\end{center}
\end{figure}
\end{center}

\nin The top of the summary reports general model statistics such as log-likelihood, deviance, AIC, and BIC. These statistics can be used for model comparisons and relative goodness-of-fit tests. Below this is a table with the variance components. Since logistic regression assumes a constant unit-level variance of $\frac{\pi^2}{3}$, there is no variance component for this level. The only variance component estimated in the model is $\sigma^2_{\alpha}$. The standard deviation is 0.34, suggesting a significant amount of variance around the average intercept. This result confirms our expectation since we know the mean defection rate differs across elections (see Figure~\ref{fig:year_defect}).

\nin The remaining estimated coefficients do not differ significantly from the original non-MLM results.

\nin Multilevel modeling also allows us to retain group-specific estimates of varying coefficients. These estimates are combinations of the fixed and random effects.\footnote{Alternative approaches include Bayesian shrinkage estimators, which are combinations of complete and no pooling estimates using the reliability of the group-specific estimate to determine the weighted average. Though biased towards the overall mean, this estimator is more precise than either complete or no pooling estimates since it accounts for uncertainty in the model. Groups with larger sample sizes will be weighted more heavily towards the mean value of the observations within the group since we possess more certainty about the group-specific contribution to the model.} These group-specific effects can be generated automatically in \RR using the \texttt{coef} function.
\begin{verbatim}
> coef(vary.int)
$year
     (Intercept)    pid_abs  feel_def defect_cor       inc
1972  -1.2707808 -0.2876706 0.1207224  0.8500272 0.6400601
1976  -1.5697500 -0.2876706 0.1207224  0.8500272 0.6400601
1980  -0.9761839 -0.2876706 0.1207224  0.8500272 0.6400601
1984  -1.4226680 -0.2876706 0.1207224  0.8500272 0.6400601
1988  -0.9434875 -0.2876706 0.1207224  0.8500272 0.6400601
1992  -0.4546737 -0.2876706 0.1207224  0.8500272 0.6400601
1996  -1.0803212 -0.2876706 0.1207224  0.8500272 0.6400601
2000  -1.3294888 -0.2876706 0.1207224  0.8500272 0.6400601
2004  -1.5873772 -0.2876706 0.1207224  0.8500272 0.6400601
2008  -1.3083487 -0.2876706 0.1207224  0.8500272 0.6400601
\end{verbatim}
\nin In every year, respondents on average have a low baseline log-odds of defecting (holding all covariates equal to zero). As suggested by $\sigma^2_{\alpha}$, the magnitude of this coefficient varies significantly, ranging from a low of $-1.59$ to a high of $-0.45$.

\subss{Varying slope(s) and intercept}
\nin Now let us consider potential varying slopes. While we could estimate the model with a varying slope for each coefficient, unless we have hundreds of thousands of observations we will quickly run into issues maximizing the log-likelihood (this will be covered shortly under the limitations section). First and foremost we should use theory to guide our selection of varying slopes. However if it seems plausible that any of the covariates should need a varying slope, we need a method to determine which coefficients should receive one in the final model.

\nin One can use a likelihood ratio test to determine whether or not the additional variance component significantly improves overall model fit. Note that since GLM implementations of MLM utilize the Laplace approximation rather than full MLE, deviance or likelihood ratio tests can only be applied to models with identical constant parameters. That is, not only must the dataset be the same, the model specifications must be identical except for the variance component. The results of a test to determine if a varying slope should be utilized for PID intensity is
\begin{verbatim}
> defect <- glmer(defect ~ pid_abs + feel_def + defect_cor + inc + (1|year), data=data, family=binomial(link="logit"))
> defect.pid3.rc1 <- glmer(defect ~ pid_abs + feel_def + defect_cor + inc + (1|year) + (pid_abs|year), data=data, family=binomial(link="logit"))
> anova(defect,defect.pid3.rc1)
Data: data
Models:
defect: defect ~ pid_abs + feel_def + defect_cor + inc + (1 | year)
defect.pid3.rc1: defect ~ pid_abs + feel_def + defect_cor + inc + (1 | year) +
defect.pid3.rc1:     (pid_abs | year)
                Df    AIC    BIC  logLik  Chisq Chi Df Pr(>Chisq)
defect           6 5209.1 5252.6 -2598.6
defect.pid3.rc1  9 5214.5 5279.7 -2598.2 0.6265      3     0.8903
\end{verbatim}
\nin The bottom two lines report the degrees of freedom, AIC, BIC, and log-likelihood for each model. The $\chi^2$ test statistic is simply the difference in log-likelihoods, with the degrees of freedom for the test statistic derived from the difference in the overall models' degrees of freedom. Notice that by specifying a varying slope for PID intensity, we're actually adding three parameters to the model: $\mu_{\beta_{1}}$, $\sigma^2_{\beta_{1}}$ and the correlation $\rho$ between $\sigma^2_{\alpha}$ and $\sigma^2_{\beta_{1}}$. The p-value for the test statistic is $0.89$, far above the traditional threshold of $0.05$. Adding a varying slope for PID intensity does not significantly improve overall model fit, so it is unnecessary in the final model. Sequential tests of the other covariates indicate relative favorability should vary across elections, so the final model is specified as:
\begin{align}
 Pr(Y_{ei} = 1) &= \text{logit}^{-1}[\alpha_{e[i]} + \beta_{1}\text{PID Intensity}_{ei} \nonumber \\
 &\quad + \beta_{2e[i]}{\text{Relative Favorability}_{ei}} + \beta_{3}\text{Defect is Correct}_{ei}]  \nonumber \\
 \begin{pmatrix}
 \alpha_{e} \\
 \beta_{2e} \\
 \end{pmatrix} &\sim N\left(\begin{pmatrix}
 \gamma_{0}^{\alpha} + \gamma_{1}^{\alpha}\text{Incumbent} \\
 \gamma_{0}^{\beta_{2}} \\
 \end{pmatrix},\begin{pmatrix}
 \sigma^2_{\alpha} & \rho\sigma_{\alpha}\sigma_{\beta_2}  \\
 \rho\sigma_{\alpha}\sigma_{\beta_2} & \sigma^{2}_{\beta_2}  \\
 \end{pmatrix}\right) \nonumber \\
  \text{logit}^{-1}[x] &= \frac{e^x}{1+e^x}
\end{align}
\newpage
\begin{verbatim}
> final <- zelig(defect ~ pid_abs + feel_def + defect_cor + inc + tag(1+feel_def|year), data=data, model="logit.mixed")
> summary(final)
Generalized linear mixed model fit by the Laplace approximation
Formula: defect ~ pid_abs + feel_def + defect_cor + inc + tag(1 + feel_def |      year)
   Data: data
  AIC  BIC logLik deviance
 5166 5232  -2574     5148
Random effects:
 Groups Name        Variance   Std.Dev.   Corr
 year   (Intercept) 8.5640e-12 2.9264e-06
 year   (Intercept) 6.4193e-02 2.5336e-01
        feel_def    4.0744e-04 2.0185e-02 -1.000
Number of obs: 10381, groups: year, 10

Fixed effects:
             Estimate Std. Error z value Pr(>|z|)
(Intercept) -1.012315   0.139544  -7.254 4.03e-13 ***
pid_abs     -0.296685   0.049976  -5.937 2.91e-09 ***
feel_def     0.128643   0.007275  17.682  < 2e-16 ***
defect_cor   0.823013   0.077535  10.615  < 2e-16 ***
inc          0.394069   0.134336   2.933  0.00335 **
---
Signif. codes:  0 �***� 0.001 �**� 0.01 �*� 0.05 �.� 0.1 � � 1

Correlation of Fixed Effects:
           (Intr) pid_bs fel_df dfct_c
pid_abs    -0.311
feel_def   -0.447  0.002
defect_cor -0.229  0.179 -0.036
inc        -0.662 -0.034 -0.033 -0.035
> coef(final)
$year
     (Intercept)   pid_abs   feel_def defect_cor       inc
1972   -1.012315 -0.296685 0.12479018  0.8230129 0.3940692
1976   -1.012315 -0.296685 0.14576976  0.8230129 0.3940692
1980   -1.012315 -0.296685 0.10751863  0.8230129 0.3940692
1984   -1.012315 -0.296685 0.14385602  0.8230129 0.3940692
1988   -1.012315 -0.296685 0.12698725  0.8230129 0.3940692
1992   -1.012315 -0.296685 0.08611483  0.8230129 0.3940692
1996   -1.012315 -0.296685 0.11673889  0.8230129 0.3940692
2000   -1.012315 -0.296685 0.14193514  0.8230129 0.3940692
2004   -1.012315 -0.296685 0.14187790  0.8230129 0.3940692
2008   -1.012315 -0.296685 0.14710442  0.8230129 0.3940692
\end{verbatim}

\begin{center}
\begin{figure}[H]\caption{Logistic Regression Models of Partisan Defection}\label{fig:results_3}
\begin{center}
\resizebox{5.5in}{!}{\includegraphics{../results_3.pdf}}
\end{center}
\end{figure}
\end{center}


\begin{center}
\begin{figure}[H]\caption{Election-Specific Estimates of the Constant for Partisan Defection}\label{fig:constant}
\begin{center}
\resizebox{5.5in}{!}{\includegraphics{../constant.pdf}}
\end{center}

\footnotesize
\raggedright
Note: Point estimates and confidence intervals derived from 1000 simulations of the posterior distribution for $\hat{\alpha}$.
\end{figure}
\end{center}


\begin{center}
\begin{figure}[H]\caption{Election-Specific Estimates of the Effect of Relative Favorability on Partisan Defection}\label{fig:relfav}
\begin{center}
\resizebox{5.5in}{!}{\includegraphics{../relfav.pdf}}
\end{center}

\footnotesize
\raggedright
Note: Point estimates and confidence intervals derived from 1000 simulations of the posterior distribution for $\hat{\beta_{2}}$.
\end{figure}
\end{center}



\subs{Limitations of MLM for this application}
\subss{Likelihood maximization}
\nin Multilevel modeling is not without its drawbacks. Perhaps the most important in this application is the requirement of approximations for the likelihood. MLE for a linear multi-level model works because the portions of the likelihood function which need maximizing are (relatively) easily calculated by optimizers. Specifically, all the variance components are additive combinations of several normally distributed variables which themselves have a normal distribution. The variance is the same for all possible predicted values of $Y_{ei}$. This would also be true for a logistic regression with no multilevel component.

\nin When you make a non-linear model multilevel, you have to add the group-level variability to the non-linear system of equations. There is a probability distribution of fitted values and a probability of a yes response goes with each fitted value (based upon the link function and probability process). The probabilities for each point along the linear predictor $\eta$ are weighted by group-level variability. Since the function is non-linear, the computation of probability for each case is terribly complex to integrate. Computing MLE is enormously more complex for generalized (nonlinear) multilevel models.

\nin The \texttt{lme4} package in \RR uses an approximation of the likelihood through the Laplace approximation. Another approache is Restricted Estimation of Maximum Likelihood (REML). These approximations work pretty well for estimating $\gamma$s, but less so for $\sigma$s. If the researcher cares most about understanding how and why parameters vary across groups/elections, approximate likelihood approaches will not be useful. I cannot be confident that the $\hat{\sigma}$ are fully accurate, so any measures of statistical significance or associated confidence intervals could be suspect (though the decision to utilize such an interpretation remains with the researcher).

\nin One could potentially optimize the full MLE, but this may be difficult to do and would require a significant amount of time to maximize the likelihood function if it is even possible. Alternatively we could use Bayesian estimation methods to simulate election-specific effects (see Figure~\ref{fig:relfav}), but again these methods are computationally intensive and require an understanding of Bayesian approaches to inference.

\subss{Determining which coefficients should have varying slopes}
\nin We would like to have a theoretical guide to determining which coefficients should be allowed to vary. Ideally, we could apply soft constraints to each coefficient and allow the data to tell us whether or not there is significant variance between years. The more observations one has in the data, the more parameters that can be estimated (more data $=$ more information about the likelihood the model fits the data). However even with the reduced number of parameters compared to a fixed effects approach, estimating approximate likelihoods with more than two or three varying slopes becomes extremely lengthy and laborious. We would need hundreds of thousands of observations to estimate even a half-dozen varying slopes.



\newpage
\subs{Appendix}
\begin{table}[ht]
\centering
\caption{Estimated Influences on Partisan Defection in U.S. Presidential Elections, 1972--2008}
\label{tab:defect}
\begin{tabular}{lr@{.}lr@{.}lr@{.}l}
  \hline
& \multicolumn{2}{c}{Classical GLM} & \multicolumn{2}{c}{Varying Intercept} & \multicolumn{2}{c}{Varying Slopes} \\
  \hline
\hline
Constant & -1&22 $^*$ & -1&20 $^*$ & -1&01 $^*$ \\
   & (0&09) & (0&22) & (0&14) \\
  PID Intensity & -0&28 $^*$ & -0&29 $^*$ & -0&30 $^*$ \\
   & (0&05) & (0&05) & (0&05) \\
  Relative Favorability & 0&12 $^*$ & 0&12 $^*$ & 0&13 $^*$ \\
   & (0&00) & (0&00) & (0&01) \\
  Defect is Correct & 0&91 $^*$ & 0&85 $^*$ & 0&82 $^*$ \\
   & (0&08) & (0&08) & (0&08) \\
  Incumbent Candidate & 0&75 $^*$ & 0&64 $^*$ & 0&39 $^*$ \\
   & (0&08) & (0&25) & (0&13) \\
 $\sigma_{\alpha}$ & \multicolumn{2}{c}{--} & 0&345 & 0&253 \\ 
  $\sigma_{\beta_{2}}$ & \multicolumn{2}{c}{--} & \multicolumn{2}{c}{--} & 0&020 \\
  PRE & 0&37 & 0&38 & 0&38 \\
  AIC & 5298&95 & 5209&10 & 5166&50 \\
  Number of observations & \multicolumn{2}{l}{10381} & \multicolumn{2}{l}{10381} & \multicolumn{2}{l}{10381} \\
  \hline
\hline
 \multicolumn{7}{l}{\footnotesize{Coefficients reported as log-odds. Standard errors in parentheses.}}\\
\multicolumn{7}{l}{\footnotesize{$^*$ indicates significance at $p< 0.05 $}}
\end{tabular}
\end{table}


\begin{sidewaystable}[ht]
\centering
\caption{Estimated Influences on Partisan Defection in Presidential Elections, 1972--2008}
\label{tab:defect_abs}
\begin{tabular}{lr@{.}lr@{.}lr@{.}lr@{.}lr@{.}ll}
  \hline
Year & \multicolumn{2}{c}{Intercept} & \multicolumn{2}{c}{Partisan Intensity} & \multicolumn{2}{c}{Relative Favorability} & \multicolumn{2}{c}{Defect is Correct} & \multicolumn{2}{c}{Incumbent} & Fit Stats \\
  \hline
Overall & -1&22* & -0&28* & 0&12* & 0&91* & 0&75* & Deviance = 5288.95 \\
  (N = 10381) & (0&09) & (0&05) & (0&00) & (0&08) & (0&08) & PRE = 0.37 \\
  1972 & -0&67* & -0&46* & 0&12* & 1&26* & \multicolumn{2}{l}{--} & Deviance = 381.25 \\
  (N = 741) & (0&25) & (0&20) & (0&01) & (0&26) & \multicolumn{2}{l}{--} & PRE = 0.61 \\
  1976 & -1&02* & -0&22 & 0&14* & 0&88* & \multicolumn{2}{l}{--} & Deviance = 358.33 \\
  (N = 740) & (0&23) & (0&20) & (0&01) & (0&32) & \multicolumn{2}{l}{--} & PRE = 0.45 \\
  1980 & -0&42 & -0&29 & 0&10* & 0&85* & \multicolumn{2}{l}{--} & Deviance = 621.53 \\
  (N = 832) & (0&25) & (0&15) & (0&01) & (0&22) & \multicolumn{2}{l}{--} & PRE = 0.36 \\
  1984 & -0&77* & 0&01 & 0&16* & 0&12 & \multicolumn{2}{l}{--} & Deviance = 482.42 \\
  (N = 1244) & (0&22) & (0&17) & (0&01) & (0&28) & \multicolumn{2}{l}{--} & PRE = 0.51 \\
  1988 & -1&21* & -0&01 & 0&15* & 1&24* & \multicolumn{2}{l}{--} & Deviance = 459.48 \\
  (N = 1094) & (0&23) & (0&17) & (0&01) & (0&26) & \multicolumn{2}{l}{--} & PRE = 0.41 \\
  1992 & 0&07 & -0&37* & 0&09* & 0&60* & \multicolumn{2}{l}{--} & Deviance = 1246.77 \\
  (N = 1493) & (0&14) & (0&10) & (0&01) & (0&15) & \multicolumn{2}{l}{--} & PRE = 0.30 \\
  1996 & -0&21 & -0&50* & 0&12* & 0&70* & \multicolumn{2}{l}{--} & Deviance = 545.95 \\
  (N = 1048) & (0&21) & (0&16) & (0&01) & (0&24) & \multicolumn{2}{l}{--} & PRE = 0.36 \\
  2000 & -1&16* & -0&71* & 0&13* & 1&21* & \multicolumn{2}{l}{--} & Deviance = 356.40 \\
  (N = 1026) & (0&23) & (0&20) & (0&01) & (0&29) & \multicolumn{2}{l}{--} & PRE = 0.33 \\
  2004 & -1&14* & -0&28 & 0&13* & 1&24* & \multicolumn{2}{l}{--} & Deviance = 216.60 \\
  (N = 752) & (0&30) & (0&26) & (0&01) & (0&38) & \multicolumn{2}{l}{--} & PRE = 0.38 \\
  2008 & -1&42* & -0&14 & 0&15* & 0&89* & \multicolumn{2}{l}{--} & Deviance = 403.09 \\
  (N = 1411) & (0&23) & (0&18) & (0&01) & (0&32) & \multicolumn{2}{l}{--} & PRE = 0.41 \\
   \hline
\hline
 \multicolumn{12}{l}{\footnotesize{Coefficients reported as log-odds. Standard errors in parentheses.}}\\
\multicolumn{12}{l}{\footnotesize{$^*$ indicates significance at $p< 0.05 $}}
\end{tabular}
\end{sidewaystable}

# Acknowledgments {.toc-ignore}

* [Gelman, Andrew, and Jennifer Hill. *Data analysis using regression and multilevel/hierarchical models*. Cambridge university press, 2006.](http://www.stat.columbia.edu/~gelman/arm/)

# Session Info {.toc-ignore}

```{r child='_sessioninfo.Rmd'}
```




