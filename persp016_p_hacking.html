<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />


<meta name="author" content="MACS 30200 - Perspectives on Computational Research" />


<title>p-hacking</title>

<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/readable.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="site_libs/jqueryui-1.11.4/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<script src="site_libs/navigation-1.1/codefolding.js"></script>
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-45631879-2', 'auto');
  ga('send', 'pageview');

</script>


<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
div.sourceCode {
  overflow-x: visible;
}
</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>


<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>


</head>

<body>

<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
.tabbed-pane {
  padding-top: 12px;
}
button.code-folding-btn:focus {
  outline: none;
}
</style>


<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 66px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 71px;
  margin-top: -71px;
}

.section h2 {
  padding-top: 71px;
  margin-top: -71px;
}
.section h3 {
  padding-top: 71px;
  margin-top: -71px;
}
.section h4 {
  padding-top: 71px;
  margin-top: -71px;
}
.section h5 {
  padding-top: 71px;
  margin-top: -71px;
}
.section h6 {
  padding-top: 71px;
  margin-top: -71px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>


<div class="container-fluid main-container">

<!-- tabsets -->
<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});
</script>

<!-- code folding -->
<style type="text/css">
.code-folding-btn { margin-bottom: 4px; }
</style>
<script>
$(document).ready(function () {
  window.initializeCodeFolding("hide" === "show");
});
</script>




<script>
$(document).ready(function ()  {

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_').toLowerCase();
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}


.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
  padding-left: 25px;
  text-indent: 0;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>

<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row-fluid">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">Computing for the Social Sciences</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">Home</a>
</li>
<li>
  <a href="faq.html">FAQ</a>
</li>
<li>
  <a href="syllabus.html">Syllabus</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div class="fluid-row" id="header">

<div class="btn-group pull-right">
<button type="button" class="btn btn-default btn-xs dropdown-toggle" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false"><span>Code</span> <span class="caret"></span></button>
<ul class="dropdown-menu" style="min-width: 50px;">
<li><a id="rmd-show-all-code" href="#">Show All Code</a></li>
<li><a id="rmd-hide-all-code" href="#">Hide All Code</a></li>
</ul>
</div>



<h1 class="title toc-ignore">p-hacking</h1>
<h4 class="author"><em>MACS 30200 - Perspectives on Computational Research</em></h4>

</div>


<div id="objectives" class="section level1">
<h1>Objectives</h1>
<ul>
<li>Define a p-value</li>
<li>Identify methods for obtaining a statistically significant p-value</li>
<li>Critique the “women wearing pink” article for potential p-hacking</li>
<li>Identify approaches to reduce the chance of p-hacking</li>
</ul>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(tidyverse)
<span class="kw">library</span>(broom)
<span class="kw">library</span>(forcats)
<span class="kw">library</span>(modelr)
<span class="kw">library</span>(stringr)
<span class="kw">library</span>(car)
<span class="kw">library</span>(rcfss)
<span class="kw">library</span>(coefplot)
<span class="kw">library</span>(RColorBrewer)
<span class="kw">library</span>(lme4)

<span class="kw">options</span>(<span class="dt">digits =</span> <span class="dv">3</span>)
<span class="kw">set.seed</span>(<span class="dv">1234</span>)
<span class="kw">theme_set</span>(<span class="kw">theme_minimal</span>())</code></pre></div>
</div>
<div id="what-is-a-p-value" class="section level1">
<h1>What is a p-value?</h1>
<div id="hypothesis-testing" class="section level2">
<h2>Hypothesis testing</h2>
<p>When conducting inference, we test hypotheses about real-world relationships using observed data. Because we generally rely on a <strong>sample</strong> rather than the <strong>population</strong>, there is a chance that the results we observe could be purely driven by random chance. We just happened to draw a fluky sample from the population and the statistic of interest we observe is not in fact representative of the broader population.</p>
<p>In the context of regression, we generally test the <strong>null hypothesis</strong> against the <strong>alternative hypothesis</strong>. The null hypothesis <span class="math inline">\(H_0\)</span> states “there is no relationship between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>”, whereas the alternative hypothesis <span class="math inline">\(H_a\)</span> states “there is some relationship between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>”. Mathematically, this corresponds to</p>
<ul>
<li><span class="math inline">\(H_0\)</span>: <span class="math inline">\(\beta_1 = 0\)</span></li>
<li><span class="math inline">\(H_a\)</span>: <span class="math inline">\(\beta_1 \neq 0\)</span></li>
</ul>
<p>since if <span class="math inline">\(\beta_1 = 0\)</span>, then the model reduces to <span class="math inline">\(Y = \beta_0 + \epsilon\)</span> and <span class="math inline">\(X\)</span> is not associated with <span class="math inline">\(Y\)</span>. To test the null hypothesis, we need to determine whether the estimated coefficient <span class="math inline">\(\hat{\beta}_1\)</span> is sufficiently far from zero that we can be confident that <span class="math inline">\(\beta_1\)</span> is non-zero.</p>
<p>This determination is based on the <strong>standard error</strong> of the coefficient. If <span class="math inline">\(\text{SE}(\hat{\beta}_1)\)</span> is small, than even relatively small values of <span class="math inline">\(\hat{\beta}_1\)</span> may provide strong evidence that <span class="math inline">\(\beta_1 \neq 0\)</span> and there is a relationship between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>. If <span class="math inline">\(\text{SE}(\hat{\beta}_1)\)</span> is large, than <span class="math inline">\(\hat{\beta}_1\)</span> must also be large to reject the null hypothesis. In practice, we compute a <strong>t-statistic</strong> given by</p>
<p><span class="math display">\[t = \frac{\hat{\beta}_1 - 0}{\text{SE}(\hat{\beta}_1)}\]</span></p>
<p>which measures the number of standard deviations that <span class="math inline">\(\hat{\beta}_1\)</span> is away from 0. If there really is no relationship between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, then we expect this function to follow a <span class="math inline">\(t\)</span>-distribution with <span class="math inline">\(n-2\)</span> degrees of freedom. From this we can calculate the probability of observing any value equal to <span class="math inline">\(|t|\)</span> or larger, assuming <span class="math inline">\(\beta_1=0\)</span>. This is the <strong>p-value</strong>. Informally, a small p-value indicates that it is unlikely to observare such substantial association between the predictor and the response due to chance, in the absence of any real association between the predictor and response.</p>
</div>
<div id="the-importance-of-the-.05-cutoff" class="section level2">
<h2>The importance of the .05 cutoff</h2>
<div class="figure">
<img src="http://marginalrevolution.com/wp-content/uploads/2014/05/Type-I-and-II-errors1-625x468.jpg" />

</div>
<p>p-values can fall anywhere between 0 and 1. Setting the cutoff point determines how likely one is to make a type I (false positive) or type II (false negative) error. Historically, the social sciences adopted a cutoff of <span class="math inline">\(p=.05\)</span>. Anything below that value is considered “statistically significant”. But there is no scientific basis for this threshold - it is an arbitrary standard, to which many academic journals and scholars continue to adhere.</p>
</div>
<div id="what-p-values-can-and-cannot-do" class="section level2">
<h2>What p-values can and cannot do</h2>
<p>p-values are a key component of frequentist inference, dating back to the early 1900s. While their usage is widespread in academia, in practice their meaning is actually very limited.</p>
<blockquote>
<p>A p-value is the probability under a specified statistical model that a statistical summary of the data would be equal to or more extreme than its observed values.</p>
</blockquote>
<p><strong>p-values can indicate how incompatible the data are with a specified statistical model</strong>. This is why they are used for null hypothesis testing. The smaller the p-value, the greater the statistical incompatability of the data with the null hypothesis. This incompatability casts doubt on or provides evidence agains the null hypothesis.</p>
<p><strong>But it does not prove the alternative hypothesis to be true.</strong></p>
<ul>
<li><p>p-values do not measure the probability that the observed relationship is true, or the probability that the data were produced by random chance alone. p-values cannot be used to prove the null hypothesis:</p>
<div class="figure">
<img src="http://www.azquotes.com/picture-quotes/quote-the-absence-of-evidence-is-not-the-evidence-of-absence-carl-sagan-43-51-12.jpg" />

</div>
p-values do not provide any conclusion about the explanation itself, only the probability of the observed data being generated if the null hypothesis is true.</li>
<li>p-values below an arbitrary threshold should not be used for scientific, business, or policy conclusions. Again, the .05 threshold is a completely arbitrary standard. A statistic with a p-value of <span class="math inline">\(.04\)</span> is not “true” versus a statistic with a p-value of <span class="math inline">\(.06\)</span> is false. p-values are determined not only by the data itself but also by the researcher’s decisions used to formulate and estimate the statistical model, and there are many ways to tweak or search for a significant p-value.</li>
<li>p-values and statistical significance do not measure the size or importance of a result. Statistical significance is not the same thing as scientific, human, or economic significance. Smaller p-values do not imply important effects. p-values are determined by the precision of the estimated statistic, which is in turn influenced by sample size and measurement accuracy. A statistic can have a very small p-value but have an insignificant substantive effect because of a large sample size. Likewise, even large effects can have unimpressive p-values if the sample size is small.</li>
<li><p>The p-value does not provide a good measure of evidence regarding a model or hypothesis. Just because a p-value is large does not mean the null hypothesis is true; many other alternative hypotheses could exist to explain the observed data, you just did not test for it.</p></li>
</ul>
</div>
</div>
<div id="how-to-find-a-significant-p-value" class="section level1">
<h1>How to find a significant p-value</h1>
<p>Consider a sample of 100 observations of a continuous outcome of interest <span class="math inline">\(Y\)</span> measured with 10 continuous covariates <span class="math inline">\(\mathbf{X}\)</span>. In truth, none of the variables are actually predictive of <span class="math inline">\(Y\)</span> in the population. That is, all the covariates <span class="math inline">\(\mathbf{X}\)</span> and the outcome <span class="math inline">\(Y\)</span> are drawn independently from a normal distribution <span class="math inline">\(\sim N(0,1)\)</span>. If we use <a href="persp003_linear_regression.html">ordinary least squares regression</a> and focus on just a single variable as a predictor, a test of significance will yield <span class="math inline">\(p &lt; .05\)</span> in approximately 5% of the samples. Below I simulate this process 1000 times, and in each simulation estimate a single regression model between <span class="math inline">\(Y\)</span> and a randomly selected <span class="math inline">\(X_k\)</span>:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">n_obs &lt;-<span class="st"> </span><span class="dv">100</span>

pval_dist &lt;-<span class="st"> </span>function(n_obs){
  x &lt;-<span class="st"> </span><span class="kw">replicate</span>(<span class="dv">10</span>, <span class="kw">rnorm</span>(n_obs))
  y &lt;-<span class="st"> </span><span class="kw">rnorm</span>(n_obs)
  
  mod &lt;-<span class="st"> </span><span class="kw">lm</span>(y ~<span class="st"> </span>x[, <span class="kw">sample</span>(<span class="dv">1</span>:<span class="dv">10</span>, <span class="dv">1</span>)])
  
  <span class="kw">return</span>(<span class="kw">tidy</span>(mod)[<span class="dv">2</span>,])
}

pvals &lt;-<span class="st"> </span><span class="dv">1000</span> %&gt;%
<span class="st">  </span><span class="kw">rerun</span>(<span class="kw">pval_dist</span>(n_obs)) %&gt;%
<span class="st">  </span>bind_rows %&gt;%
<span class="st">  </span>as_tibble %&gt;%
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">sig =</span> p.value &lt;<span class="st"> </span>.<span class="dv">05</span>)

<span class="kw">ggplot</span>(pvals, <span class="kw">aes</span>(p.value, <span class="dt">fill =</span> sig)) +
<span class="st">  </span><span class="kw">geom_histogram</span>(<span class="dt">binwidth =</span> .<span class="dv">025</span>, <span class="dt">boundary =</span> <span class="dv">0</span>) +
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="st">&quot;Distribution of p-values when null is true&quot;</span>,
       <span class="dt">x =</span> <span class="kw">expression</span>(P),
       <span class="dt">y =</span> <span class="ot">NULL</span>) +
<span class="st">  </span><span class="kw">theme</span>(<span class="dt">legend.position =</span> <span class="st">&quot;none&quot;</span>)</code></pre></div>
<p><img src="persp016_p_hacking_files/figure-html/pval-sim-1.png" width="672" /></p>
<p>The distribution of the p-values is approximately uniform and on average 5% of the p-values are <span class="math inline">\(&lt; .05\)</span>. In this situation, the p-value and our inferences drawn from the p-value are as we would expect because we conducted exactly one null hypothesis test against the sample of data.</p>
<p>What happens instead if we evaluate multiple variables during each test? That is, in each iteration of our simulation we regress all 10 predictors individually against <span class="math inline">\(Y\)</span>. What is the chance that we will find <span class="math inline">\(p &lt; .05\)</span> for at least one of the predictors?</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">pval_dist_mult &lt;-<span class="st"> </span>function(n_obs){
  <span class="co"># generate simulated data</span>
  x &lt;-<span class="st"> </span><span class="kw">replicate</span>(<span class="dv">10</span>, <span class="kw">rnorm</span>(n_obs))
  y &lt;-<span class="st"> </span><span class="kw">rnorm</span>(n_obs)
  
  <span class="co"># estimate a linear model for each column in x and find min pvalue</span>
  x %&gt;%
<span class="st">    </span>as_tibble %&gt;%
<span class="st">    </span><span class="kw">mutate</span>(<span class="dt">y =</span> y) %&gt;%
<span class="st">    </span><span class="kw">gather</span>(i, x, -y) %&gt;%
<span class="st">    </span><span class="kw">group_by</span>(i) %&gt;%
<span class="st">    </span><span class="kw">nest</span>() %&gt;%
<span class="st">    </span><span class="kw">mutate</span>(<span class="dt">mod =</span> <span class="kw">map</span>(data, ~<span class="st"> </span><span class="kw">lm</span>(y ~<span class="st"> </span>x, <span class="dt">data =</span> .x)),
           <span class="dt">results =</span> <span class="kw">map</span>(mod, tidy)) %&gt;%
<span class="st">    </span><span class="kw">unnest</span>(results) %&gt;%
<span class="st">    </span><span class="kw">filter</span>(term ==<span class="st"> &quot;x&quot;</span>) %&gt;%
<span class="st">    </span><span class="kw">filter</span>(p.value ==<span class="st"> </span><span class="kw">min</span>(p.value))
}

pvals_mult &lt;-<span class="st"> </span><span class="dv">1000</span> %&gt;%
<span class="st">  </span><span class="kw">rerun</span>(<span class="kw">pval_dist_mult</span>(n_obs)) %&gt;%
<span class="st">  </span>bind_rows %&gt;%
<span class="st">  </span>as_tibble %&gt;%
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">sig =</span> p.value &lt;<span class="st"> </span>.<span class="dv">05</span>)

<span class="kw">ggplot</span>(pvals_mult, <span class="kw">aes</span>(p.value, <span class="dt">fill =</span> sig)) +
<span class="st">  </span><span class="kw">geom_histogram</span>(<span class="dt">binwidth =</span> .<span class="dv">025</span>, <span class="dt">boundary =</span> <span class="dv">0</span>) +
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="st">&quot;Distribution of minimmum p-values for 10 tests when null is true&quot;</span>,
       <span class="dt">x =</span> <span class="kw">expression</span>(P),
       <span class="dt">y =</span> <span class="ot">NULL</span>) +
<span class="st">  </span><span class="kw">theme</span>(<span class="dt">legend.position =</span> <span class="st">&quot;none&quot;</span>)</code></pre></div>
<p><img src="persp016_p_hacking_files/figure-html/pval-sim-mult-test-1.png" width="672" /></p>
<p>Now we have a 40% chance of finding a predictor with <span class="math inline">\(p &lt; .05\)</span>, and the distribution of the minimum p-values is not uniform. When we search for the most significant result, we do not have a fixed null hypothesis. Instead, we are conducting 10 null hypothesis tests using the same sample of data. The distribution of the minimum of 10 random uniform distributions has a density <span class="math inline">\(k(1 - x)^{k-1}\)</span> for <span class="math inline">\(k\)</span> independent tests. When <span class="math inline">\(k=10\)</span>, the probability of observing <span class="math inline">\(p &lt; .05\)</span> is <span class="math inline">\(1 - (1 - 0.05)^{10} = 0.40\)</span>.</p>
<p>By failing to correct for the fact that we conducted multiple hypothesis tests on the same sample of data, we risk <strong>false discovery</strong> and is a form of selection bias. Even if the tests were not actually performed, we still risk selection bias when any choice of results is based on the outcome, rather than the prespecified hypotheses.</p>
<p><strong>This happens all the time in social science.</strong> All scholars and researchers do this. We form a theory, generate hypotheses, collect data to test the hypotheses, and explore multiple model formulations until we settle on the final form. This doesn’t mean we are attempting to commit fraud, we are just using our knowledge to try and estimate a “good” model.</p>
<div id="confidence-intervals-dont-save-us" class="section level2">
<h2>Confidence intervals don’t save us</h2>
<p>A common suggestion for supplementing p-values is to report the confidence interval of the effect. The problem is that the confidence interval, like the p-value, is directly related to the size of the standard error and the resulting t-statistic. Consider the confidence intervals for the first set of simulation results, where we conduct just a single hypothesis test on each sample of data:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>(pvals, <span class="kw">aes</span>(p.value, estimate, <span class="dt">color =</span> sig)) +
<span class="st">  </span><span class="kw">geom_hline</span>(<span class="dt">yintercept =</span> <span class="dv">0</span>) +
<span class="st">  </span><span class="kw">geom_pointrange</span>(<span class="kw">aes</span>(<span class="dt">ymin =</span> estimate -<span class="st"> </span><span class="fl">1.96</span> *<span class="st"> </span>std.error,
                      <span class="dt">ymax =</span> estimate +<span class="st"> </span><span class="fl">1.96</span> *<span class="st"> </span>std.error), <span class="dt">alpha =</span> .<span class="dv">25</span>) +
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="st">&quot;95% CIs when null is true&quot;</span>,
       <span class="dt">x =</span> <span class="kw">expression</span>(P),
       <span class="dt">y =</span> <span class="st">&quot;Estimated effect size&quot;</span>) +
<span class="st">  </span><span class="kw">theme</span>(<span class="dt">legend.position =</span> <span class="st">&quot;none&quot;</span>)</code></pre></div>
<p><img src="persp016_p_hacking_files/figure-html/ci-single-test-1.png" width="672" /></p>
<p>Again, looks correct. However, what about when we conduct 10 tests instead of just one?</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>(pvals_mult, <span class="kw">aes</span>(p.value, estimate, <span class="dt">color =</span> sig)) +
<span class="st">  </span><span class="kw">geom_hline</span>(<span class="dt">yintercept =</span> <span class="dv">0</span>) +
<span class="st">  </span><span class="kw">geom_pointrange</span>(<span class="kw">aes</span>(<span class="dt">ymin =</span> estimate -<span class="st"> </span><span class="fl">1.96</span> *<span class="st"> </span>std.error,
                      <span class="dt">ymax =</span> estimate +<span class="st"> </span><span class="fl">1.96</span> *<span class="st"> </span>std.error), <span class="dt">alpha =</span> .<span class="dv">25</span>) +
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="st">&quot;Most significant 95% CIs of 10 tests when null is true&quot;</span>,
       <span class="dt">x =</span> <span class="kw">expression</span>(P),
       <span class="dt">y =</span> <span class="st">&quot;Estimated effect size&quot;</span>) +
<span class="st">  </span><span class="kw">theme</span>(<span class="dt">legend.position =</span> <span class="st">&quot;none&quot;</span>)</code></pre></div>
<p><img src="persp016_p_hacking_files/figure-html/ci-mult-test-1.png" width="672" /></p>
<p>Selection bias of this type is not addressed or corrected merely by reporting both the p-value and the confidence interval.</p>
</div>
<div id="selection-of-predictors" class="section level2">
<h2>Selection of predictors</h2>
<blockquote>
<p>For more detail on subset selection methods, see chapter 6.1 in ISLR.</p>
</blockquote>
<p>Another common analysis in which p-values can be misinterpreted is the selection of a prediction mdoel for multiple regression or classification. <strong>Subset selection</strong> procedures automate the process of deciding which predictors to include or exclude in a regression model. Under the <strong>best subset selection</strong>:</p>
<ol style="list-style-type: decimal">
<li>Let <span class="math inline">\(M_0\)</span> denote the null model which contains no predictors. This model simply predicts the sample mean for each observation.</li>
<li>For <span class="math inline">\(k = 1, 2, \dots, p\)</span>:
<ol style="list-style-type: decimal">
<li>Fit all <span class="math inline">\({p}\choose{k}\)</span> models that contain exactly <span class="math inline">\(k\)</span> predictors</li>
<li>Pick the best among these <span class="math inline">\({p}\choose{k}\)</span> models and call it <span class="math inline">\(M_k\)</span>. Best is defined by the smallest RSS or RMSE, or largest <span class="math inline">\(R^2\)</span></li>
</ol></li>
<li>Select a single best model from among <span class="math inline">\(M_0, \dots, M_p\)</span> using cross-validated prediction error or similar metrics</li>
</ol>
<p>Of course this approach can be computationally infeasible as the number of predictors <span class="math inline">\(p\)</span> increases. An alternative (and popular) choice is <strong>stepwise selection</strong>. In <strong>forward stepwise regression</strong>:</p>
<ol style="list-style-type: decimal">
<li>Let <span class="math inline">\(M_0\)</span> denote the null model which contains no predictors</li>
<li>For <span class="math inline">\(k = 1, 2, \dots, p - 1\)</span>:
<ol style="list-style-type: decimal">
<li>Fit all <span class="math inline">\(p-k\)</span> models that augment the predictors in <span class="math inline">\(M_k\)</span> with one additional predictor</li>
<li>Pick the best among these <span class="math inline">\(p - k\)</span> models and call it <span class="math inline">\(M_{k+1}\)</span>. Best is defined by the smallest RSS or RMSE, or largest <span class="math inline">\(R^2\)</span></li>
</ol></li>
<li>Select a single best model from among <span class="math inline">\(M_0, \dots, M_p\)</span> using cross-validated prediction error or similar metrics</li>
</ol>
<p>This method avoids estimating all <span class="math inline">\(2^p\)</span> models. Instead, you only have to estimate <span class="math inline">\(1 + \frac{p(p+1)}{2}\)</span>. When <span class="math inline">\(p=20\)</span>, this is the difference between estimating <span class="math inline">\(p^{20} = 1,048,576\)</span> models versus <span class="math inline">\(1 + \frac{10(10+1)}{2} = 211\)</span> models.</p>
<p>In <strong>backward stepwise selection</strong>, you start with the full model and remove predictors from the model during each iteration.</p>
<ol style="list-style-type: decimal">
<li>Let <span class="math inline">\(M_0\)</span> denote the full model which contains all <span class="math inline">\(p\)</span> predictors</li>
<li>For <span class="math inline">\(k = p, p-1, \dots, 1\)</span>:
<ol style="list-style-type: decimal">
<li>Fit all <span class="math inline">\(k\)</span> models that contain all but one of the predictors in <span class="math inline">\(M_k\)</span>, for a total of <span class="math inline">\(k-1\)</span> predictors</li>
<li>Pick the best among these <span class="math inline">\(k\)</span> models and call it <span class="math inline">\(M_{k-1}\)</span>. Best is defined by the smallest RSS or RMSE, or largest <span class="math inline">\(R^2\)</span></li>
</ol></li>
<li>Select a single best model from among <span class="math inline">\(M_0, \dots, M_p\)</span> using cross-validated prediction error or similar metrics</li>
</ol>
<p>While these approaches are based on minimizing overall model error, they are still problematic for interpreting the resulting p-values.<a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a> Consider if we applied forward stepwise selection to our simulated data from earlier. First let’s use as our baseline a multiple regression model containing all 10 predictors from <span class="math inline">\(\mathbf{X}\)</span>. How often do we reject the null hypothesis of no association between <span class="math inline">\(X_k\)</span> and <span class="math inline">\(Y\)</span>?</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">full_mod_sim &lt;-<span class="st"> </span>function(n_obs){
  x &lt;-<span class="st"> </span><span class="kw">replicate</span>(<span class="dv">10</span>, <span class="kw">rnorm</span>(n_obs))
  y &lt;-<span class="st"> </span><span class="kw">rnorm</span>(n_obs)
  
  mod &lt;-<span class="st"> </span><span class="kw">lm</span>(y ~<span class="st"> </span>x)
  
  <span class="kw">return</span>(<span class="kw">tidy</span>(mod))
}

pvals_full &lt;-<span class="st"> </span><span class="dv">1000</span> %&gt;%
<span class="st">  </span><span class="kw">rerun</span>(<span class="kw">full_mod_sim</span>(n_obs)) %&gt;%
<span class="st">  </span>bind_rows %&gt;%
<span class="st">  </span>as_tibble %&gt;%
<span class="st">  </span><span class="kw">filter</span>(term !=<span class="st"> &quot;(Intercept)&quot;</span>) %&gt;%
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">sig =</span> p.value &lt;<span class="st"> </span>.<span class="dv">05</span>)

<span class="kw">ggplot</span>(pvals_full, <span class="kw">aes</span>(p.value, <span class="dt">fill =</span> sig)) +
<span class="st">  </span><span class="kw">geom_histogram</span>(<span class="dt">binwidth =</span> .<span class="dv">025</span>, <span class="dt">boundary =</span> <span class="dv">0</span>) +
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="st">&quot;Distribution of p-values from full model when null is true&quot;</span>,
       <span class="dt">subtitle =</span> <span class="st">&quot;All covariates&quot;</span>,
       <span class="dt">x =</span> <span class="kw">expression</span>(P),
       <span class="dt">y =</span> <span class="ot">NULL</span>) +
<span class="st">  </span><span class="kw">theme</span>(<span class="dt">legend.position =</span> <span class="st">&quot;none&quot;</span>)</code></pre></div>
<p><img src="persp016_p_hacking_files/figure-html/sim-step-single-1.png" width="672" /></p>
<p>As expected, only approximately 5% of the time do we make a false discovery. What happens if we use forward stepwise selection to choose the “best” model? How often will we reject the null hypothesis for each of the variables in the model?</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">step_mod_sim &lt;-<span class="st"> </span>function(n_obs){
  x &lt;-<span class="st"> </span><span class="kw">replicate</span>(<span class="dv">10</span>, <span class="kw">rnorm</span>(n_obs)) %&gt;%
<span class="st">    </span><span class="kw">as_tibble</span>()
  y &lt;-<span class="st"> </span><span class="kw">rnorm</span>(n_obs)
  
  sim_data &lt;-<span class="st"> </span>x %&gt;%
<span class="st">    </span><span class="kw">mutate</span>(<span class="dt">y =</span> y)
  
  <span class="co"># estimate full model</span>
  mod &lt;-<span class="st"> </span><span class="kw">lm</span>(y ~<span class="st"> </span>., <span class="dt">data =</span> sim_data)
  
  <span class="co"># pick model with lowest aic based on forward stepwise selection</span>
  <span class="kw">invisible</span>(MASS::<span class="kw">stepAIC</span>(mod))
}

pvals_step &lt;-<span class="st"> </span><span class="dv">1000</span> %&gt;%
<span class="st">  </span><span class="kw">rerun</span>(<span class="kw">step_mod_sim</span>(n_obs))</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># tidy</span>
pvals_step_tidy &lt;-<span class="st"> </span>pvals_step %&gt;%
<span class="st">  </span><span class="kw">map_df</span>(tidy, <span class="dt">.id =</span> <span class="st">&quot;sim&quot;</span>) %&gt;%
<span class="st">  </span>as_tibble

<span class="co"># glance</span>
pvals_step_glance &lt;-<span class="st"> </span>pvals_step %&gt;%
<span class="st">  </span><span class="kw">map_df</span>(glance, <span class="dt">.id =</span> <span class="st">&quot;sim&quot;</span>) %&gt;%
<span class="st">  </span>as_tibble

<span class="co"># plot of k</span>
n_k &lt;-<span class="st"> </span>pvals_step_tidy %&gt;%
<span class="st">  </span><span class="kw">count</span>(sim) %&gt;%
<span class="st">  </span><span class="kw">count</span>(n) %&gt;%
<span class="st">  </span><span class="co"># remove intercept</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">n =</span> n -<span class="st"> </span><span class="dv">1</span>)

<span class="kw">ggplot</span>(n_k, <span class="kw">aes</span>(n, nn)) +
<span class="st">  </span><span class="kw">geom_col</span>() +
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="st">&quot;Number of times k variables were selected&quot;</span>,
       <span class="dt">x =</span> <span class="kw">expression</span>(k),
       <span class="dt">y =</span> <span class="ot">NULL</span>)</code></pre></div>
<p><img src="persp016_p_hacking_files/figure-html/sim-step-mult-plot-1.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># distribution of model fit metrics</span>
pvals_step_glance %&gt;%
<span class="st">  </span><span class="kw">select</span>(r.squared, p.value) %&gt;%
<span class="st">  </span><span class="kw">gather</span>(stat, val) %&gt;%
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="kw">fct_rev</span>(stat), val)) +
<span class="st">  </span><span class="kw">geom_hline</span>(<span class="dt">yintercept =</span> <span class="fl">0.05</span>) +
<span class="st">  </span><span class="kw">geom_boxplot</span>() +
<span class="st">  </span><span class="kw">scale_x_discrete</span>(<span class="dt">labels =</span> <span class="kw">c</span>(<span class="kw">expression</span>(R^<span class="dv">2</span>), <span class="kw">expression</span>(p))) +
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="kw">expression</span>(<span class="kw">paste</span>(<span class="st">&quot;Distribution of &quot;</span>, R^<span class="dv">2</span>, <span class="st">&quot; and p-values after variable selection&quot;</span>)),
       <span class="dt">subtitle =</span> <span class="st">&quot;p-values from F-test&quot;</span>,
       <span class="dt">x =</span> <span class="ot">NULL</span>,
       <span class="dt">y =</span> <span class="ot">NULL</span>)</code></pre></div>
<p><img src="persp016_p_hacking_files/figure-html/sim-step-mult-plot-2.png" width="672" /></p>
<p>Using stepwise regression, we correctly identify 0 variables as predictive in just 165 of 1000 simulations. We reject the null hypothesis for at least one predictor in 83.5% of the simulations. Our results have a very high false discovery rate, even with just 100 observations and only 10 predictors. Stepwise regression can boost the false discovery rate dramatically.</p>
</div>
<div id="finding-p-values-without-trying" class="section level2">
<h2>Finding p-values without trying</h2>
<div class="figure">
<img src="https://espnfivethirtyeight.files.wordpress.com/2015/08/truth-vigilantes-soccer-calls2.png?quality=90&amp;strip=info&amp;w=1024&amp;ssl=1" />

</div>
<p>Researcher decisions on how to operationalize concepts into variables, select statistical learning methods, specify functional form, transform or specify variables, etc., all influence the p-value and resulting conclusions. The major point is that p-hacking and false discovery can occur even with the best of intentions.</p>
</div>
</div>
<div id="article-critique" class="section level1">
<h1>Article critique</h1>
<div class="figure">
<img src="https://imgs.xkcd.com/comics/significant.png" />

</div>
<ul>
<li><a href="http://journals.sagepub.com.proxy.uchicago.edu/doi/abs/10.1177/0956797613476045">Beall, A. T., &amp; Tracy, J. L. (2013). Women are more likely to wear red or pink at peak fertility. <em>Psychological Science</em>, 0956797613476045.</a></li>
<li><a href="http://www.slate.com/articles/health_and_science/science/2013/07/statistics_and_psychology_multiple_comparisons_give_spurious_results.html">“Too Good to Be True” by Andrew Gelman. <em>Slate</em>.</a></li>
<li><a href="http://andrewgelman.com/2013/07/31/response-by-jessica-tracy-and-alec-beall-to-my-criticism-of-their-paper/">“Response by Jessica Tracy and Alec Beall to my critique of the methods in their paper, ‘Women Are More Likely to Wear Red or Pink at Peak Fertility’” by Andrew Gelman.</a></li>
</ul>
<div id="researcher-degrees-of-freedom" class="section level2">
<h2>Researcher degrees of freedom</h2>
<p><strong>Researcher degrees of freedom</strong> - decision points in the research process of collecting and analyzing data that have an influence on the results obtained from analysis. When these decisions are made during the analysis process, they have the potential to corrupt the results and identify statistically significant effects by cherry-picking and p-hacking the data.</p>
<div id="activity-identify-researcher-degrees-of-freedom-in-beall-and-tracy-2013-article" class="section level3">
<h3>Activity: Identify researcher degrees of freedom in Beall and Tracy (2013) article</h3>
<ul>
<li>Color of shirt - how to group pink and red (or whether to)
<ul>
<li>Why should red matter? Why not white or gray?</li>
</ul></li>
<li>What garment of clothing to ask about</li>
<li>How to define high- vs. low-conception risk group</li>
<li>???</li>
</ul>
</div>
</div>
<div id="other-sources-of-critique" class="section level2">
<h2>Other sources of critique</h2>
<ul>
<li>Representativeness - should these results generalize outside of UBC students and female American MTurkers?</li>
<li>Measurement error - self-reporting is not as accurate as hormone assessment</li>
<li>Effect size - does the 3x effect change with a larger or more representative sample?</li>
</ul>
</div>
<div id="critique-of-critique" class="section level2">
<h2>Critique of critique</h2>
<ul>
<li>Did the authors engage in p-hacking or false discovery? How can we know?</li>
<li>What steps could the author take to minimize false discovery? Did they do any of this?</li>
<li>Should a journal publish this article? Is this “good” science?</li>
</ul>
</div>
</div>
<div id="preventing-false-discovery" class="section level1">
<h1>Preventing false discovery</h1>
<div id="cross-validation" class="section level2">
<h2>Cross-validation</h2>
<p><a href="persp006_resampling.html"><strong>Cross-validation</strong></a> techniques are a useful tool to combat false discovery. You attempt to build an explanatory model using the training set of data, which you can repeatedly use to test different models. You then test the model against the test set of data to evaluate its robustness on out-of-sample data. The one key addition is that you save a third portion of <strong>validation set</strong> data that you only use once. After you build the final model, you test it against the withheld validation set. But you can only use it once. If you use it again, you are conducting multiple hypothesis tests. This is how Kaggle operates for its competitions. They give you a portion of the dataset on which to build a machine learning model. You can split that into a training and test set and build to your heart’s content. But it doesn’t matter what your test error rate is; instead, you submit the code to generate your final model and that is applied to a validation set to which you do not have access.</p>
</div>
<div id="corrected-p-values" class="section level2">
<h2>Corrected p-values</h2>
<p>One approach is to correct the p-value and account for the fact that you are conducting multiple hypotheses test and should therefore expect to have some amount of false discovery. The <strong>family-wise error rate</strong> (FWER) is the probability of making one or more false discoveries (false positives) when performing multiple hypotheses tests. By assuring <span class="math inline">\(\text{FWER} \leq \alpha\)</span>, the probability of making one or more false positives in the family is controlled at level <span class="math inline">\(\alpha\)</span>. The <strong>Bonferroni correction</strong> accomplishes this by rejecting the null hypothesis <span class="math inline">\(H_i\)</span> for each <span class="math inline">\(p_i\)</span> if and only if <span class="math inline">\(p_i \leq \frac{\alpha}{m}\)</span> where <span class="math inline">\(m\)</span> is the number of hypotheses being tested. For example, if a study tested <span class="math inline">\(m=20\)</span> hypotheses with a desired <span class="math inline">\(\alpha = 0.05\)</span> (the standard threshold), then the Bonferroni correction would test each individual hypothesis at <span class="math inline">\(\alpha = \frac{0.05}{20} = 0.0025\)</span>.</p>
<p>For example, draw 100 observations from a normal distribution <span class="math inline">\(N(0,1)\)</span> and test the null hypothesis <span class="math inline">\(H_0: \mu = 0\)</span> using a t-test.<a href="#fn2" class="footnoteRef" id="fnref2"><sup>2</sup></a> If we simulate this process multiple times, we should reject <span class="math inline">\(H_0\)</span> approximately 5% of the time.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">sim_norm_null &lt;-<span class="st"> </span><span class="dv">1000</span> %&gt;%
<span class="st">  </span><span class="kw">rerun</span>(<span class="kw">rnorm</span>(n_obs)) %&gt;%
<span class="st">  </span><span class="kw">map</span>(~<span class="st"> </span><span class="kw">t.test</span>(<span class="dt">x =</span> .x, <span class="dt">mu =</span> <span class="dv">0</span>)) %&gt;%
<span class="st">  </span><span class="kw">map_dbl</span>(~<span class="st"> </span>.x$p.value) %&gt;%
<span class="st">  </span>as_tibble %&gt;%
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">sig =</span> value &lt;<span class="st"> </span>.<span class="dv">05</span>)

<span class="kw">mean</span>(sim_norm_null$value)</code></pre></div>
<pre><code>## [1] 0.501</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>(sim_norm_null, <span class="kw">aes</span>(value, <span class="dt">fill =</span> sig)) +
<span class="st">  </span><span class="kw">geom_histogram</span>(<span class="dt">binwidth =</span> .<span class="dv">025</span>, <span class="dt">boundary =</span> <span class="dv">0</span>) +
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="st">&quot;Distribution of p-values for single test&quot;</span>,
       <span class="dt">x =</span> <span class="kw">expression</span>(P),
       <span class="dt">y =</span> <span class="ot">NULL</span>) +
<span class="st">  </span><span class="kw">theme</span>(<span class="dt">legend.position =</span> <span class="st">&quot;none&quot;</span>)</code></pre></div>
<p><img src="persp016_p_hacking_files/figure-html/sim-norm-null-1.png" width="672" /></p>
<p>Now let’s simulate 5 random variables and test the null hypothesis that all means are simultaneously 0, then the probability of at least one significant result is <span class="math inline">\(1 - (1 - 0.05)^5 = 0.226\)</span>:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">sim_norm_mult &lt;-<span class="st"> </span><span class="dv">1000</span> %&gt;%
<span class="st">  </span><span class="kw">rerun</span>(<span class="dv">5</span> %&gt;%
<span class="st">          </span><span class="kw">rerun</span>(<span class="kw">rnorm</span>(n_obs)) %&gt;%
<span class="st">          </span><span class="kw">map</span>(~<span class="st"> </span><span class="kw">t.test</span>(<span class="dt">x =</span> .x, <span class="dt">mu =</span> <span class="dv">0</span>)) %&gt;%
<span class="st">          </span><span class="kw">map_dbl</span>(~<span class="st"> </span>.x$p.value) %&gt;%
<span class="st">          </span>as_tibble %&gt;%
<span class="st">          </span><span class="kw">mutate</span>(<span class="dt">sig =</span> value &lt;<span class="st"> </span>.<span class="dv">05</span>)) %&gt;%
<span class="st">  </span><span class="kw">bind_rows</span>(<span class="dt">.id =</span> <span class="st">&quot;sim&quot;</span>) %&gt;%
<span class="st">  </span><span class="kw">group_by</span>(sim) %&gt;%
<span class="st">  </span><span class="kw">rename</span>(<span class="dt">raw =</span> value) %&gt;%
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">correct =</span> raw &lt;<span class="st"> </span>(.<span class="dv">05</span> /<span class="st"> </span><span class="kw">n</span>()))

sim_norm_mult %&gt;%
<span class="st">  </span><span class="kw">summarize</span>(<span class="dt">sig =</span> <span class="kw">any</span>(raw &lt;<span class="st"> </span>.<span class="dv">05</span>)) %&gt;%
<span class="st">  </span>ungroup %&gt;%
<span class="st">  </span><span class="kw">summarize</span>(<span class="kw">mean</span>(sig))</code></pre></div>
<pre><code>## # A tibble: 1 x 1
##   `mean(sig)`
##         &lt;dbl&gt;
## 1       0.239</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">sim_norm_mult %&gt;%
<span class="st">  </span><span class="kw">filter</span>(raw ==<span class="st"> </span><span class="kw">min</span>(raw)) %&gt;%
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(raw, <span class="dt">fill =</span> sig)) +
<span class="st">  </span><span class="kw">geom_histogram</span>(<span class="dt">binwidth =</span> .<span class="dv">01</span>, <span class="dt">boundary =</span> <span class="dv">0</span>) +
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="st">&quot;Distribution of p-values for multiple tests&quot;</span>,
       <span class="dt">x =</span> <span class="kw">expression</span>(P),
       <span class="dt">y =</span> <span class="ot">NULL</span>) +
<span class="st">  </span><span class="kw">theme</span>(<span class="dt">legend.position =</span> <span class="st">&quot;none&quot;</span>)</code></pre></div>
<p><img src="persp016_p_hacking_files/figure-html/sim-norm-mult-1.png" width="672" /></p>
<p>But if we use the Bonferroni correction:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">sim_norm_mult %&gt;%
<span class="st">  </span><span class="kw">summarize</span>(<span class="dt">sig =</span> <span class="kw">any</span>(correct)) %&gt;%
<span class="st">  </span>ungroup %&gt;%
<span class="st">  </span><span class="kw">summarize</span>(<span class="kw">mean</span>(sig))</code></pre></div>
<pre><code>## # A tibble: 1 x 1
##   `mean(sig)`
##         &lt;dbl&gt;
## 1        0.05</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">sim_norm_mult %&gt;%
<span class="st">  </span><span class="kw">filter</span>(raw ==<span class="st"> </span><span class="kw">min</span>(raw)) %&gt;%
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(raw, <span class="dt">fill =</span> correct)) +
<span class="st">  </span><span class="kw">geom_histogram</span>(<span class="dt">binwidth =</span> .<span class="dv">01</span>, <span class="dt">boundary =</span> <span class="dv">0</span>) +
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="st">&quot;Distribution of p-values for multiple tests&quot;</span>,
       <span class="dt">subtitle =</span> <span class="st">&quot;With Bonferroni correction&quot;</span>,
       <span class="dt">x =</span> <span class="kw">expression</span>(P),
       <span class="dt">y =</span> <span class="ot">NULL</span>) +
<span class="st">  </span><span class="kw">theme</span>(<span class="dt">legend.position =</span> <span class="st">&quot;none&quot;</span>)</code></pre></div>
<p><img src="persp016_p_hacking_files/figure-html/bonferroni-1.png" width="672" /></p>
<p>One can also use the Bonferroni correction to adjust confidence intervals. If one establishes <span class="math inline">\(m\)</span> confidence intervals and wants an overall confidence level of <span class="math inline">\(1 - \alpha\)</span>, then the confidence interval should be adjusted to the level of <span class="math inline">\(1 - \frac{\alpha}{m}\)</span>.</p>
<p>The Bonferroni correction is a conservative adjustment, and errs on the side of caution. You minimize the risk of a false positive, but therefore increase the risk of a false negative. Alternative correction methods such as the Benjamini–Hochberg procedure are less conservative. See <code>?p.adjust</code> for more information on implementing these correction procedures in R.</p>
</div>
<div id="pre-registration-of-research-design" class="section level2">
<h2>Pre-registration of research design</h2>
<p>A growing movement within academia (especially within the open-source realm) proposes <strong>pre-registration</strong> of research studies. With pre-registration, researchers submit their research rationale, hypotheses, design, and analytic strategy to a journal for peer review <strong>before beginning the study</strong>. The submission undergoes peer review at this point and can be rejected or R&amp;Red just as in a typical peer review process. If the article is accepted at this stage, then the journal conditionally commits to publishing the article once the study is completed.</p>
<p>This procedure frees researchers from the burden of feeling any pressure to p-hack their results to obtain statistically significant (aka “publishable”) results. As long as the methodology is sound, even if the study generates null findings then the journal still publishes the article.</p>
<div id="benefits-of-pre-registration" class="section level3">
<h3>Benefits of pre-registration</h3>
<ul>
<li>Improved use of theory and stronger research methods. In fact, this probably makes the research more difficult because researchers have to consider all the potential research design elements before conducting the study. They cannot make corrections or adjustments along the way.</li>
<li>Decline in false-positive publications. This should also minimize the “replication crisis” throughout the social sciences (most prominently in psychology) by publishing results that, if the p-values lead to correct inference, then the results should be reproducible in independent studies.</li>
<li>Null findings don’t stay hidden in the shadows. If multiple scholars asking the same question get null results, maybe it isn’t a question worth asking. Yet if no one publishes on it, future scholars don’t realize it’s a dead end.</li>
<li>Forces the hand of funding agencies. Minimizes the potential for interference with the results because the study has to be declared beforehand.</li>
</ul>
</div>
<div id="concerns-of-pre-registration" class="section level3">
<h3>Concerns of pre-registration</h3>
<ul>
<li>Minimizes exploratory research. If exploratory research is deemed to be p-hacking, then it becomes less publishable under this new regime.</li>
<li>Journals may skew towards accepting studies only from researchers with established prestige, since they want to minimize publishing mundane or null results. This hurts graduate students and early career scholars who don’t yet have name recognition.</li>
<li>What happens if when conducting the study, the scholar clearly sees an element that is faulty or needs to be adjusted? If they pre-register the design, they open themselves up to accusations of p-hacking if they change the protocol (and the journal is no longer obligated to publish the article).</li>
</ul>
</div>
</div>
</div>
<div id="acknowledgments" class="section level1 toc-ignore">
<h1>Acknowledgments</h1>
<ul>
<li>p-value distribution simulation from <a href="http://www.nature.com/nmeth/journal/v14/n1/full/nmeth.4120.html">Altman, Naomi, and Martin Krzywinski. “Points of significance: P values and the search for significance.” <em>Nature Methods</em> 14.1 (2017): 3-4.</a></li>
</ul>
</div>
<div id="session-info" class="section level1 toc-ignore">
<h1>Session Info</h1>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">devtools::<span class="kw">session_info</span>()</code></pre></div>
<pre><code>##  setting  value                       
##  version  R version 3.3.3 (2017-03-06)
##  system   x86_64, darwin13.4.0        
##  ui       X11                         
##  language (EN)                        
##  collate  en_US.UTF-8                 
##  tz       America/Chicago             
##  date     2017-05-16                  
## 
##  package      * version    date      
##  assertthat     0.2.0      2017-04-11
##  backports      1.0.5      2017-01-18
##  base         * 3.3.3      2017-03-07
##  broom        * 0.4.2      2017-02-13
##  car          * 2.1-4      2016-12-02
##  cellranger     1.1.0      2016-07-27
##  codetools      0.2-15     2016-10-05
##  coefplot     * 1.2.4.9000 2017-04-25
##  colorspace     1.3-2      2016-12-14
##  datasets     * 3.3.3      2017-03-07
##  DBI            0.6-1      2017-04-01
##  devtools       1.13.0     2017-05-08
##  digest         0.6.12     2017-01-27
##  dplyr        * 0.5.0      2016-06-24
##  evaluate       0.10       2016-10-11
##  forcats      * 0.2.0      2017-01-23
##  foreign        0.8-68     2017-04-24
##  ggplot2      * 2.2.1.9000 2017-05-12
##  graphics     * 3.3.3      2017-03-07
##  grDevices    * 3.3.3      2017-03-07
##  grid           3.3.3      2017-03-07
##  gtable         0.2.0      2016-02-26
##  haven          1.0.0      2016-09-23
##  hms            0.3        2016-11-22
##  htmltools      0.3.6      2017-04-28
##  httr           1.2.1      2016-07-03
##  jsonlite       1.4        2017-04-08
##  knitr          1.15.1     2016-11-22
##  labeling       0.3        2014-08-23
##  lattice        0.20-35    2017-03-25
##  lazyeval       0.2.0      2016-06-12
##  lme4         * 1.1-13     2017-04-19
##  lubridate      1.6.0      2016-09-13
##  magrittr       1.5        2014-11-22
##  MASS           7.3-47     2017-04-21
##  Matrix       * 1.2-10     2017-04-28
##  MatrixModels   0.4-1      2015-08-22
##  memoise        1.1.0      2017-04-21
##  methods      * 3.3.3      2017-03-07
##  mgcv           1.8-17     2017-02-08
##  minqa          1.2.4      2014-10-09
##  mnormt         1.5-5      2016-10-15
##  modelr       * 0.1.0      2016-08-31
##  munsell        0.4.3      2016-02-13
##  nlme           3.1-131    2017-02-06
##  nloptr         1.0.4      2014-08-04
##  nnet           7.3-12     2016-02-02
##  parallel       3.3.3      2017-03-07
##  pbkrtest       0.4-7      2017-03-15
##  plyr           1.8.4      2016-06-08
##  psych          1.7.5      2017-05-03
##  purrr        * 0.2.2.2    2017-05-11
##  quantreg       5.33       2017-04-18
##  R6             2.2.1      2017-05-10
##  rcfss        * 0.1.4      2017-02-28
##  RColorBrewer * 1.1-2      2014-12-07
##  Rcpp           0.12.10    2017-03-19
##  readr        * 1.1.0      2017-03-22
##  readxl         1.0.0      2017-04-18
##  reshape2       1.4.2      2016-10-22
##  rlang          0.1.9000   2017-05-12
##  rmarkdown      1.5        2017-04-26
##  rprojroot      1.2        2017-01-16
##  rvest          0.3.2      2016-06-17
##  scales         0.4.1      2016-11-09
##  SparseM        1.77       2017-04-23
##  splines        3.3.3      2017-03-07
##  stats        * 3.3.3      2017-03-07
##  stringi        1.1.5      2017-04-07
##  stringr      * 1.2.0      2017-02-18
##  tibble       * 1.3.0.9002 2017-05-12
##  tidyr        * 0.6.2      2017-05-04
##  tidyverse    * 1.1.1      2017-01-27
##  tools          3.3.3      2017-03-07
##  useful         1.2.1      2016-06-29
##  utils        * 3.3.3      2017-03-07
##  withr          1.0.2      2016-06-20
##  xml2           1.1.1      2017-01-24
##  yaml           2.1.14     2016-11-12
##  source                               
##  cran (@0.2.0)                        
##  CRAN (R 3.3.2)                       
##  local                                
##  CRAN (R 3.3.2)                       
##  CRAN (R 3.3.2)                       
##  CRAN (R 3.3.0)                       
##  CRAN (R 3.3.3)                       
##  Github (jaredlander/coefplot@0755a00)
##  CRAN (R 3.3.2)                       
##  local                                
##  CRAN (R 3.3.2)                       
##  CRAN (R 3.3.2)                       
##  CRAN (R 3.3.2)                       
##  CRAN (R 3.3.0)                       
##  CRAN (R 3.3.0)                       
##  CRAN (R 3.3.2)                       
##  CRAN (R 3.3.2)                       
##  Github (tidyverse/ggplot2@f4398b6)   
##  local                                
##  local                                
##  local                                
##  CRAN (R 3.3.0)                       
##  cran (@1.0.0)                        
##  CRAN (R 3.3.2)                       
##  cran (@0.3.6)                        
##  CRAN (R 3.3.0)                       
##  cran (@1.4)                          
##  cran (@1.15.1)                       
##  CRAN (R 3.3.0)                       
##  CRAN (R 3.3.2)                       
##  CRAN (R 3.3.0)                       
##  cran (@1.1-13)                       
##  CRAN (R 3.3.0)                       
##  CRAN (R 3.3.0)                       
##  CRAN (R 3.3.2)                       
##  CRAN (R 3.3.2)                       
##  CRAN (R 3.3.0)                       
##  CRAN (R 3.3.2)                       
##  local                                
##  CRAN (R 3.3.3)                       
##  cran (@1.2.4)                        
##  CRAN (R 3.3.0)                       
##  CRAN (R 3.3.0)                       
##  CRAN (R 3.3.0)                       
##  CRAN (R 3.3.3)                       
##  cran (@1.0.4)                        
##  CRAN (R 3.3.3)                       
##  local                                
##  CRAN (R 3.3.2)                       
##  CRAN (R 3.3.0)                       
##  CRAN (R 3.3.3)                       
##  CRAN (R 3.3.3)                       
##  CRAN (R 3.3.2)                       
##  CRAN (R 3.3.2)                       
##  local                                
##  CRAN (R 3.3.0)                       
##  cran (@0.12.10)                      
##  cran (@1.1.0)                        
##  CRAN (R 3.3.2)                       
##  CRAN (R 3.3.0)                       
##  Github (hadley/rlang@c17568e)        
##  CRAN (R 3.3.2)                       
##  CRAN (R 3.3.2)                       
##  CRAN (R 3.3.0)                       
##  CRAN (R 3.3.1)                       
##  CRAN (R 3.3.2)                       
##  local                                
##  local                                
##  CRAN (R 3.3.2)                       
##  CRAN (R 3.3.2)                       
##  Github (tidyverse/tibble@9103a30)    
##  CRAN (R 3.3.2)                       
##  CRAN (R 3.3.2)                       
##  local                                
##  CRAN (R 3.3.0)                       
##  local                                
##  CRAN (R 3.3.0)                       
##  CRAN (R 3.3.2)                       
##  cran (@2.1.14)</code></pre>
</div>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>This is probably why they are advertised for building <a href="persp001_stat_learn.html#why_estimate_(f)"><strong>prediction</strong> models, not inferential models</a>.<a href="#fnref1">↩</a></p></li>
<li id="fn2"><p>Example drawn from <a href="https://stats.stackexchange.com/questions/135279/simulating-a-multiple-comparisons-problem-using-r-and-bonferroni-correction">this StackOverflow question</a>.<a href="#fnref2">↩</a></p></li>
</ol>
</div>

<p>This work is licensed under the  <a href="http://creativecommons.org/licenses/by-nc/4.0/">CC BY-NC 4.0 Creative Commons License</a>.</p>


</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
